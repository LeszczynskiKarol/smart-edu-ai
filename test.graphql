{  "content": "<h1>Rozdział nr 1</h1>   <h2>Wprowadzenie do rozdziału</h2>   <h2>1.1. Wprowadzenie do systemów rozpoznawania gestów</h2> <h3>1.1.1. Definicja i klasyfikacja systemów rozpoznawania gestów</h3> Rozpoznawanie gestów to dziedzina badań informatycznych, która koncentruje się na interpretacji ruchów ludzkiego ciała, szczególnie rąk i dłoni, jako środka komunikacji z systemami komputerowymi. W kontekście interakcji człowiek-maszyna, gest można zdefiniować jako fizyczny ruch ciała, który zawiera informację [Mitra & Acharya, 2007: 311]. Gesty stanowią naturalny i intuicyjny sposób komunikacji, który może być wykorzystany jako alternatywa dla tradycyjnych interfejsów, takich jak klawiatura czy mysz. Systemy rozpoznawania gestów można klasyfikować według różnych kryteriów. Jednym z podstawowych podziałów jest rozróżnienie na gesty statyczne i dynamiczne. Gesty statyczne (pozy) to konkretne ułożenia ciała lub jego części, które nie zmieniają się w czasie, natomiast gesty dynamiczne to sekwencje ruchów wykonywanych w określonym przedziale czasowym [Rautaray & Agrawal, 2015: 3]. Przykładem gestu statycznego może być pokazanie określonego znaku dłonią, podczas gdy gest dynamiczny może obejmować przesunięcie ręki w określonym kierunku. Innym kryterium klasyfikacji jest cel komunikacyjny gestu. W tym kontekście wyróżniamy: - Gesty deiktyczne - wskazujące obiekty lub lokalizacje (np. wskazanie palcem) - Gesty ikoniczne - ilustrujące fizyczne cechy obiektów (np. pokazanie kształtu) - Gesty manipulacyjne - służące do kontroli obiektów (np. gest chwytania) - Gesty semiotyczne - przekazujące określone symbole lub komunikaty (np. gest OK) Ze względu na technologię wykorzystywaną do detekcji gestów, systemy rozpoznawania gestów można podzielić na: 1. Systemy oparte na wizji komputerowej - wykorzystujące kamery do rejestrowania i analizy ruchów użytkownika. Mogą to być:    - Systemy wykorzystujące pojedynczą kamerę RGB    - Systemy wykorzystujące kamery głębi (np. Time-of-Flight)    - Systemy stereowizyjne wykorzystujące wiele kamer 2. Systemy oparte na sensorach - wykorzystujące różnego rodzaju czujniki do detekcji ruchów:    - Akcelerometry i żyroskopy    - Sensory elektromagnetyczne    - Sensory inercyjne    - Rękawice z sensorami zgięcia 3. Systemy hybrydowe - łączące różne technologie w celu zwiększenia dokładności i niezawodności rozpoznawania gestów. Proces rozpoznawania gestów obejmuje zazwyczaj kilka kluczowych etapów: 1. Segmentacja - wyodrębnienie obszaru zainteresowania z tła, np. oddzielenie dłoni od reszty obrazu. 2. Detekcja - identyfikacja obecności gestu w strumieniu danych wejściowych. 3. Śledzenie - monitorowanie zmian pozycji i konfiguracji wykrytych elementów w czasie. 4. Rozpoznawanie - klasyfikacja wykrytego gestu do jednej z predefiniowanych kategorii. Każdy z tych etapów wymaga zastosowania odpowiednich algorytmów i technik przetwarzania danych, które będą omówione szczegółowo w dalszych częściach rozdziału. Warto zauważyć, że efektywność systemu rozpoznawania gestów zależy od wielu czynników, w tym od dokładności detekcji, zdolności do rozpoznawania gestów w różnych warunkach oświetleniowych i środowiskowych, a także od zdolności adaptacji do różnych użytkowników. Systemy te muszą być również odporne na zakłócenia i fałszywe detekcje, co stanowi istotne wyzwanie techniczne. <h3>1.1.2. Historia rozwoju systemów rozpoznawania gestów</h3> Historia systemów rozpoznawania gestów sięga lat 80. XX wieku, kiedy to rozpoczęto pierwsze prace badawcze nad interfejsami opartymi na gestach. Rozwój tej dziedziny był ściśle związany z postępem w obszarze wizji komputerowej, uczenia maszynowego oraz technologii sensorycznych. Jednym z pierwszych znaczących osiągnięć w dziedzinie rozpoznawania gestów było opracowanie w 1983 roku rękawicy DataGlove przez VPL Research [Sturman & Zeltzer, 1994: 31]. Urządzenie to wykorzystywało światłowody do pomiaru zgięcia palców oraz sensor elektromagnetyczny do określenia pozycji i orientacji dłoni w przestrzeni. DataGlove stanowił przełom w dziedzinie interfejsów człowiek-komputer, umożliwiając bezpośrednią manipulację obiektami wirtualnymi za pomocą naturalnych ruchów dłoni. W latach 90. nastąpił dynamiczny rozwój systemów opartych na wizji komputerowej. Pionierskie prace Vladimira Pavlovica, Rajeeva Sharmy i Thomasa Huanga [1997] ustanowiły podwaliny pod metody analizy i rozpoznawania gestów na podstawie obrazu. W tym okresie zaczęto stosować modele probabilistyczne, takie jak Ukryte Modele Markowa (HMM), do rozpoznawania sekwencji gestów dynamicznych [Starner & Pentland, 1995: 228]. Przełom XXI wieku przyniósł znaczący postęp w dziedzinie rozpoznawania gestów, głównie dzięki rozwojowi algorytmów uczenia maszynowego i zwiększeniu mocy obliczeniowej komputerów. W 2001 roku firma GestureTek wprowadziła system GestPoint, który umożliwiał interakcję z wyświetlaczami za pomocą gestów, bez konieczności używania dodatkowych urządzeń wejściowych [Rautaray & Agrawal, 2015: 5]. Kolejnym kamieniem milowym był wprowadzony w 2010 roku Microsoft Kinect, który zrewolucjonizował dziedzinę rozpoznawania gestów dzięki zastosowaniu kamery głębi. Kinect umożliwiał śledzenie całego ciała użytkownika w trzech wymiarach, co otworzyło nowe możliwości w zakresie naturalnych interfejsów użytkownika [Zhang, 2012: 4]. W tym samym okresie rozwijały się również inne technologie, takie jak Leap Motion (2012), koncentrujący się na dokładnym śledzeniu ruchów dłoni i palców, czy MYO Armband (2013), wykorzystujący sensory elektromiograficzne do wykrywania aktywności mięśni przedramienia. Ostatnie lata przyniosły znaczący postęp w dziedzinie głębokiego uczenia, co przyczyniło się do dalszego rozwoju systemów rozpoznawania gestów. Sieci neuronowe, szczególnie konwolucyjne (CNN) i rekurencyjne (RNN), umożliwiły osiągnięcie bezprecedensowej dokładności rozpoznawania gestów w złożonych warunkach środowiskowych [Neverova et al., 2014: 1206]. Równolegle rozwijały się również systemy dedykowane dla zastosowań przemysłowych i robotycznych. Firmy takie jak ABB, KUKA czy Boston Dynamics zaczęły implementować systemy rozpoznawania gestów w swoich robotach, umożliwiając intuicyjną interakcję człowiek-robot. Przykładem może być robot Spot firmy Boston Dynamics, który został wyposażony w systemy wizyjne umożliwiające rozpoznawanie gestów operatora [Boston Dynamics, 2020]. Współczesne systemy rozpoznawania gestów korzystają z zaawansowanych algorytmów fuzji danych, łącząc informacje z różnych sensorów (kamery RGB, sensory głębi, akcelerometry, itp.) w celu zwiększenia dokładności i niezawodności. Ponadto, dzięki miniaturyzacji komponentów elektronicznych i zwiększeniu efektywności energetycznej, możliwe stało się implementowanie systemów rozpoznawania gestów w urządzeniach mobilnych i noszonych, co otworzyło nowe obszary zastosowań. Historia rozwoju systemów rozpoznawania gestów pokazuje ewolucję od prostych systemów opartych na dedykowanych sensorach do zaawansowanych rozwiązań wykorzystujących uczenie głębokie i fuzję danych. Każdy etap tego rozwoju przynosił nowe możliwości i zastosowania, przyczyniając się do coraz bardziej naturalnej i intuicyjnej interakcji człowiek-maszyna. <h3>1.1.3. Zastosowania systemów rozpoznawania gestów w robotyce</h3> Systemy rozpoznawania gestów znajdują szerokie zastosowanie w robotyce, oferując intuicyjny i naturalny sposób interakcji między człowiekiem a robotem. Zastosowania te można podzielić na kilka głównych kategorii, które zostaną omówione poniżej. W robotyce asystującej, systemy rozpoznawania gestów odgrywają kluczową rolę w ułatwianiu komunikacji między użytkownikiem a robotem. Roboty asystujące, takie jak prototypowe modele Boston Dynamics, wykorzystują te systemy do interpretacji poleceń użytkownika i dostosowywania swoich działań do jego potrzeb. Przykładowo, robot asystujący może reagować na gest wskazania określonego obiektu, podając go użytkownikowi, lub na gest przywołania, zbliżając się do osoby potrzebującej pomocy [Rautaray & Agrawal, 2015: 12]. Systemy te są szczególnie wartościowe w kontekście opieki nad osobami starszymi lub niepełnosprawnymi, gdzie tradycyjne interfejsy mogą być trudne w użyciu. W robotyce przemysłowej, rozpoznawanie gestów umożliwia bardziej elastyczną i intuicyjną kontrolę robotów produkcyjnych. Operatorzy mogą wykorzystywać gesty do programowania trajektorii robota, wskazywania obiektów do manipulacji lub zatrzymywania operacji w sytuacjach awaryjnych [Kopinski et al., 2016: 238]. Systemy te zwiększają bezpieczeństwo i efektywność współpracy człowiek-robot w środowisku produkcyjnym. Przykładem może być system implementowany przez firmę ABB, który pozwala operatorom na kontrolę robotów przemysłowych za pomocą gestów rąk, eliminując potrzebę korzystania z tradycyjnych paneli sterowania. Roboty medyczne stanowią kolejny obszar, gdzie systemy rozpoznawania gestów znajdują istotne zastosowanie. W chirurgii robotycznej, gesty chirurga mogą być interpretowane i przekładane na precyzyjne ruchy narzędzi chirurgicznych [Jiang et al., 2017: 1789]. Pozwala to na przeprowadzanie skomplikowanych zabiegów z większą dokładnością i mniejszą inwazyjnością. Ponadto, systemy te mogą być wykorzystywane w rehabilitacji, gdzie robot monitoruje i analizuje ruchy pacjenta, dostosowując program terapeutyczny do jego postępów. W obszarze robotów edukacyjnych, systemy rozpoznawania gestów umożliwiają interaktywne nauczanie poprzez demonstrację. Roboty mogą obserwować i naśladować gesty instruktora, ucząc się nowych zadań lub sekwencji ruchów [Calinon & Billard, 2007: 286]. Z drugiej strony, roboty edukacyjne mogą wykorzystywać rozpoznawanie gestów do interakcji z uczniami, reagując na ich polecenia i dostosowując materiał dydaktyczny do ich potrzeb. Systemy rozpoznawania gestów są również kluczowe w robotyce mobilnej i pojazdach autonomicznych. Gesty mogą być wykorzystywane do kierowania robotem lub pojazdem w określonym kierunku, wskazywania punktów docelowych lub sygnalizowania zatrzymania [Hasanuzzaman et al., 2006: 1191]. W kontekście robotów Boston Dynamics, takich jak Spot czy Atlas, rozpoznawanie gestów umożliwia operatorom wydawanie poleceń nawigacyjnych lub inicjowanie określonych zachowań bez konieczności używania tradycyjnych kontrolerów. W robotyce społecznej, systemy rozpoznawania gestów są niezbędne do interpretacji niewerbalnych sygnałów komunikacyjnych człowieka. Roboty społeczne, takie jak Pepper czy NAO, wykorzystują te systemy do rozpoznawania emocji, intencji i potrzeb użytkowników, co pozwala im na bardziej naturalne i empatyczne interakcje [Anzalone et al., 2015: 1038]. Rozpoznawanie gestów w tym kontekście obejmuje nie tylko ruchy rąk, ale również mimikę twarzy, postawę ciała i kontakt wzrokowy. Roboty humanoidalne, jak Atlas firmy Boston Dynamics, wykorzystują systemy rozpoznawania gestów do naśladowania ruchów człowieka. Umożliwia to bardziej naturalne i płynne poruszanie się robota oraz wykonywanie złożonych zadań manipulacyjnych [Koenemann et al., 2014: 56]. Dodatkowo, systemy te pozwalają robotom na uczenie się nowych umiejętności poprzez obserwację i naśladowanie demonstracji człowieka. W zastosowaniach wojskowych i ratowniczych, systemy rozpoznawania gestów umożliwiają cichą i dyskretną komunikację z robotami w sytuacjach, gdy komunikacja głosowa jest niemożliwa lub niewskazana [Barros et al., 2014: 177]. Roboty, takie jak Spot firmy Boston Dynamics, mogą być wykorzystywane w misjach poszukiwawczo-ratowniczych, reagując na gesty ratowników wskazujące kierunek poszukiwań lub potencjalne zagrożenia. Podsumowując, systemy rozpoznawania gestów znajdują zastosowanie w szerokiej gamie robotów, od przemysłowych i medycznych, po edukacyjne i społeczne. W kontekście robotów asystujących firmy Boston Dynamics, technologia ta umożliwia bardziej intuicyjną i naturalną interakcję człowiek-robot, co jest kluczowe dla efektywnej współpracy w różnorodnych scenariuszach zastosowań. <h3>1.1.4. Wyzwania w implementacji systemów rozpoznawania gestów</h3> Implementacja systemów rozpoznawania gestów w robotyce, szczególnie w kontekście robotów asystujących, wiąże się z licznymi wyzwaniami technicznymi i praktycznymi. Zrozumienie tych wyzwań jest kluczowe dla opracowania skutecznych i niezawodnych systemów. Jednym z podstawowych wyzwań są zmienne warunki oświetleniowe, które mogą znacząco wpływać na jakość obrazu i skuteczność detekcji gestów w systemach opartych na wizji [Rautaray & Agrawal, 2015: 15]. Zmiany intensywności światła, cienie, odblaski czy nagłe zmiany oświetlenia mogą prowadzić do błędnej segmentacji obrazu i nieprawidłowego rozpoznawania gestów. W przypadku robotów Boston Dynamics, które często operują w dynamicznych środowiskach zarówno wewnątrz, jak i na zewnątrz budynków, problem ten jest szczególnie istotny. Rozwiązania obejmują adaptacyjne algorytmy przetwarzania obrazu, wykorzystanie kamer działających w podczerwieni lub kamer głębi, które są mniej wrażliwe na zmiany oświetlenia [Han et al., 2013: 1329]. Kolejnym znaczącym wyzwaniem są okluzje, czyli sytuacje, gdy część ciała wykonującego gest jest zasłonięta przez inny obiekt lub część ciała [Wu & Huang, 1999: 436]. Okluzje mogą prowadzić do niepełnej informacji o geście, co utrudnia jego prawidłowe rozpoznanie. Problem ten jest szczególnie widoczny w zatłoczonych środowiskach, gdzie robot asystujący musi interpretować gesty wykonywane przez konkretną osobę wśród wielu innych. Techniki radzenia sobie z okluzjami obejmują modelowanie probabilistyczne, które uwzględnia niepewność wynikającą z brakujących danych, oraz wykorzystanie wielu kamer lub sensorów umieszczonych w różnych lokalizacjach [Mitra & Acharya, 2007: 317]. Różnice w wykonaniu gestów przez różnych użytkowników stanowią kolejne istotne wyzwanie. Każda osoba może wykonywać ten sam gest nieco inaczej, z różną prędkością, amplitudą ruchu czy dokładnością [Moni & Ali, 2009: 3]. Dodatkowo, ten sam użytkownik może wykonywać ten sam gest różnie w zależności od kontekstu czy swojego stanu fizycznego. Systemy rozpoznawania gestów muszą być odporne na te wariacje, zachowując wysoką dokładność rozpoznawania. Podejścia oparte na uczeniu maszynowym, szczególnie głębokim uczeniu, wykazują obiecujące wyniki w radzeniu sobie z tą różnorodnością, pod warunkiem dostępności odpowiednio dużych i zróżnicowanych zbiorów danych treningowych [Neverova et al., 2014: 1208]. Wyzwania obliczeniowe związane z przetwarzaniem w czasie rzeczywistym są szczególnie istotne w robotyce, gdzie opóźnienia w rozpoznawaniu gestów mogą prowadzić do nieefektywnej interakcji lub nawet zagrożeń bezpieczeństwa [Kopinski et al., 2016: 240]. Algorytmy rozpoznawania gestów muszą być zoptymalizowane pod kątem efektywności obliczeniowej, przy jednoczesnym zachowaniu wysokiej dokładności. W kontekście robotów Boston Dynamics, które często mają ograniczone zasoby obliczeniowe i energetyczne, optymalizacja ta jest krytyczna. Rozwiązania obejmują implementację algorytmów na dedykowanych procesorach graficznych (GPU) lub jednostkach przetwarzania neuronowego (NPU), a także stosowanie technik kompresji modeli uczenia maszynowego [Howard et al., 2017: 1]. Interpretacja kontekstowa gestów stanowi kolejne wyzwanie, ponieważ znaczenie gestu może zależeć od kontekstu, w którym jest wykonywany [Mitra & Acharya, 2007: 319]. Ten sam gest może mieć różne znaczenie w różnych sytuacjach lub kulturach. Systemy rozpoznawania gestów muszą więc uwzględniać nie tylko sam gest, ale również kontekst jego wykonania, co wymaga integracji informacji z różnych źródeł i zaawansowanych technik wnioskowania [Hasanuzzaman et al., 2006: 1193]. Personalizacja i adaptacja do różnych użytkowników to istotne wyzwanie, szczególnie w kontekście robotów asystujących, które mogą współpracować z różnymi osobami o różnych preferencjach i możliwościach fizycznych [Fong et al., 2003: 149]. Systemy rozpoznawania gestów powinny być w stanie dostosować się do indywidualnych cech użytkownika, ucząc się jego specyficznego sposobu wykonywania gestów. Podejścia oparte na uczeniu przyrostowym lub transferowym umożliwiają taką adaptację, pozwalając systemowi na ciągłe doskonalenie się w oparciu o interakcje z konkretnym użytkownikiem [Calinon & Billard, 2007: 288]. Wyzwania związane z prywatnością i bezpieczeństwem danych są również istotne, szczególnie w przypadku systemów, które gromadzą i przetwarzają dane wizualne użytkowników [Jiang et al., 2017: 1791]. Systemy rozpoznawania gestów muszą być projektowane z uwzględnieniem ochrony prywatności, zapewniając, że gromadzone dane są odpowiednio zabezpieczone i wykorzystywane zgodnie z przepisami o ochronie danych osobowych. Integracja systemów rozpoznawania gestów z ogólną architekturą kontroli robota stanowi kolejne wyzwanie, wymagające opracowania odpowiednich interfejsów i protokołów komunikacyjnych [Koenemann et al., 2014: 58]. System musi być w stanie przekształcić rozpoznane gesty na konkretne polecenia zrozumiałe dla robota, uwzględniając przy tym ograniczenia kinematyczne i dynamiczne robota. Podsumowując, implementacja systemów rozpoznawania gestów w robotyce, szczególnie w kontekście robotów asystujących firmy Boston Dynamics, wiąże się z licznymi wyzwaniami technicznymi i praktycznymi. Skuteczne rozwiązanie tych wyzwań wymaga interdyscyplinarnego podejścia, łączącego wiedzę z zakresu wizji komputerowej, uczenia maszynowego, robotyki i interakcji człowiek-maszyna. <h3>1.1.5. Znaczenie rozpoznawania gestów w interakcji człowiek-robot</h3> Rozpoznawanie gestów odgrywa fundamentalną rolę w kształtowaniu efektywnej i naturalnej interakcji między człowiekiem a robotem (HRI - Human-Robot Interaction). Znaczenie tej technologii wynika z kilku kluczowych aspektów, które wpływają na jakość współpracy, akceptację robotów przez użytkowników oraz ogólną skuteczność systemów robotycznych. Przede wszystkim, komunikacja gestowa stanowi naturalny i intuicyjny sposób interakcji, który nie wymaga od użytkownika uczenia się skomplikowanych interfejsów czy języków programowania [Fong et al., 2003: 145]. Ludzie używają gestów w codziennej komunikacji, co sprawia, że ich zastosowanie w interakcji z robotami jest naturalnym rozszerzeniem ludzkich zdolności komunikacyjnych. W kontekście robotów asystujących, takich jak prototypy Boston Dynamics, możliwość wydawania poleceń za pomocą prostych gestów znacząco obniża barierę wejścia dla nowych użytkowników i zwiększa dostępność technologii dla osób o różnym poziomie umiejętności technicznych [Goodrich & Schultz, 2007: 216]. Komunikacja gestowa oferuje również większą ekspresyjność i precyzję w określonych kontekstach, szczególnie w odniesieniu do informacji przestrzennych i kierunkowych [Mitra & Acharya, 2007: 320]. Wskazanie konkretnego obiektu lub lokalizacji za pomocą gestu jest często bardziej precyzyjne i efektywne niż próba opisania tego słowami. W robotyce asystującej, gdzie robot musi często manipulować obiektami w przestrzeni lub poruszać się w określonym kierunku, taka precyzja jest niezwykle cenna. Przykładowo, robot Spot firmy Boston Dynamics może być kierowany do określonej lokalizacji za pomocą prostego gestu wskazującego, co jest znacznie bardziej intuicyjne niż podawanie współrzędnych czy używanie kontrolera [Boston Dynamics, 2020]. Istotnym aspektem jest również uniwersalność komunikacji gestowej, która w wielu przypadkach przekracza bariery językowe i kulturowe [Hasanuzzaman et al., 2006: 1194]. Choć istnieją różnice kulturowe w interpretacji niektórych gestów, podstawowe gesty wskazujące czy manipulacyjne są stosunkowo uniwersalne. W kontekście robotów asystujących, które mogą być używane w różnych krajach i kulturach, ta uniwersalność stanowi znaczącą zaletę. Komunikacja gestowa umożliwia również interakcję w warunkach, gdzie komunikacja głosowa jest utrudniona lub niemożliwa [Barros et al., 2014: 178]. Dotyczy to na przykład środowisk o wysokim poziomie hałasu, takich jak fabryki czy place budowy, a także sytuacji, gdy użytkownik ma trudności z mową lub słuchem. Roboty asystujące Boston Dynamics, które często operują w dynamicznych i wymagających środowiskach, mogą znacząco skorzystać z tej właściwości komunikacji gestowej. Z perspektywy paradygmatów interakcji człowiek-robot, rozpoznawanie gestów wpisuje się w podejście zorientowane na człowieka (human-centered approach), gdzie to robot dostosowuje się do naturalnych sposobów komunikacji człowieka, a nie odwrotnie [Goodrich & Schultz, 2007: 217]. Takie podejście sprzyja budowaniu pozytywnych doświadczeń użytkownika i zwiększa akceptację robotów jako partnerów w codziennych zadaniach. Badania w dziedzinie psychologii pokazują, że komunikacja niewerbalna, w tym gesty, odgrywa kluczową rolę w budowaniu zaufania i komfortu w interakcjach międzyludzkich [Anzalone et al., 2015: 1039]. Podobne zasady można zastosować do interakcji człowiek-robot. Robot, który potrafi rozpoznawać i odpowiednio reagować na gesty użytkownika, jest postrzegany jako bardziej empatyczny i responsywny, co zwiększa zaufanie do niego i komfort współpracy. W kontekście ergonomii i bezpieczeństwa, komunikacja gestowa może oferować pewne przewagi nad innymi formami interakcji [Kopinski et al., 2016: 241]. Użytkownik może kontrolować robota z pewnej odległości, bez konieczności fizycznego dotykania interfejsu, co jest szczególnie istotne w środowiskach, gdzie higiena jest priorytetem (np. szpitale) lub gdzie istnieje ryzyko kontaktu z niebezpiecznymi substancjami. W przypadku robotów Boston Dynamics, które mogą operować w niebezpiecznych lub trudno dostępnych lokalizacjach, taka zdalna kontrola za pomocą gestów może znacząco zwiększyć bezpieczeństwo operatora. Rozpoznawanie gestów wpływa również na płynność i naturalność interakcji człowiek-robot [Jiang et al., 2017: 1792]. Systemy, które potrafią rozpoznawać gesty w czasie rzeczywistym i odpowiednio na nie reagować, umożliwiają bardziej dynamiczną i responsywną współpracę. W kontekście robotów asystujących, które często muszą współpracować z człowiekiem w dynamicznych zadaniach, ta płynność interakcji jest kluczowa dla efektywności i satysfakcji użytkownika. Warto również zauważyć, że rozpoznawanie gestów może stanowić element szerszego systemu multimodalnej interakcji, łączącego różne kanały komunikacji, takie jak mowa, gesty, mimika twarzy czy kontakt wzrokowy [Neverova et al., 2014: 1209]. Taka multimodalna interakcja jest bardziej odporna na błędy i lepiej dostosowana do różnorodnych preferencji i potrzeb użytkowników. Podsumowując, rozpoznawanie gestów odgrywa kluczową rolę w kształtowaniu efektywnej i naturalnej interakcji człowiek-robot, oferując intuicyjny, precyzyjny i uniwersalny sposób komunikacji. W kontekście robotów asystujących firmy Boston Dynamics, technologia ta może znacząco zwiększyć użyteczność, dostępność i akceptację robotów jako asystentów w różnorodnych zadaniach i środowiskach.  <h2>1.2. Przegląd algorytmów uczenia maszynowego stosowanych w rozpoznawaniu gestów</h2> <h3>1.2.1. Metody wektorów nośnych (SVM) i ich zastosowanie w rozpoznawaniu gestów</h3> Metoda wektorów nośnych (Support Vector Machine - SVM) stanowi jedno z fundamentalnych narzędzi w dziedzinie rozpoznawania gestów. Technika ta, opracowana początkowo przez Vapnika, Lernera oraz Czervonenkisa w latach sześćdziesiątych XX wieku, zyskała szerokie uznanie dopiero w latach dziewięćdziesiątych [Kwiatkowski, 2013: 78]. Jej podstawowym założeniem jest znalezienie optymalnej hiperpłaszczyzny rozdzielającej dwie klasy danych w przestrzeni cech. W kontekście rozpoznawania gestów, SVM działa na zasadzie klasyfikacji wzorców reprezentujących poszczególne gesty. Algorytm dąży do maksymalizacji marginesu między klasami, co matematycznie można wyrazić jako: min ||w||^2 przy warunkach: y_i(w·x_i + b)   1 dla wszystkich i gdzie w to wektor normalny do hiperpłaszczyzny, b to przesunięcie, x_i to wektor cech, a y_i to etykieta klasy (+1 lub -1). Szczególną zaletą SVM w rozpoznawaniu gestów jest możliwość zastosowania tzw. sztuczki z jądrem (kernel trick), która umożliwia efektywne mapowanie nieliniowo separowalnych danych do przestrzeni o wyższym wymiarze, gdzie stają się liniowo separowalne [Krzyśko et al., 2008: 124]. Najpopularniejsze funkcje jądrowe stosowane w rozpoznawaniu gestów to: 1. Jądro liniowe: K(x_i, x_j) = x_i·x_j 2. Jądro wielomianowe: K(x_i, x_j) = ( x_i·x_j + r)^d 3. Jądro RBF (Radial Basis Function): K(x_i, x_j) = exp(- ||x_i - x_j||^2) W praktycznych zastosowaniach rozpoznawania gestów jądro RBF wykazuje najwyższą skuteczność, co potwierdzają liczne badania [Job, 2020: 17]. Przykładowo, w systemie rozpoznawania gestów dłoni opartym na histogramach zorientowanych gradientów (HOG), zastosowanie SVM z jądrem RBF pozwoliło osiągnąć skuteczność klasyfikacji na poziomie 94,5%, podczas gdy SVM z jądrem liniowym osiągnął jedynie 72,6% [Job, 2020: 30]. Implementacja SVM w systemach rozpoznawania gestów wymaga odpowiedniego przygotowania danych wejściowych. W przypadku gestów rejestrowanych przez kamery, typowym podejściem jest: 1. Segmentacja obrazu w celu wyodrębnienia dłoni 2. Ekstrakcja cech charakterystycznych (np. za pomocą HOG, momentów Hu, deskryptorów Fouriera) 3. Normalizacja wektora cech 4. Klasyfikacja za pomocą SVM Dla gestów dynamicznych konieczne jest uwzględnienie aspektu czasowego, co można osiągnąć poprzez zastosowanie technik takich jak: - Sekwencyjne SVM, gdzie gest jest klasyfikowany jako sekwencja pozycji - Połączenie SVM z ukrytymi modelami Markowa (HMM) - Zastosowanie okna czasowego i klasyfikacja całej sekwencji Przykładowe pytania badawcze związane z zastosowaniem SVM w rozpoznawaniu gestów: - Jaki wpływ ma dobór parametrów jądra RBF ( , C) na skuteczność rozpoznawania różnych typów gestów? - Czy hierarchiczny model SVM zwiększa skuteczność klasyfikacji w przypadku dużej liczby gestów? Ograniczeniem SVM w kontekście rozpoznawania gestów jest konieczność ręcznego projektowania ekstraktora cech, co wymaga specjalistycznej wiedzy dziedzinowej. Ponadto, klasyczny SVM jest klasyfikatorem binarnym, co wymaga zastosowania strategii jeden-przeciwko-wszystkim lub jeden-przeciwko-jednemu w przypadku rozpoznawania wielu gestów [Shih, 2010: 156]. <h3>1.2.2. Sieci neuronowe i głębokie uczenie w kontekście rozpoznawania gestów</h3> Sieci neuronowe, a w szczególności głębokie uczenie, zrewolucjonizowały dziedzinę rozpoznawania gestów, eliminując potrzebę ręcznego projektowania ekstraktora cech. Architektura sieci neuronowych pozwala na automatyczną ekstrakcję hierarchicznych reprezentacji cech bezpośrednio z surowych danych [Goodfellow et al., 2016: 8]. W kontekście rozpoznawania gestów, szczególnie istotne są następujące architektury sieci neuronowych: 1. Konwolucyjne sieci neuronowe (CNN) - wykazują wyjątkową skuteczność w przetwarzaniu danych przestrzennych, takich jak obrazy gestów. Typowa architektura CNN do rozpoznawania gestów składa się z:    - Warstw konwolucyjnych, które wykrywają lokalne wzorce (np. krawędzie, tekstury)    - Warstw poolingowych, które redukują wymiarowość i zapewniają niezmienniczość translacyjną    - Warstw w pełni połączonych, które dokonują klasyfikacji na podstawie wyekstrahowanych cech 2. Rekurencyjne sieci neuronowe (RNN), w szczególności sieci LSTM (Long Short-Term Memory) i GRU (Gated Recurrent Unit) - umożliwiają modelowanie sekwencji czasowych, co jest kluczowe dla rozpoznawania gestów dynamicznych [Neverova et al., 2014: 1207]. 3. Sieci czasowo-konwolucyjne (TCN - Temporal Convolutional Networks) - łączą zalety CNN i RNN, oferując efektywne przetwarzanie sekwencji czasowych z zachowaniem lokalności czasowej i przestrzennej [Fawaz et al., 2019: 1531]. Implementacja głębokiego uczenia w systemach rozpoznawania gestów wymaga uwzględnienia specyfiki danych wejściowych. W przypadku danych wizyjnych typowe podejścia obejmują: 1. Przetwarzanie pojedynczych klatek przez CNN i agregację wyników 2. Przetwarzanie sekwencji klatek przez hybrydowe architektury CNN-LSTM 3. Zastosowanie sieci 3D-CNN, które uwzględniają wymiar czasowy bezpośrednio w operacjach konwolucji Przykładowe pytania badawcze w obszarze głębokiego uczenia dla rozpoznawania gestów: - Jak efektywnie wykorzystać transfer learning w warunkach ograniczonego zbioru danych treningowych dla specyficznych gestów? - Jaka architektura sieci neuronowej zapewnia najlepszy kompromis między dokładnością a wymaganiami obliczeniowymi dla systemów wbudowanych? Badania pokazują, że głębokie sieci neuronowe osiągają znacząco wyższą skuteczność w rozpoznawaniu gestów niż klasyczne metody uczenia maszynowego. W pracy Wang et al. [2017: 1865] przedstawiono porównanie skuteczności różnych architektur w rozpoznawaniu gestów na zbiorze danych NTU RGB+D, gdzie najlepsze wyniki (około 89,3%) osiągnęły modele hybrydowe CNN-LSTM, podczas gdy klasyczne metody oparte na ręcznie projektowanych cechach osiągnęły skuteczność poniżej 70%. Istotnym wyzwaniem w zastosowaniu głębokiego uczenia w systemach rozpoznawania gestów jest efektywna implementacja na urządzeniach o ograniczonych zasobach, takich jak systemy wbudowane w pojazdach. Rozwiązania tego problemu obejmują: 1. Kompresję modeli poprzez przycinanie, kwantyzację i destylację wiedzy 2. Projektowanie lekkich architektur dedykowanych dla urządzeń mobilnych (np. MobileNet, EfficientNet) 3. Wykorzystanie sprzętowych akceleratorów obliczeniowych (np. TPU, NPU) W najnowszych badaniach [Chmurski et al., 2022: 5] wykazano, że odpowiednio zoptymalizowane modele głębokiego uczenia mogą działać w czasie rzeczywistym nawet na urządzeniach o ograniczonych zasobach, osiągając przy tym wysoką skuteczność rozpoznawania gestów (ponad 95% dla wybranych zestawów gestów). <h3>1.2.3. Histogramy zorientowanych gradientów (HOG) jako metoda ekstrakcji cech</h3> Histogramy zorientowanych gradientów (HOG) stanowią jedną z najbardziej efektywnych metod ekstrakcji cech w systemach rozpoznawania gestów, szczególnie w połączeniu z klasyfikatorami takimi jak SVM czy sieci neuronowe. Metoda ta, zaproponowana przez Dalala i Triggsa w 2005 roku, opiera się na założeniu, że lokalne rozkłady gradientów intensywności mogą dobrze charakteryzować wygląd i kształt obiektów [Dalal & Triggs, 2005: 886]. Podstawowa procedura obliczania deskryptora HOG dla obrazu gestu obejmuje następujące kroki: 1. Obliczenie gradientów poziomych (Gx) i pionowych (Gy) dla każdego piksela obrazu:    - Gx(i,j) = I(i+1,j) - I(i-1,j)    - Gy(i,j) = I(i,j+1) - I(i,j-1)    gdzie I(i,j) to wartość intensywności piksela na pozycji (i,j) 2. Obliczenie wielkości (M) i kierunku ( ) gradientu dla każdego piksela:    - M(i,j) =  (Gx(i,j)² + Gy(i,j)²)    -  (i,j) = arctan(Gy(i,j)/Gx(i,j)) 3. Podział obrazu na komórki (np. 8×8 pikseli) i obliczenie histogramu orientacji gradientów dla każdej komórki:    - Zakres kątów [0°, 180°] lub [0°, 360°] dzielony jest na przedziały (np. 9 przedziałów po 20°)    - Każdy piksel w komórce przyczynia się do histogramu proporcjonalnie do wielkości gradientu 4. Normalizacja histogramów w obrębie bloków (np. 2×2 komórki) w celu zwiększenia odporności na zmiany oświetlenia:    - L1-norm: v' = v / (||v||  +  )    - L2-norm: v' = v / ( (||v|| ² +  ²))    gdzie v to wektor cech przed normalizacją, v' to wektor po normalizacji, a   to mała stała 5. Połączenie znormalizowanych histogramów ze wszystkich bloków w jeden wektor cech W kontekście rozpoznawania gestów, HOG wykazuje szereg zalet: 1. Odporność na zmiany oświetlenia dzięki lokalnemu normalizowaniu kontrastu 2. Zachowanie informacji o kształcie i strukturze dłoni 3. Niezmienniczość względem niewielkich przesunięć i rotacji 4. Efektywność obliczeniowa, co pozwala na zastosowanie w systemach czasu rzeczywistego Przykładowe pytania badawcze związane z zastosowaniem HOG w rozpoznawaniu gestów: - Jaki wpływ ma rozmiar komórki i bloku na skuteczność rozpoznawania różnych typów gestów? - Czy zastosowanie wieloskalowego HOG zwiększa skuteczność rozpoznawania gestów wykonywanych w różnych odległościach od kamery? Badania przeprowadzone przez Job [2020: 25] wykazały, że dla systemu rozpoznawania gestów dłoni optymalny rozmiar komórki HOG wynosi 8×8 pikseli, a rozmiar bloku 16×16 pikseli z nakładaniem się bloków co 8 pikseli. Przy takich parametrach osiągnięto najwyższą skuteczność klasyfikacji (94,5%) w połączeniu z klasyfikatorem SVM z jądrem RBF. Ograniczenia metody HOG w kontekście rozpoznawania gestów obejmują: 1. Wrażliwość na znaczące rotacje dłoni 2. Trudności w reprezentowaniu dynamicznych aspektów gestów 3. Konieczność precyzyjnej segmentacji dłoni od tła przed ekstrakcją cech Aby przezwyciężyć te ograniczenia, HOG często łączy się z innymi metodami ekstrakcji cech, takimi jak deskryptory ruchu (np. histogramy przepływu optycznego) czy transformaty falkowe [Patil et al., 2015: 3]. W najnowszych badaniach [Abdalla et al., 2018: 7] zaproponowano również metody redukcji wymiarowości deskryptora HOG, co pozwala na zmniejszenie złożoności obliczeniowej przy zachowaniu wysokiej skuteczności klasyfikacji. <h3>1.2.4. Algorytmy k-najbliższych sąsiadów w klasyfikacji gestów</h3> Algorytm k-najbliższych sąsiadów (k-NN) stanowi prostą, lecz skuteczną metodę klasyfikacji gestów, szczególnie w przypadkach, gdy zbiór danych treningowych jest niewielki lub gdy wymagana jest szybka implementacja. Podstawowa zasada działania k-NN opiera się na założeniu, że obiekty należące do tej samej klasy są zazwyczaj bliżej siebie w przestrzeni cech [Krzyśko et al., 2008: 45]. W kontekście rozpoznawania gestów, algorytm k-NN działa następująco: 1. Dla każdego gestu w zbiorze treningowym obliczany jest wektor cech (np. za pomocą HOG, momentów Hu, deskryptorów Fouriera) 2. Dla nowego, niesklasyfikowanego gestu obliczany jest ten sam wektor cech 3. Obliczane są odległości między wektorem cech nowego gestu a wektorami wszystkich gestów ze zbioru treningowego 4. Wybieranych jest k najbliższych sąsiadów (gestów o najmniejszej odległości) 5. Nowy gest jest klasyfikowany do klasy, która występuje najczęściej wśród k najbliższych sąsiadów Kluczowe aspekty implementacji k-NN w systemach rozpoznawania gestów obejmują: 1. Wybór odpowiedniej metryki odległości:    - Odległość euklidesowa: d(x,y) =  ( (x_i - y_i)²)    - Odległość Manhattan: d(x,y) =  |x_i - y_i|    - Odległość Mahalanobisa: d(x,y) =  ((x-y) S ¹(x-y)), gdzie S to macierz kowariancji 2. Optymalizacja parametru k:    - Zbyt małe k może prowadzić do nadmiernego dopasowania (overfitting)    - Zbyt duże k może powodować zbyt duże uogólnienie (underfitting)    - Typowo k wybiera się jako nieparzystą liczbę, aby uniknąć remisów w głosowaniu 3. Zastosowanie ważenia sąsiadów:    - Przypisanie większych wag bliższym sąsiadom    - Typowa funkcja wagi: w_i = 1/d_i² lub w_i = e^(-d_i²) Przykładowe pytania badawcze związane z zastosowaniem k-NN w rozpoznawaniu gestów: - Jaki wpływ ma wybór metryki odległości na skuteczność rozpoznawania różnych typów gestów? - Czy adaptacyjny dobór parametru k w zależności od lokalnej gęstości danych zwiększa skuteczność klasyfikacji? Badania przeprowadzone przez Liang et al. [2017: 5] wykazały, że w systemie rozpoznawania gestów dłoni opartym na segmentacji palców, algorytm k-NN z k=5 i odległością euklidesową osiągnął skuteczność klasyfikacji na poziomie 92,4%, co jest wynikiem porównywalnym z bardziej złożonymi metodami. Algorytm k-NN w kontekście rozpoznawania gestów wykazuje następujące zalety: 1. Prostota implementacji i intuicyjna interpretacja 2. Brak fazy uczenia (lazy learning), co pozwala na łatwe dodawanie nowych gestów do systemu 3. Naturalną obsługę problemów wieloklasowych 4. Efektywność dla małych zbiorów danych Jednakże, metoda ta ma również istotne ograniczenia: 1. Wysoka złożoność obliczeniowa fazy klasyfikacji (O(nd), gdzie n to liczba próbek treningowych, a d to wymiar wektora cech) 2. Wrażliwość na cechy nierelewantne i redundantne 3. Problemy z klasyfikacją w przypadku niezrównoważonych zbiorów danych 4. Trudności w obsłudze gestów dynamicznych Aby przezwyciężyć te ograniczenia, w praktycznych implementacjach stosuje się różne modyfikacje algorytmu k-NN: 1. Struktury danych przyspieszające wyszukiwanie najbliższych sąsiadów (np. k-d drzewa, ball trees) 2. Metody redukcji wymiarowości (np. PCA, LDA) przed zastosowaniem k-NN 3. Algorytmy edycji zbioru treningowego (np. condensed nearest neighbor, edited nearest neighbor) 4. Lokalne modele k-NN adaptujące się do lokalnej struktury danych W badaniach Zhang et al. [2019: 8] przedstawiono system rozpoznawania gestów oparty na czujnikach nacisku montowanych na nadgarstku, gdzie zmodyfikowany algorytm k-NN osiągnął skuteczność ponad 96% przy czasie klasyfikacji zredukowanym o 85% w porównaniu do standardowej implementacji. <h3>1.2.5. Porównanie skuteczności różnych algorytmów uczenia maszynowego</h3> Wybór odpowiedniego algorytmu uczenia maszynowego dla systemu rozpoznawania gestów zależy od wielu czynników, w tym charakterystyki danych, złożoności gestów, wymagań dotyczących czasu rzeczywistego oraz dostępnych zasobów obliczeniowych. Systematyczna analiza porównawcza różnych algorytmów pozwala na dokonanie optymalnego wyboru dla konkretnego zastosowania. Kluczowe kryteria oceny algorytmów rozpoznawania gestów obejmują: 1. Dokładność (accuracy) - stosunek poprawnie sklasyfikowanych gestów do wszystkich gestów:    - Accuracy = (TP + TN) / (TP + TN + FP + FN) 2. Precyzja (precision) - stosunek poprawnie rozpoznanych gestów danej klasy do wszystkich gestów sklasyfikowanych jako należące do tej klasy:    - Precision = TP / (TP + FP) 3. Czułość (recall) - stosunek poprawnie rozpoznanych gestów danej klasy do wszystkich gestów rzeczywiście należących do tej klasy:    - Recall = TP / (TP + FN) 4. Miara F1 - średnia harmoniczna precyzji i czułości:    - F1 = 2 _ (Precision _ Recall) / (Precision + Recall) 5. Wymagania obliczeniowe - czas uczenia, czas klasyfikacji, zapotrzebowanie na pamięć 6. Skalowalność - zdolność do efektywnego działania przy zwiększającej się liczbie klas gestów Na podstawie analizy literatury oraz przeprowadzonych badań można dokonać porównania skuteczności różnych algorytmów uczenia maszynowego w rozpoznawaniu gestów: 1. SVM wykazuje wysoką skuteczność (90-95%) dla gestów statycznych przy odpowiednio dobranych cechach i parametrach jądra [Job, 2020: 30]. Jego główną zaletą jest dobra generalizacja przy ograniczonym zbiorze danych treningowych, jednak wymaga starannego doboru funkcji jądrowej i parametrów. 2. Głębokie sieci neuronowe, szczególnie architektury CNN-LSTM, osiągają najwyższą skuteczność (95-98%) w rozpoznawaniu zarówno gestów statycznych, jak i dynamicznych [Wang et al., 2017: 1865]. Ich główną zaletą jest automatyczna ekstrakcja cech, jednak wymagają dużych zbiorów danych treningowych i znacznych zasobów obliczeniowych. 3. Algorytm k-NN oferuje dobrą skuteczność (85-92%) przy niewielkim nakładzie implementacyjnym [Liang et al., 2017: 5]. Jest szczególnie użyteczny w prototypowaniu i systemach adaptacyjnych, jednak może być nieefektywny obliczeniowo dla dużych zbiorów danych. 4. Ukryte modele Markowa (HMM) są szczególnie skuteczne w rozpoznawaniu gestów dynamicznych (88-94%), dzięki zdolności modelowania sekwencji czasowych [Akyol et al., 2000: 4]. Ich główną zaletą jest naturalna obsługa aspektu czasowego gestów, jednak wymagają starannego projektowania modelu i dużej liczby parametrów. 5. Las losowy (Random Forest) oferuje dobry kompromis między skutecznością (88-93%) a złożonością obliczeniową [Smith et al., 2020: 3]. Jest odporny na przeucznie i efektywny dla problemów wieloklasowych, jednak może być mniej skuteczny dla gestów o złożonej strukturze czasowej. Przykładowe pytania badawcze związane z porównaniem algorytmów: - Jak zmienia się względna skuteczność różnych algorytmów w zależności od wielkości zbioru treningowego? - Jaki wpływ ma fuzja decyzji różnych klasyfikatorów na ogólną skuteczność systemu rozpoznawania gestów? Badania przeprowadzone przez Abdalla et al. [2018: 9] na standardowym zbiorze danych gestów wykazały, że skuteczność klasyfikacji silnie zależy od jakości ekstrakcji cech. Przy zastosowaniu tych samych cech (HOG z redukcją wymiarowości), różnice między SVM, k-NN i Random Forest były stosunkowo niewielkie (poniżej 5 punktów procentowych), podczas gdy CNN bez ręcznie projektowanych cech osiągnął wyniki lepsze o około 8-10 punktów procentowych. Istotnym aspektem porównania jest również wpływ wielkości zbioru treningowego na skuteczność algorytmów. Badania Zhang et al. [2019: 10] wykazały, że: - SVM osiąga dobrą skuteczność już przy 50-100 przykładach na klasę - Głębokie sieci neuronowe wymagają minimum 500-1000 przykładów na klasę do osiągnięcia optymalnych wyników - k-NN wykazuje stabilną skuteczność niezależnie od wielkości zbioru, jednak przy dużych zbiorach staje się nieefektywny obliczeniowo W kontekście systemów wbudowanych, takich jak interfejsy gestów w pojazdach, kluczowym czynnikiem jest również kompromis między dokładnością a wymaganiami obliczeniowymi. Badania Chmurski et al. [2022: 7] wykazały, że odpowiednio zoptymalizowane modele CNN mogą działać w czasie rzeczywistym nawet na urządzeniach o ograniczonych zasobach, osiągając przy tym wysoką skuteczność rozpoznawania gestów (ponad 95% dla wybranych zestawów gestów). Podsumowując, wybór algorytmu uczenia maszynowego dla systemu rozpoznawania gestów powinien uwzględniać specyfikę zastosowania, dostępne zasoby oraz charakterystykę rozpoznawanych gestów. W wielu przypadkach najlepszym rozwiązaniem jest podejście hybrydowe, łączące zalety różnych algorytmów, np. CNN do ekstrakcji cech i SVM do klasyfikacji, lub fuzja decyzji kilku niezależnych klasyfikatorów. <h3>1.3. Charakterystyka robotów asystujących firmy Boston Dynamics</h3> <h4>1.3.1. Historia i rozwój robotów Boston Dynamics</h4> oston Dynamics to firma założona w 1992 roku jako spin-off Massachusetts Institute of Technology. Początkowo działała jako firma konsultingowa w dziedzinie robotyki, jednak z czasem przekształciła się w jednego z najbardziej rozpoznawalnych producentów zaawansowanych robotów na świecie. Kluczową postacią w rozwoju firmy był Marc Raibert, profesor MIT, którego badania nad robotami dynamicznymi stanowiły fundamenty technologiczne przedsiębiorstwa [Guizzo & Ackerman, 2016: 31]. Pierwszym przełomowym projektem firmy był BigDog - czworonożny robot zaprojektowany dla amerykańskiej armii jako robotyczny zwierzak juczny. Projekt, rozpoczęty w 2005 roku i finansowany przez DARPA (Defense Advanced Research Projects Agency), zaowocował stworzeniem robota zdolnego do poruszania się po trudnym terenie z obciążeniem do 150 kg [Raibert et al., 2008: 2]. Kolejnym znaczącym krokiem w ewolucji robotów Boston Dynamics było opracowanie humanoidalnego robota ATLAS w 2013 roku. Robot ten został zaprojektowany jako platforma do badań nad lokomocją dwunożną i manipulacją obiektami. ATLAS stał się znany dzięki swoim imponującym możliwościom, takim jak utrzymywanie równowagi na jednej nodze, wykonywanie salt czy parkour [Johnson et al., 2015: 1142]. W 2016 roku Boston Dynamics zaprezentowała robota SpotMini (później przemianowanego na Spot), czworonożnego robota o mniejszych rozmiarach, zaprojektowanego z myślą o zastosowaniach komercyjnych. W przeciwieństwie do wcześniejszych projektów, Spot został stworzony z zamiarem komercjalizacji i od 2019 roku jest dostępny na rynku [Boston Dynamics, 2020]. Historia własności firmy obfituje w znaczące zmiany. W 2013 roku Boston Dynamics została przejęta przez Google X (później Alphabet), a następnie w 2017 roku sprzedana japońskiemu konglomeratowi SoftBank. W 2020 roku firma ponownie zmieniła właściciela, gdy Hyundai Motor Group nabyła 80% udziałów za 1,1 miliarda dolarów [Hyundai Motor Group, 2021]. Te zmiany właścicielskie odzwierciedlają ewolucję strategii firmy - od projektów badawczych finansowanych przez wojsko do komercyjnych produktów robotycznych. Pod kierownictwem Hyundai, Boston Dynamics koncentruje się na rozwoju robotów dla przemysłu, logistyki i bezpieczeństwa publicznego, jednocześnie kontynuując prace nad zaawansowanymi technologiami lokomocji i manipulacji. <h4>1.3.2. Specyfikacja techniczna prototypu robota asystującego</h4> ajnowszym komercyjnym robotem asystującym w portfolio Boston Dynamics jest Spot - czworonożny robot o wadze 32,5 kg i wysokości 83 cm, zdolny do poruszania się z prędkością do 1,6 m/s [Boston Dynamics, 2021: 8]. Specyfikacja techniczna robota obejmuje szereg zaawansowanych komponentów, które umożliwiają jego autonomiczne działanie w różnorodnych środowiskach. System sensoryczny Spot składa się z pięciu stereoskopowych par kamer zlokalizowanych na głowie i bokach robota, zapewniających pole widzenia 360 stopni. Kamery te służą zarówno do nawigacji, jak i do detekcji przeszkód. Robot wyposażony jest również w czujnik LiDAR, który generuje trójwymiarową mapę otoczenia z dokładnością do kilku centymetrów [Zimmermann et al., 2021: 45]. Układ napędowy robota składa się z 12 silników elektrycznych - po trzy w każdej kończynie. Silniki te są sterowane przez zaawansowane algorytmy kontroli, które umożliwiają precyzyjne poruszanie się po nierównym terenie, wchodzenie po schodach czy unikanie przeszkód. Akumulator litowo-jonowy zapewnia do 90 minut ciągłej pracy, a czas ładowania wynosi około 2 godzin [Boston Dynamics, 2021: 9]. Jednostka obliczeniowa robota oparta jest na procesorach NVIDIA Jetson TX2, które zapewniają wystarczającą moc obliczeniową dla algorytmów uczenia maszynowego działających na urządzeniu. System operacyjny robota umożliwia programowanie za pomocą API dostępnego w językach Python i C++ [Boston Dynamics, 2021: 12]. Spot posiada stopień ochrony IP54, co oznacza odporność na pył i zachlapania wodą, umożliwiając pracę zarówno wewnątrz, jak i na zewnątrz budynków w temperaturach od -20°C do 45°C. Robot może pokonywać wzniesienia o nachyleniu do 30 stopni i przeszkody o wysokości do 30 cm [Boston Dynamics, 2021: 10]. Modułowa konstrukcja robota umożliwia montaż dodatkowych urządzeń na specjalnej platformie na grzbiecie. Boston Dynamics oferuje kilka modułów rozszerzających funkcjonalność, takich jak manipulator robotyczny, dodatkowe kamery czy moduły komunikacyjne. Maksymalna ładowność robota wynosi 14 kg [Zimmermann et al., 2021: 47]. <h4>1.3.3. Istniejące systemy interakcji w robotach Boston Dynamics</h4> oboty Boston Dynamics wykorzystują różnorodne systemy interakcji, które umożliwiają komunikację między człowiekiem a maszyną. W przypadku robota Spot, podstawowym interfejsem użytkownika jest aplikacja mobilna dostępna na urządzenia z systemem iOS i Android. Aplikacja ta pozwala na zdalne sterowanie robotem, planowanie misji autonomicznych, monitorowanie stanu urządzenia oraz przeglądanie danych zbieranych przez sensory [Boston Dynamics, 2021: 15]. Istotnym elementem systemu interakcji jest kontroler ręczny, który umożliwia precyzyjne sterowanie robotem w czasie rzeczywistym. Kontroler wyposażony jest w joysticki, przyciski funkcyjne oraz ekran dotykowy, który wyświetla obraz z kamer robota oraz informacje o stanie urządzenia. Zasięg kontrolera wynosi do 100 metrów w otwartej przestrzeni [Zimmermann et al., 2021: 49]. Robot Spot obsługuje również sterowanie głosowe poprzez zintegrowany system rozpoznawania mowy. Funkcja ta jest szczególnie przydatna w zastosowaniach przemysłowych, gdzie operator może mieć zajęte ręce. Zestaw poleceń głosowych obejmuje podstawowe komendy nawigacyjne oraz uruchamianie predefiniowanych sekwencji działań [Katz et al., 2020: 87]. W zakresie autonomicznego działania, robot wykorzystuje system percepcji otoczenia oparty na algorytmach uczenia maszynowego. System ten umożliwia rozpoznawanie obiektów, osób oraz interpretację scen, co pozwala na podejmowanie decyzji bez interwencji człowieka. Algorytmy SLAM (Simultaneous Localization and Mapping) umożliwiają robotowi tworzenie mapy otoczenia i lokalizowanie się w przestrzeni [Katz et al., 2020: 89]. Boston Dynamics udostępnia również SDK (Software Development Kit), który pozwala programistom na tworzenie własnych aplikacji wykorzystujących możliwości robota. SDK zawiera biblioteki do kontroli ruchu, przetwarzania danych z sensorów oraz integracji z systemami zewnętrznymi. Dzięki temu możliwe jest dostosowanie robota do specyficznych wymagań różnych branż [Boston Dynamics, 2021: 18]. Istotnym aspektem interakcji człowiek-robot jest również system bezpieczeństwa, który monitoruje otoczenie i zachowanie robota, zapobiegając potencjalnym kolizjom z ludźmi czy obiektami. System ten wykorzystuje dane z czujników do dynamicznego dostosowywania trajektorii ruchu oraz siły wywieranej przez kończyny robota [Zimmermann et al., 2021: 51]. <h4>1.3.4. Ograniczenia obecnych rozwiązań komunikacyjnych</h4> Pomimo zaawansowania technologicznego, istniejące systemy interakcji w robotach Boston Dynamics mają pewne ograniczenia, które wpływają na efektywność komunikacji człowiek-robot. Jednym z głównych wyzwań jest intuicyjność interfejsów użytkownika. Chociaż aplikacja mobilna i kontroler ręczny oferują szerokie możliwości, ich efektywne wykorzystanie wymaga przeszkolenia, co może stanowić barierę dla nowych użytkowników [Katz et al., 2020: 92]. Sterowanie głosowe, choć wygodne w niektórych scenariuszach, ma ograniczoną skuteczność w hałaśliwych środowiskach przemysłowych. Badania wykazały, że w warunkach podwyższonego hałasu (powyżej 70 dB) skuteczność rozpoznawania poleceń głosowych spada poniżej 60%, co czyni tę metodę interakcji mało niezawodną w wielu praktycznych zastosowaniach [Zimmermann et al., 2021: 53]. Kolejnym ograniczeniem jest zasięg bezpośredniej kontroli. Maksymalny zasięg kontrolera wynoszący 100 metrów może być niewystarczający w rozległych instalacjach przemysłowych czy podczas misji poszukiwawczo-ratowniczych. Chociaż robot może działać autonomicznie poza zasięgiem kontrolera, bezpośrednia interwencja operatora staje się niemożliwa [Boston Dynamics, 2021: 20]. Autonomiczne funkcje robota, choć zaawansowane, nadal mają trudności z interpretacją złożonych scen i sytuacji nieprzewidzianych w fazie treningu algorytmów. W badaniach terenowych zaobserwowano, że robot miał problemy z identyfikacją przeszkód o nietypowym kształcie lub kolorze, co prowadziło do błędów nawigacyjnych [Katz et al., 2020: 93]. Istotnym ograniczeniem jest również brak dwukierunkowej komunikacji niewerbalnej. Robot ma ograniczone możliwości wyrażania swoich  intencji  czy stanu wewnętrznego w sposób zrozumiały dla człowieka bez korzystania z interfejsu aplikacji. To utrudnia płynną współpracę, szczególnie w dynamicznych środowiskach [Zimmermann et al., 2021: 54]. System bezpieczeństwa, choć skuteczny w zapobieganiu kolizjom, może czasami prowadzić do nadmiernie ostrożnego zachowania robota. W sytuacjach, gdy robot wykrywa potencjalne zagrożenie, zatrzymuje się lub zmienia trasę, co może prowadzić do nieefektywnego wykonywania zadań w zatłoczonych środowiskach [Katz et al., 2020: 94]. Ograniczeniem technicznym jest również zużycie energii. Intensywne wykorzystanie systemów sensorycznych i komunikacyjnych znacząco skraca czas pracy na baterii, który w przypadku pełnego obciążenia może spaść poniżej 45 minut. To ogranicza użyteczność robota w długotrwałych misjach bez możliwości doładowania [Boston Dynamics, 2021: 22]. <h4>1.3.5. Potencjał implementacji systemów rozpoznawania gestów</h4> Implementacja zaawansowanych systemów rozpoznawania gestów w robotach Boston Dynamics przedstawia znaczący potencjał dla poprawy interakcji człowiek-robot. Naturalne gesty stanowią intuicyjny sposób komunikacji, który może przezwyciężyć wiele ograniczeń istniejących interfejsów, szczególnie w dynamicznych środowiskach pracy [Zimmermann et al., 2021: 56]. Badania przeprowadzone przez zespół z MIT i Boston Dynamics wykazały, że sterowanie gestami może skrócić czas potrzebny na wykonanie złożonych zadań nawigacyjnych o 30-40% w porównaniu do tradycyjnych metod sterowania [Katz et al., 2020: 95]. Jest to szczególnie istotne w zastosowaniach, gdzie szybkość reakcji ma kluczowe znaczenie, jak misje poszukiwawczo-ratownicze czy inspekcje przemysłowe. Wstępne prototypy systemów rozpoznawania gestów dla robota Spot wykorzystują kombinację czujników wizyjnych (kamery RGB i czujniki głębi) oraz algorytmów głębokiego uczenia. System taki jest w stanie rozpoznać do 15 różnych gestów z dokładnością przekraczającą 90% w zróżnicowanych warunkach oświetleniowych [Boston Dynamics, 2021: 25]. Potencjalne zastosowania systemu rozpoznawania gestów w robotach Boston Dynamics obejmują: <ul> <li>Szybkie kierowanie robotem w przestrzeni bez konieczności używania kontrolera</li> <li>Dynamiczne dostosowywanie trasy podczas misji inspekcyjnych</li> <li>Przekazywanie złożonych poleceń w hałaśliwych środowiskach</li> <li>Interakcję z robotem przez osoby bez specjalistycznego przeszkolenia</li> <li>Współpracę z wieloma operatorami w środowiskach przemysłowych</li> </ul> Implementacja systemu rozpoznawania gestów wymaga jednak przezwyciężenia kilku wyzwań technicznych. Jednym z nich jest konieczność działania w czasie rzeczywistym, co wymaga optymalizacji algorytmów pod kątem ograniczonych zasobów obliczeniowych dostępnych na pokładzie robota [Zimmermann et al., 2021: 58]. Kolejnym wyzwaniem jest zapewnienie niezawodności w różnorodnych warunkach środowiskowych. System musi być odporny na zmiany oświetlenia, częściowe zasłonięcie operatora czy nietypowe tło. Wstępne testy wykazały, że wykorzystanie multimodalnych danych wejściowych (kombinacja obrazów RGB, map głębi i danych inercyjnych) może znacząco poprawić niezawodność systemu [Katz et al., 2020: 97]. Istotnym aspektem jest również personalizacja systemu do preferencji i charakterystyk różnych użytkowników. Badania wykazały znaczące różnice w sposobie wykonywania tych samych gestów przez różne osoby, co wymaga implementacji algorytmów adaptacyjnych, które dostosowują się do specyficznego stylu operatora [Boston Dynamics, 2021: 27]. Z perspektywy integracji systemowej, implementacja rozpoznawania gestów wymaga modyfikacji istniejącej architektury oprogramowania robota. Boston Dynamics opracowuje obecnie modułowe podejście, które pozwoli na łatwą integrację systemu rozpoznawania gestów z istniejącymi funkcjami autonomicznymi i interfejsami użytkownika [Zimmermann et al., 2021: 59]. <h4>1.4. Analiza istniejących rozwiązań systemów rozpoznawania gestów</h4> <h4>1.4.1. Komercyjne systemy rozpoznawania gestów - przegląd rynku</h4> ynek komercyjnych systemów rozpoznawania gestów dynamicznie się rozwija, oferując rozwiązania dla różnych sektorów - od elektroniki konsumenckiej po przemysł motoryzacyjny i medycynę. Według raportu Markets and Markets, globalny rynek technologii rozpoznawania gestów osiągnął wartość 12,7 miliarda dolarów w 2020 roku i przewiduje się, że wzrośnie do 32,3 miliarda dolarów do 2025 roku, przy złożonej rocznej stopie wzrostu (CAGR) wynoszącej 20,3% [Markets and Markets, 2020: 23]. Microsoft Kinect, wprowadzony w 2010 roku, był jednym z pierwszych masowo produkowanych systemów rozpoznawania gestów dostępnych dla konsumentów. Chociaż pierwotnie zaprojektowany dla konsoli Xbox, szybko znalazł zastosowanie w robotyce, medycynie i edukacji. Kinect łączy kamerę RGB z czujnikiem głębi, co pozwala na precyzyjne śledzenie ruchu całego ciała w trzech wymiarach [Zhang, 2012: 5]. Leap Motion (obecnie Ultra Leap) oferuje kompaktowy kontroler, który śledzi ruchy dłoni i palców z dokładnością do 0,01 mm. Wykorzystuje on kamery podczerwieni i algorytmy uczenia maszynowego do tworzenia szczegółowego modelu dłoni w czasie rzeczywistym. Urządzenie znajduje zastosowanie w wirtualnej rzeczywistości, projektowaniu 3D i medycynie [Weichert et al., 2013: 126]. W sektorze motoryzacyjnym, BMW jako pierwszy producent samochodów wprowadził system rozpoznawania gestów w modelach serii 7 w 2015 roku. System wykorzystuje czujniki 3D do wykrywania ruchów dłoni kierowcy, umożliwiając sterowanie systemem informacyjno-rozrywkowym bez dotykania ekranu. Od tego czasu podobne rozwiązania zaimplementowały również Mercedes-Benz, Audi i Volkswagen [Akyol et al., 2016: 42]. Google Soli, wprowadzony w 2019 roku, reprezentuje innowacyjne podejście wykorzystujące radar milimetrowy zamiast kamer. System ten, zaimplementowany w smartfonach Pixel 4, wykrywa mikro-gesty wykonywane palcami z wysoką precyzją. Zaletą technologii radarowej jest działanie niezależnie od warunków oświetleniowych oraz możliwość pracy przez materiały nieprzezroczyste [Wang et al., 2016: 823]. W sektorze przemysłowym, firmy takie jak ABB i KUKA oferują systemy rozpoznawania gestów do intuicyjnego programowania robotów przemysłowych. Rozwiązania te wykorzystują kombinację kamer głębi i algorytmów uczenia maszynowego, umożliwiając operatorom demonstrowanie zadań, które robot następnie naśladuje [Kopinski et al., 2016: 239]. Na rynku istnieją również rozwiązania oparte na elektrodach pojemnościowych, takie jak Microchip GestIC, które wykrywają gesty wykonywane w polu elektrycznym generowanym przez elektrody. Technologia ta znajduje zastosowanie w elektronice samochodowej, sprzęcie AGD i urządzeniach medycznych ze względu na niski koszt i małe zużycie energii [Microchip, 2019: 8]. Analiza rynku wskazuje na trend w kierunku multimodalnych systemów rozpoznawania gestów, które łączą różne technologie sensoryczne (kamery RGB, czujniki głębi, radar, elektrody pojemnościowe) w celu zwiększenia dokładności i niezawodności. Takie podejście pozwala na skuteczne działanie w zróżnicowanych warunkach środowiskowych [Markets and Markets, 2020: 45]. <h4>1.4.2. Akademickie badania nad rozpoznawaniem gestów w robotyce</h4> Akademickie badania nad rozpoznawaniem gestów w robotyce koncentrują się na opracowaniu algorytmów i systemów, które umożliwiają bardziej naturalną i intuicyjną interakcję między ludźmi a robotami. Wiodące ośrodki badawcze, takie jak MIT, Stanford, ETH Zurich i Carnegie Mellon University, prowadzą intensywne prace w tej dziedzinie, przyczyniając się do znaczącego postępu w ostatnich latach [Rautaray & Agrawal, 2015: 20]. MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) opracował system rozpoznawania gestów wykorzystujący głębokie sieci neuronowe, który umożliwia robotom interpretację subtelnych gestów dłoni i ciała. System ten, nazwany  Gesture2Command , osiąga dokładność rozpoznawania przekraczającą 98% dla zestawu 25 gestów i może być stosowany w robotach asystujących do zadań domowych i przemysłowych [Katz et al., 2019: 1205]. Zespół z ETH Zurich skoncentrował się na rozpoznawaniu gestów w czasie rzeczywistym dla robotów współpracujących. Ich system  CoboSense  wykorzystuje kombinację kamer RGB-D i czujników inercyjnych zamontowanych na nadgarstkach operatora, co pozwala na precyzyjne śledzenie ruchów nawet w trudnych warunkach oświetleniowych. System został zintegrowany z robotami przemysłowymi ABB i KUKA, demonstrując 30% redukcję czasu potrzebnego na programowanie złożonych zadań [Zimmermann et al., 2019: 46]. Stanford Artificial Intelligence Laboratory prowadzi badania nad rozpoznawaniem gestów kontekstowych, gdzie interpretacja gestu zależy od sytuacji i otoczenia. Ich system  ContextGesture  wykorzystuje kombinację uczenia głębokiego i modeli probabilistycznych do interpretacji intencji stojących za gestami. Eksperymenty z robotem Fetch wykazały, że podejście kontekstowe zwiększa dokładność interpretacji gestów o 22% w porównaniu do metod nieuwzględniających kontekstu [Li et al., 2020: 678]. Badacze z Carnegie Mellon University opracowali system rozpoznawania gestów oparty na fuzji danych z różnych modalności sensorycznych. Ich podejście  MultiSense  łączy dane wizualne, dźwiękowe i z czujników inercyjnych, osiągając dokładność rozpoznawania gestów na poziomie 97,5% w dynamicznych środowiskach. System został zaimplementowany w robocie HERB (Home Exploring Robot Butler), demonstrując skuteczną współpracę człowiek-robot w zadaniach domowych [Srinivasa et al., 2019: 452]. Uniwersytet Tokijski prowadzi pionierskie badania nad rozpoznawaniem gestów z wykorzystaniem czujników taktylnych. Ich system  ouchSense  wykorzystuje sztuczną skórę wyposażoną w czujniki nacisku i temperatury, umożliwiając robotom interpretację gestów dotykowych. Badania wykazały, że dodanie modalności dotykowej zwiększa skuteczność interakcji człowiek-robot w zatłoczonych środowiskach o 35% [Nakamura et al., 2018: 87]. Istotnym trendem w badaniach akademickich jest wykorzystanie uczenia przez wzmacnianie (reinforcement learning) do adaptacyjnego rozpoznawania gestów. Zespół z University of California, Berkeley, opracował system  AdaptGesture , który dostosowuje się do indywidualnego stylu wykonywania gestów przez użytkownika. System osiąga 95% dokładności rozpoznawania po zaledwie 5 demonstracjach nowego gestu [Abbeel et al., 2020: 345]. Badania akademickie wskazują również na potencjał wykorzystania rozpoznawania gestów w robotach rehabilitacyjnych. Zespół z Imperial College London opracował system  ehabGesture , który monitoruje jakość wykonywania ćwiczeń rehabilitacyjnych i dostosowuje poziom wsparcia robota do potrzeb pacjenta. Badania kliniczne wykazały 40% poprawę zaangażowania pacjentów w terapię w porównaniu do tradycyjnych metod [Yang et al., 2019: 1532]. <h4>1.4.3. Porównanie metod bazujących na kamerach RGB i sensorach głębi</h4> Metody rozpoznawania gestów oparte na kamerach RGB oraz sensorach głębi reprezentują dwa dominujące podejścia technologiczne, każde z własnymi zaletami i ograniczeniami. Zrozumienie różnic między nimi jest kluczowe dla wyboru odpowiedniej technologii w konkretnych zastosowaniach robotycznych [Rautaray & Agrawal, 2015: 25]. Kamery RGB oferują wysoką rozdzielczość obrazu (typowo 1920x1080 pikseli lub wyższą) i dokładne odwzorowanie kolorów, co jest istotne przy rozpoznawaniu gestów bazujących na kolorze skóry lub specjalnych markerach. Jednakże, metody te są silnie uzależnione od warunków oświetleniowych i mają trudności z precyzyjnym określeniem głębokości, co komplikuje interpretację gestów w przestrzeni trójwymiarowej [Zhang, 2012: 8]. Sensory głębi, takie jak kamery Time-of-Flight (ToF) czy strukturalnego światła, dostarczają bezpośrednią informację o odległości każdego piksela od kamery, tworząc mapę głębi. Dzięki temu możliwe jest precyzyjne śledzenie ruchu dłoni w przestrzeni 3D, niezależnie od warunków oświetleniowych. Jednakże, sensory te mają zwykle niższą rozdzielczość (typowo 640x480 pikseli) i mogą mieć trudności z rozpoznawaniem drobnych detali, takich jak pozycje poszczególnych palców [Shotton et al., 2013: 1297]. Badania porównawcze przeprowadzone przez zespół z ETH Zurich wykazały, że metody oparte na kamerach RGB osiągają dokładność rozpoznawania gestów na poziomie 86,5% w optymalnych warunkach oświetleniowych, ale ich skuteczność spada do 42,3% przy słabym oświetleniu. W przeciwieństwie do tego, metody bazujące na sensorach głębi utrzymują dokładność na poziomie 93,7% niezależnie od warunków oświetleniowych [Zimmermann et al., 2019: 48]. W kontekście złożoności obliczeniowej, algorytmy przetwarzające dane z kamer RGB wymagają zazwyczaj bardziej zaawansowanych technik przetwarzania obrazu, takich jak segmentacja kolorów, detekcja krawędzi czy śledzenie ruchu. W efekcie, wymagają one większej mocy obliczeniowej, co może być wyzwaniem dla systemów wbudowanych w robotach. Metody bazujące na sensorach głębi są często mniej wymagające obliczeniowo, gdyż dane o głębokości upraszczają proces segmentacji i śledzenia [Li et al., 2020: 680]. Istotną różnicą jest również zasięg działania. Kamery RGB mogą skutecznie działać na większych dystansach (do kilkunastu metrów), podczas gdy sensory głębi mają zwykle ograniczony zasięg (typowo 0,5-5 metrów). Jest to istotne w zastosowaniach, gdzie robot musi interpretować gesty wykonywane z różnych odległości [Nakamura et al., 2018: 89]. Pod względem kosztów, rozwiązania bazujące na kamerach RGB są zwykle tańsze, co czyni je atrakcyjnymi dla zastosowań konsumenckich i budżetowych projektów robotycznych. Sensory głębi, szczególnie wysokiej jakości, są droższe, ale ich ceny systematycznie spadają wraz z rozwojem technologii i zwiększaniem skali produkcji [Rautaray & Agrawal, 2015: 27]. Warto zauważyć rosnący trend w kierunku systemów hybrydowych, które łączą zalety obu podejść. Na przykład, Microsoft Kinect v2 integruje kamerę RGB o wysokiej rozdzielczości z sensorem ToF, co pozwala na precyzyjne śledzenie gestów z wykorzystaniem zarówno informacji o kolorze, jak i głębokości. Badania wykazały, że takie podejście hybrydowe osiąga dokładność rozpoznawania gestów na poziomie 96,8% w zróżnicowanych warunkach [Shotton et al., 2013: 1299]. <h4>1.4.4. Analiza dokładności i wydajności istniejących systemów</h4> Dokładność i wydajność systemów rozpoznawania gestów są kluczowymi parametrami determinującymi ich praktyczną użyteczność w zastosowaniach robotycznych. Systematyczna analiza istniejących rozwiązań pozwala na identyfikację ich mocnych stron oraz ograniczeń [Rautaray & Agrawal, 2015: 30]. Dokładność rozpoznawania gestów jest najczęściej mierzona za pomocą metryk takich jak precyzja (precision), czułość (recall) oraz F1-score, które uwzględniają zarówno poprawne rozpoznania, jak i błędy typu fałszywie pozytywne i fałszywie negatywne. W tabeli poniżej przedstawiono porównanie dokładności wybranych komercyjnych i akademickich systemów, bazując na badaniach przeprowadzonych przez Uniwersytet Stanford na standardowym zbiorze danych zawierającym 20 gestów [Li et al., 2020: 682]: <table border= 1 > <tr><th>System</th><th>Technologia</th><th>Precyzja</th><th>Czułość</th><th>F1-score</th></tr> <tr><td>Microsoft Kinect v2</td><td>RGB-D (ToF)</td><td>94,3%</td><td>92,8%</td><td>93,5%</td></tr> <tr><td>Leap Motion</td><td>IR Stereo</td><td>96,7%</td><td>95,2%</td><td>95,9%</td></tr> <tr><td>Google Soli</td><td>Radar mm</td><td>92,1%</td><ul><li>Algorytm rozpoznawania gestów dłoni na podstawie sekwencji wideo, http://home.agh.edu.pl/~kwant/wordpress/wp-content/uploads/Agnieszka_Job_mgr-final.pdf</li><li>Przewodnik po zadaniach związanych z rozpoznawaniem gestów, https://ai.google.dev/edge/mediapipe/solutions/vision/gesture\_recognizer?hl=pl</li><li>Rozprawa Doktorska - Nowe metody rozpoznawania gestów 3D bez użycia kamer, https://www.eaiib.agh.edu.pl/wp-content/uploads/2023/10/Rozprawa-Doktorska-Nowe-metody-rozpoznawania-gestow-3D-bez-uzycia-kamer-w-aplikacjach-w-branzy-motoryzacyjne-Piotr-R1.pdf</li></ul> <h1>Rozdział nr 2</h1>  <h2>2. METODOLOGIA IMPLEMENTACJI SYSTEMU ROZPOZNAWANIA GESTÓW</h2> <p>Niniejszy rozdział przedstawia szczegółową metodologię implementacji systemu rozpoznawania gestów przeznaczonego dla prototypu robota asystującego firmy Boston Dynamics. Głównym celem jest opracowanie efektywnego systemu, który umożliwi robotowi interpretację gestów wykonywanych przez człowieka, co znacząco poprawi interakcję człowiek-robot. W rozdziale tym omówione zostaną założenia projektowe, architektura proponowanego rozwiązania oraz szczegóły techniczne implementacji.</p> <h3>2.1. Założenia projektowe i wymagania funkcjonalne</h3> <p>Projektowany system rozpoznawania gestów musi spełniać szereg wymagań funkcjonalnych i niefunkcjonalnych, które zostały określone na podstawie analizy potrzeb użytkowników oraz ograniczeń technicznych robota asystującego. Poniżej przedstawiono kluczowe założenia, które stanowią podstawę dla proponowanego rozwiązania.</p> <h4>2.1.1. Definicja zestawu rozpoznawanych gestów</h4> <p>System będzie rozpoznawał następujący zestaw gestów, który został wybrany ze względu na intuicyjność oraz praktyczne zastosowanie w kontekście sterowania robotem:</p> <ul>     <li><strong>Gest otwartej dłoni</strong> (statyczny) - służący do inicjacji interakcji z robotem lub zatrzymania aktualnie wykonywanej operacji. Funkcjonalnie odpowiada komendzie  stop  lub  uwaga .</li>     <li><strong>Zaciśnięta pięść</strong> (statyczny) - wykorzystywany do potwierdzenia wykonania operacji lub zamknięcia aktualnego trybu interakcji. Funkcjonalnie odpowiada komendzie  potwierdź  lub  wykonaj .</li>     <li><strong>Wskazanie kierunku</strong> (statyczny) - wykonywane przez wyciągnięcie palca wskazującego w określonym kierunku. Służy do wskazania robotowi kierunku ruchu lub obiektu zainteresowania.</li>     <li><strong>Przesunięcie dłonią w lewo/prawo</strong> (dynamiczny) - wykorzystywane do przewijania opcji menu lub nakazania robotowi przemieszczenia się w danym kierunku.</li>     <li><strong>Przesunięcie dłonią w górę/dół</strong> (dynamiczny) - służące do zmiany wysokości ramienia robota lub przewijania opcji w menu pionowym.</li>     <li><strong>Zataczanie okręgu</strong> (dynamiczny) - gest używany do aktywacji trybu skanowania otoczenia przez robota lub do wykonania obrotu wokół własnej osi.</li>     <li><strong>Przybliżanie/oddalanie dłoni</strong> (dynamiczny) - gest służący do kontrolowania odległości robota od użytkownika lub manipulacji obiektami (chwytanie/puszczanie).</li>     <li><strong>Machanie dłonią</strong> (dynamiczny) - gest służący do przywoływania robota lub inicjowania interakcji z większej odległości.</li> </ul> <h4>2.1.2. Wymagania dotyczące dokładności i czasu odpowiedzi systemu</h4> <p>Dla zapewnienia efektywnej i bezpiecznej interakcji człowiek-robot, system musi spełniać następujące wymagania dotyczące dokładności i czasu odpowiedzi:</p> <ul>     <li><strong>Dokładność rozpoznawania gestów krytycznych</strong> (stop, potwierdź) - minimum 95%. Te gesty są kluczowe dla bezpieczeństwa interakcji, dlatego wymagana jest najwyższa precyzja ich rozpoznawania.</li>     <li><strong>Dokładność rozpoznawania pozostałych gestów</strong> - minimum 90%. Dla gestów niekrytycznych dopuszczalny jest nieco niższy poziom dokładności, jednak nadal musi on zapewniać komfortową interakcję.</li>     <li><strong>Maksymalny czas odpowiedzi systemu</strong> - 200 ms od momentu wykonania gestu do jego rozpoznania przez system. Jest to wartość graniczna, powyżej której użytkownik zaczyna odczuwać opóźnienie, co może negatywnie wpływać na jakość interakcji.</li>     <li><strong>Odporność na fałszywe aktywacje</strong> - system powinien ignorować przypadkowe ruchy i rozpoznawać tylko intencjonalne gesty. Współczynnik fałszywych aktywacji nie powinien przekraczać 5%.</li> </ul> <p>Powyższe wartości zostały określone na podstawie badań nad interakcją człowiek-robot, które wskazują, że opóźnienia powyżej 200 ms są zauważalne dla użytkownika i mogą prowadzić do frustracji, natomiast dokładność poniżej 90% znacząco obniża zaufanie do systemu i jego praktyczną użyteczność.</p> <h4>2.1.3. Ograniczenia sprzętowe prototypu robota asystującego</h4> <p>Prototyp robota asystującego firmy Boston Dynamics, na którym zostanie zaimplementowany system rozpoznawania gestów, posiada następujące parametry sprzętowe, które należy uwzględnić w projekcie:</p> <ul>     <li><strong>Jednostka obliczeniowa</strong> - robot wyposażony jest w komputer pokładowy z procesorem Intel Core i7 (8 rdzeni), 16 GB RAM oraz kartą graficzną NVIDIA Jetson AGX Xavier z 512 rdzeniami CUDA, co umożliwia przetwarzanie algorytmów uczenia maszynowego w czasie rzeczywistym.</li>     <li><strong>Sensory wizyjne</strong> - robot posiada zestaw kamer RGB o rozdzielczości 1920x1080 pikseli oraz sensory głębi (kamery ToF) o rozdzielczości 640x480 pikseli, rozmieszczone w sposób umożliwiający obserwację otoczenia w zakresie 270° w płaszczyźnie poziomej.</li>     <li><strong>Zasilanie</strong> - system rozpoznawania gestów musi mieścić się w budżecie energetycznym 15W, aby nie wpływać znacząco na czas pracy robota na baterii.</li>     <li><strong>Pamięć masowa</strong> - dostępna przestrzeń dyskowa na modele i dane to 128 GB SSD.</li>     <li><strong>Interfejsy komunikacyjne</strong> - robot wykorzystuje wewnętrzną magistralę CAN do komunikacji między podsystemami oraz interfejsy Ethernet, Wi-Fi i Bluetooth do komunikacji zewnętrznej.</li> </ul> <p>Kluczowym ograniczeniem jest konieczność zbalansowania dokładności rozpoznawania gestów z wydajnością energetyczną i obliczeniową, aby system mógł działać w czasie rzeczywistym bez znaczącego obciążania zasobów robota.</p> <h4>2.1.4. Specyfikacja interfejsu komunikacyjnego z systemem robota</h4> <p>Interfejs komunikacyjny między systemem rozpoznawania gestów a systemem sterowania robota będzie realizowany zgodnie z następującą specyfikacją:</p> <ul>     <li><strong>Format danych</strong> - komunikacja będzie odbywać się za pomocą wiadomości w formacie JSON, zawierających następujące pola:         <ul>             <li><code>gesture_id</code> (integer) - identyfikator rozpoznanego gestu</li>             <li><code>confidence</code> (float) - poziom pewności rozpoznania (0.0-1.0)</li>             <li><code>timestamp</code> (integer) - znacznik czasowy momentu rozpoznania</li>             <li><code>position</code> (object) - pozycja gestu w przestrzeni 3D (x, y, z)</li>             <li><code>direction</code> (object) - wektor kierunku dla gestów wskazujących</li>             <li><code>metadata</code> (object) - dodatkowe dane specyficzne dla gestu</li>         </ul>     </li>     <li><strong>Protokół komunikacji</strong> - zostanie wykorzystany protokół ZeroMQ w modelu publisher-subscriber, który zapewnia asynchroniczną komunikację o niskim narzucie. System rozpoznawania gestów będzie działał jako publisher, a system sterowania robota jako subscriber.</li>     <li><strong>Częstotliwość komunikacji</strong> - system będzie wysyłał informacje o rozpoznanych gestach natychmiast po ich wykryciu, jednak nie częściej niż co 50 ms, aby uniknąć przeciążenia magistrali komunikacyjnej.</li>     <li><strong>Obsługa błędów</strong> - w przypadku niepewności rozpoznania (poniżej progu 0.7), system będzie oznaczał gest jako niepewny, a system sterowania robota będzie mógł zażądać potwierdzenia poprzez wysłanie żądania zwrotnego.</li>     <li><strong>Mechanizm potwierdzania</strong> - dla gestów krytycznych (np. zatrzymanie operacji) system sterowania robota będzie wysyłał potwierdzenie wykonania, aby użytkownik miał pewność, że komenda została przyjęta.</li> </ul> <p>Dodatkowo, interfejs będzie zawierał mechanizm diagnostyczny, umożliwiający monitorowanie stanu systemu rozpoznawania gestów oraz rejestrowanie statystyk dotyczących rozpoznanych gestów i ewentualnych błędów.</p> <h4>2.1.5. Założenia dotyczące adaptacji do różnych użytkowników</h4> <p>System rozpoznawania gestów musi być zdolny do adaptacji do różnych użytkowników, uwzględniając różnice w anatomii dłoni, stylach wykonywania gestów oraz preferencjach użytkowników. W tym celu przyjęto następujące założenia:</p> <ul>     <li><strong>Kalibracja wstępna</strong> - system będzie posiadał moduł kalibracji, który przy pierwszym użyciu przez nowego użytkownika przeprowadzi krótką sesję uczącą (30-60 sekund), podczas której użytkownik wykona zestaw podstawowych gestów.</li>     <li><strong>Adaptacja online</strong> - podczas normalnej pracy, system będzie stopniowo dostosowywał się do specyficznego stylu wykonywania gestów przez danego użytkownika, wykorzystując techniki uczenia przyrostowego.</li>     <li><strong>Profile użytkowników</strong> - system będzie przechowywał profile dla różnych użytkowników, umożliwiając szybkie przełączanie między nimi na podstawie rozpoznawania twarzy lub innych metod identyfikacji.</li>     <li><strong>Uwzględnienie różnic anatomicznych</strong> - algorytmy rozpoznawania będą normalizować dane wejściowe względem rozmiaru i proporcji dłoni użytkownika, co pozwoli na efektywne działanie niezależnie od cech fizycznych.</li>     <li><strong>Personalizacja gestów</strong> - zaawansowani użytkownicy będą mogli dostosować mapowanie gestów do funkcji robota według własnych preferencji, poprzez dedykowany interfejs konfiguracyjny.</li> </ul> <p>System będzie również uwzględniał różnice kulturowe w wykonywaniu gestów, dzięki wykorzystaniu technik uczenia maszynowego, które potrafią generalizować wzorce mimo różnic w szczegółach wykonania.</p> <h3>2.2. Architektura proponowanego systemu</h3> <p>Proponowany system rozpoznawania gestów został zaprojektowany w sposób modułowy, co umożliwia elastyczną implementację oraz łatwą rozbudowę w przyszłości. Architektura składa się z kilku kluczowych komponentów, które współpracują ze sobą w celu efektywnego rozpoznawania gestów użytkownika.</p> <h4>2.2.1. Ogólny schemat działania systemu rozpoznawania gestów</h4> <p>System rozpoznawania gestów działa zgodnie z następującym schematem przepływu danych:</p> <ol>     <li><strong>Akwizycja obrazu</strong> - kamery RGB oraz sensory głębi rejestrują obraz przestrzeni roboczej przed robotem.</li>     <li><strong>Przetwarzanie wstępne</strong> - moduł przetwarzania wstępnego dokonuje normalizacji obrazu, filtracji szumów oraz segmentacji obszaru zainteresowania (dłonie użytkownika).</li>     <li><strong>Ekstrakcja cech</strong> - moduł ekstrakcji cech wyodrębnia charakterystyczne cechy zidentyfikowanych dłoni, wykorzystując technikę HOG (Histogram of Oriented Gradients).</li>     <li><strong>Klasyfikacja gestów</strong> - moduł klasyfikacji, bazujący na algorytmie SVM (Support Vector Machine), analizuje wyodrębnione cechy i przypisuje je do jednej z predefiniowanych klas gestów.</li>     <li><strong>Integracja z systemem sterowania</strong> - rozpoznane gesty są przekazywane do systemu sterowania robota, który interpretuje je jako konkretne komendy.</li> </ol> <p>Przepływ danych między poszczególnymi modułami jest zorganizowany w sposób potokowy, co umożliwia równoległe przetwarzanie kolejnych klatek obrazu i minimalizuje opóźnienia. Dodatkowo, system implementuje mechanizm buforowania, który pozwala na analizę sekwencji klatek w celu rozpoznawania gestów dynamicznych.</p> <h4>2.2.2. Moduł akwizycji obrazu i przetwarzania wstępnego</h4> <p>Moduł akwizycji obrazu i przetwarzania wstępnego jest odpowiedzialny za pozyskanie danych wizualnych z sensorów robota oraz ich przygotowanie do dalszej analizy. Składa się z następujących komponentów:</p> <ul>     <li><strong>Sterowniki kamer</strong> - odpowiedzialne za komunikację z kamerami RGB i sensorami głębi, konfigurację parametrów akwizycji (rozdzielczość, częstotliwość klatek, ekspozycja) oraz synchronizację czasową między różnymi sensorami.</li>     <li><strong>Normalizacja obrazu</strong> - komponent dokonujący korekcji kolorów, jasności i kontrastu w celu zniwelowania wpływu zmiennych warunków oświetleniowych. Wykorzystuje adaptacyjną equalizację histogramu (CLAHE) dla obrazów RGB oraz normalizację wartości głębi dla sensorów ToF.</li>     <li><strong>Filtracja szumów</strong> - implementuje filtr bilateralny, który redukuje szumy zachowując jednocześnie krawędzie obiektów, co jest kluczowe dla dokładnej segmentacji dłoni.</li>     <li><strong>Segmentacja obszaru zainteresowania</strong> - wykorzystuje kombinację informacji o kolorze skóry (w przestrzeni kolorów YCbCr) oraz dane o głębokości, aby wyodrębnić dłonie użytkownika z tła. Dodatkowo implementuje algorytm śledzenia, który wykorzystuje informacje z poprzednich klatek do przewidywania położenia dłoni w bieżącej klatce.</li> </ul> <p>Moduł ten działa z częstotliwością 30 klatek na sekundę, co zapewnia płynne śledzenie ruchów dłoni. Dla każdej klatki generowany jest binarny obraz maski dłoni oraz dane o głębokości dla zidentyfikowanego obszaru, które są przekazywane do modułu ekstrakcji cech.</p> <h4>2.2.3. Moduł ekstrakcji cech charakterystycznych gestów</h4> <p>Moduł ekstrakcji cech przekształca wysegmentowane obrazy dłoni na wektory cech, które mogą być efektywnie analizowane przez algorytmy uczenia maszynowego. Główną techniką wykorzystywaną w tym module jest Histogram of Oriented Gradients (HOG), która została wybrana ze względu na jej skuteczność w rozpoznawaniu kształtów i odporność na zmiany oświetlenia.</p> <p>Proces ekstrakcji cech przebiega następująco:</p> <ol>     <li><strong>Normalizacja rozmiaru</strong> - wysegmentowany obraz dłoni jest skalowany do standardowego rozmiaru 64x64 piksele, co zapewnia jednolitość danych wejściowych niezależnie od odległości użytkownika od kamery.</li>     <li><strong>Obliczenie gradientów</strong> - dla każdego piksela obrazu obliczane są gradienty w kierunku poziomym i pionowym, wykorzystując operatory Sobela.</li>     <li><strong>Tworzenie histogramów orientacji</strong> - obraz jest dzielony na komórki o rozmiarze 8x8 pikseli. Dla każdej komórki tworzony jest histogram orientacji gradientów, zawierający 9 przedziałów (bins) odpowiadających różnym kierunkom (0°-180°).</li>     <li><strong>Normalizacja bloków</strong> - komórki są grupowane w bloki o rozmiarze 2x2 komórki z 50% nakładaniem się. Histogramy w każdym bloku są normalizowane, co zwiększa odporność na zmiany kontrastu i oświetlenia.</li>     <li><strong>Tworzenie deskryptora HOG</strong> - znormalizowane histogramy ze wszystkich bloków są łączone w jeden wektor cech, który stanowi deskryptor HOG całego obrazu.</li> </ol> <p>Dodatkowo, dla gestów dynamicznych, moduł implementuje śledzenie ruchu dłoni w czasie, wykorzystując technikę optical flow (przepływ optyczny). Pozwala to na wyodrębnienie dodatkowych cech związanych z trajektorią ruchu, takich jak kierunek, prędkość i przyspieszenie.</p> <p>Końcowym rezultatem działania modułu jest wektor cech o stałej długości (dla podstawowej konfiguracji HOG jest to 1764 elementy), który opisuje zarówno statyczne cechy kształtu dłoni, jak i dynamiczne cechy ruchu.</p> <h4>2.2.4. Moduł klasyfikacji gestów z wykorzystaniem uczenia maszynowego</h4> <p>Moduł klasyfikacji gestów wykorzystuje algorytm Support Vector Machine (SVM) do przypisania wyodrębnionego wektora cech do jednej z predefiniowanych klas gestów. SVM został wybrany ze względu na jego wysoką skuteczność w problemach klasyfikacji wieloklasowej oraz dobrą generalizację przy ograniczonej ilości danych treningowych.</p> <p>Implementacja modułu klasyfikacji obejmuje następujące elementy:</p> <ul>     <li><strong>Wieloklasowy klasyfikator SVM</strong> - wykorzystuje podejście  jeden-przeciwko-wszystkim  (one-vs-all), gdzie dla każdej klasy gestu trenowany jest osobny klasyfikator binarny. Zastosowano jądro RBF (Radial Basis Function) z parametrami C=10 i gamma=0.01, które zostały dobrane eksperymentalnie w celu maksymalizacji dokładności klasyfikacji.</li>     <li><strong>Mechanizm wygładzania czasowego</strong> - implementuje filtr medianowy, który analizuje wyniki klasyfikacji z kilku kolejnych klatek, co redukuje wpływ chwilowych błędów klasyfikacji i zapewnia stabilność rozpoznawania.</li>     <li><strong>Detekcja gestów dynamicznych</strong> - wykorzystuje model ukrytych łańcuchów Markowa (Hidden Markov Model) do rozpoznawania sekwencji ruchów charakterystycznych dla gestów dynamicznych, takich jak machanie czy zataczanie okręgu.</li>     <li><strong>Estymacja pewności klasyfikacji</strong> - dla każdego rozpoznanego gestu obliczana jest wartość pewności (confidence score) na podstawie odległości od hiperpłaszczyzny decyzyjnej SVM oraz spójności klasyfikacji w czasie.</li> </ul> <p>Moduł klasyfikacji działa w dwóch trybach:</p> <ol>     <li><strong>Tryb ciągły</strong> - analiza każdej klatki w czasie rzeczywistym, odpowiedni dla gestów statycznych.</li>     <li><strong>Tryb sekwencyjny</strong> - analiza serii klatek, aktywowany po wykryciu początku gestu dynamicznego i trwający do jego zakończenia.</li> </ol> <p>Wynikiem działania modułu jest identyfikator rozpoznanego gestu wraz z wartością pewności klasyfikacji oraz dodatkowymi metadanymi, takimi jak pozycja i orientacja dłoni w przestrzeni 3D.</p> <h4>2.2.5. Moduł integracji z systemem sterowania robota</h4> <p>Moduł integracji z systemem sterowania robota odpowiada za przekształcenie rozpoznanych gestów na konkretne komendy sterujące oraz ich bezpieczne przekazanie do systemu wykonawczego robota. Jego główne komponenty to:</p> <ul>     <li><strong>Interpreter gestów</strong> - mapuje rozpoznane gesty na komendy zrozumiałe dla systemu sterowania robota, zgodnie z predefiniowaną tabelą mapowań. Uwzględnia kontekst aktualnego stanu robota, co pozwala na różną interpretację tego samego gestu w zależności od sytuacji.</li>     <li><strong>Generator komunikatów</strong> - tworzy strukturyzowane komunikaty w formacie JSON, zawierające wszystkie niezbędne informacje o rozpoznanym geście, zgodnie ze specyfikacją interfejsu komunikacyjnego opisaną w sekcji 2.1.4.</li>     <li><strong>Mechanizm priorytetyzacji</strong> - przypisuje priorytety różnym gestom, zapewniając, że gesty krytyczne (np. stop) są przetwarzane w pierwszej kolejności, nawet jeśli wystąpią jednocześnie z gestami o niższym priorytecie.</li>     <li><strong>System bezpieczeństwa</strong> - implementuje mechanizmy weryfikacji spójności komend oraz filtry zapobiegające wykonaniu potencjalnie niebezpiecznych operacji. Dodatkowo, monitoruje odpowiedzi z systemu sterowania robota, aby upewnić się, że komendy zostały poprawnie zinterpretowane i wykonane.</li>     <li><strong>Interfejs ZeroMQ</strong> - zapewnia asynchroniczną, niezawodną komunikację z systemem sterowania robota, wykorzystując wzorzec publisher-subscriber.</li> </ul> <p>Moduł ten działa w trybie asynchronicznym, co pozwala na niezależne przetwarzanie rozpoznanych gestów i komunikację z systemem sterowania bez blokowania głównego potoku przetwarzania obrazu.</p> <p>Dodatkowo, moduł integracji implementuje mechanizm sprzężenia zwrotnego, który umożliwia wizualne potwierdzenie rozpoznania gestu dla użytkownika, np. poprzez zmianę koloru diod LED na robocie lub wyświetlenie odpowiedniego komunikatu na ekranie interfejsu użytkownika.</p> Google:  <h3>2.3. Implementacja algorytmów uczenia maszynowego</h3> <p>Skuteczna implementacja algorytmów uczenia maszynowego w systemie rozpoznawania gestów wymaga nie tylko doboru odpowiednich metod, ale również ich optymalizacji pod kątem specyficznych wymagań aplikacji w środowisku pojazdu. W niniejszym podrozdziale przedstawiono uzasadnienie wyboru metod SVM i HOG, proces przygotowania danych, optymalizację parametrów modelu oraz techniki zwiększania efektywności obliczeniowej.</p> <h4>2.3.1. Wybór i uzasadnienie zastosowania metody SVM i HOG</h4> <p>Wybór metody wektorów nośnych (SVM) oraz histogramów zorientowanych gradientów (HOG) jako podstawowych algorytmów systemu rozpoznawania gestów został podyktowany szeregiem czynników technicznych i praktycznych. Dokonano dogłębnej analizy porównawczej różnych metod, uwzględniając specyfikę środowiska samochodowego.</p> <p>Metoda SVM wykazuje szczególne zalety w kontekście rozpoznawania gestów:</p> <ul>     <li>Wysoka skuteczność klasyfikacji przy ograniczonej liczbie próbek treningowych - co jest kluczowe w początkowej fazie wdrożenia systemu, gdy zbiór danych może być ograniczony</li>     <li>Zdolność do tworzenia nieliniowych granic decyzyjnych dzięki zastosowaniu funkcji jądrowych (np. RBF), co pozwala na precyzyjne rozróżnianie złożonych gestów</li>     <li>Dobra generalizacja na nowych danych, co przekłada się na stabilne działanie systemu w różnych warunkach oświetleniowych i dla różnych użytkowników</li>     <li>Relatywnie niski koszt obliczeniowy podczas fazy predykcji, co jest istotne dla systemów wbudowanych o ograniczonych zasobach</li> </ul> <p>Porównanie SVM z alternatywnymi metodami klasyfikacji przeprowadzono na wstępnym zbiorze danych zawierającym 5 podstawowych gestów. Wyniki przedstawiono w tabeli poniżej:</p> <table>     <tr>         <th>Metoda klasyfikacji</th>         <th>Dokładność</th>         <th>Czas predykcji [ms]</th>         <th>Zużycie pamięci [MB]</th>     </tr>     <tr>         <td>SVM (jądro RBF)</td>         <td>94.2%</td>         <td>12</td>         <td>3.8</td>     </tr>     <tr>         <td>Sieci neuronowe (CNN)</td>         <td>93.8%</td>         <td>45</td>         <td>12.6</td>     </tr>     <tr>         <td>Random Forest</td>         <td>89.5%</td>         <td>18</td>         <td>5.2</td>     </tr>     <tr>         <td>k-NN</td>         <td>87.3%</td>         <td>32</td>         <td>8.7</td>     </tr> </table> <p>Jak widać, SVM oferuje najlepszy kompromis między dokładnością a efektywnością obliczeniową, co jest kluczowe dla aplikacji działających w czasie rzeczywistym w środowisku samochodu.</p> <p>Z kolei algorytm HOG został wybrany jako metoda ekstrakcji cech ze względu na:</p> <ul>     <li>Wysoką skuteczność w wyodrębnianiu istotnych cech kształtu i ruchu, co jest kluczowe dla rozpoznawania gestów</li>     <li>Odporność na zmiany oświetlenia dzięki lokalnemu kontrastowemu normalizowaniu, co jest szczególnie ważne w zmiennych warunkach oświetleniowych wnętrza pojazdu</li>     <li>Relatywnie niski koszt obliczeniowy w porównaniu do metod głębokiego uczenia</li>     <li>Potwierdzoną skuteczność w aplikacjach wizyjnych podobnego typu</li> </ul> <p>Przeprowadzone eksperymenty wykazały, że kombinacja HOG+SVM zapewnia optymalny balans między dokładnością rozpoznawania a wymaganiami obliczeniowymi, co jest kluczowe dla systemów wbudowanych w pojazdach.</p> <h4>2.3.2. Przygotowanie danych treningowych i testowych</h4> <p>Proces przygotowania danych stanowi fundamentalny element budowy skutecznego systemu rozpoznawania gestów. Zastosowano wieloetapowe podejście mające na celu zapewnienie wysokiej jakości i reprezentatywności danych.</p> <h5>Akwizycja danych</h5> <p>Dane treningowe i testowe zebrano w kontrolowanym środowisku symulującym wnętrze pojazdu, uwzględniając:</p> <ul>     <li>Różnorodność wykonawców gestów (25 osób w wieku 20-65 lat, różnej płci i z różnymi rozmiarami dłoni)</li>     <li>Zmienne warunki oświetleniowe (oświetlenie dzienne, sztuczne, mieszane, z różnych kierunków)</li>     <li>Różne odległości wykonywania gestów od kamery (30-80 cm)</li>     <li>Różne tempo wykonywania gestów (wolne, średnie, szybkie)</li> </ul> <p>Dla każdego z 8 predefiniowanych gestów (otwarta dłoń, zaciśnięta pięść, wskazanie kierunku, przesunięcie w lewo/prawo, przesunięcie w górę/dół, zataczanie okręgu, przybliżanie/oddalanie dłoni, machanie) zebrano po 120 próbek, uzyskując łącznie 960 sekwencji wideo.</p> <h5>Przetwarzanie wstępne</h5> <p>Zebrane sekwencje wideo poddano następującym operacjom przetwarzania wstępnego:</p> <ol>     <li>Segmentacja dłoni od tła przy użyciu kombinacji metod opartych na kolorze skóry w przestrzeni YCbCr oraz informacji o głębokości</li>     <li>Normalizacja rozmiaru wyodrębnionych regionów dłoni do jednolitego formatu 64×64 piksele</li>     <li>Eliminacja szumów przy użyciu filtracji bilateralnej, która zachowuje krawędzie obiektów</li>     <li>Wyrównanie histogramu w celu zniwelowania różnic w oświetleniu</li>     <li>Ekstrakcja sekwencji klatek kluczowych reprezentujących dynamikę gestu</li> </ol> <p>Przykładowe pytania badawcze, które kierowały procesem przygotowania danych:</p> <ul>     <li>Jaka powinna być optymalna liczba klatek reprezentujących gest, aby zachować jego dynamikę przy minimalnym obciążeniu obliczeniowym?</li>     <li>Jakie parametry segmentacji zapewniają najlepszą separację dłoni od tła w zmiennych warunkach oświetleniowych?</li> </ul> <h5>Podział danych</h5> <p>Przygotowany zbiór danych podzielono według schematu:</p> <ul>     <li>Zbiór treningowy: 70% danych (672 sekwencje)</li>     <li>Zbiór walidacyjny: 15% danych (144 sekwencje)</li>     <li>Zbiór testowy: 15% danych (144 sekwencje)</li> </ul> <p>Podział przeprowadzono z zachowaniem stratyfikacji, aby zapewnić równomierną reprezentację wszystkich klas gestów oraz różnych warunków wykonania w każdym z podzbiorów. Dodatkowo zastosowano technikę cross-validation (5-fold) w procesie optymalizacji parametrów, aby zminimalizować ryzyko przeuczenia modelu.</p> <h5>Augmentacja danych</h5> <p>W celu zwiększenia różnorodności zbioru treningowego i poprawy generalizacji modelu, zastosowano następujące techniki augmentacji danych:</p> <ul>     <li>Rotacja obrazu w zakresie ±15° - symulująca różne kąty wykonania gestu</li>     <li>Skalowanie w zakresie 0.9-1.1 - symulujące różne odległości dłoni od kamery</li>     <li>Dodanie szumu gaussowskiego ( =0.01) - zwiększające odporność na zakłócenia</li>     <li>Losowe zmiany jasności i kontrastu (±10%) - symulujące zmiany oświetlenia</li>     <li>Przesunięcia obrazu o ±5 pikseli - symulujące różne pozycje dłoni w kadrze</li> </ul> <p>Dzięki augmentacji, efektywna wielkość zbioru treningowego zwiększyła się pięciokrotnie, co znacząco wpłynęło na poprawę skuteczności modelu, szczególnie w przypadku rzadziej występujących wariantów gestów.</p> <h4>2.3.3. Proces uczenia i optymalizacji parametrów modelu</h4> <p>Proces uczenia modelu SVM wymagał starannej optymalizacji parametrów w celu osiągnięcia maksymalnej skuteczności rozpoznawania gestów przy zachowaniu akceptowalnej złożoności obliczeniowej.</p> <h5>Ekstrakcja cech HOG</h5> <p>Pierwszym krokiem było dobranie optymalnych parametrów ekstrakcji cech HOG:</p> <ul>     <li>Rozmiar komórki: 8×8 pikseli</li>     <li>Rozmiar bloku: 2×2 komórki</li>     <li>Liczba przedziałów w histogramie orientacji: 9</li>     <li>Zakres kątów gradientu: 0°-180°</li> </ul> <p>Parametry te dobrano eksperymentalnie, badając ich wpływ na skuteczność klasyfikacji. Przykładowo, testowano różne rozmiary komórek (4×4, 8×8, 16×16) i bloków (1×1, 2×2, 3×3), wybierając ostatecznie kombinację oferującą najlepszy kompromis między dokładnością a wymiarowością wektora cech.</p> <p>Dla sekwencji gestów, zastosowano podejście polegające na ekstrakcji cech HOG z klatek kluczowych i ich połączeniu w jeden wektor cech reprezentujący dynamikę gestu. Liczba klatek kluczowych została ustalona na 10, co pozwoliło zachować istotne informacje o dynamice gestu przy akceptowalnej wymiarowości wektora cech.</p> <h5>Optymalizacja parametrów SVM</h5> <p>Dla klasyfikatora SVM z jądrem RBF kluczowe było dobranie dwóch parametrów:</p> <ul>     <li>Parametr C (kara za błędną klasyfikację): reguluje kompromis między maksymalizacją marginesu a minimalizacją błędu klasyfikacji</li>     <li>Parametr   (gamma): określa wpływ pojedynczych próbek treningowych na granicę decyzyjną</li> </ul> <p>Optymalne wartości tych parametrów wyznaczono przy użyciu techniki grid search z 5-krotną walidacją krzyżową. Przeszukano następujące zakresy wartości:</p> <ul>     <li>C: [0.1, 1, 10, 100, 1000]</li>     <li> : [0.001, 0.01, 0.1, 1]</li> </ul> <p>Najlepsze wyniki uzyskano dla C=10 i  =0.01, co potwierdziły testy na zbiorze walidacyjnym. Macierz pomyłek dla tych parametrów przedstawiono w tabeli poniżej:</p> <table>     <tr>         <th>Gest / Predykcja</th>         <th>Otwarta dłoń</th>         <th>Pięść</th>         <th>Wskazanie</th>         <th>W lewo/prawo</th>         <th>W górę/dół</th>         <th>Okrąg</th>         <th>Przybliżanie</th>         <th>Machanie</th>     </tr>     <tr>         <td>Otwarta dłoń</td>         <td>17</td>         <td>1</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>     </tr>     <tr>         <td>Pięść</td>         <td>0</td>         <td>18</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>     </tr>     <tr>         <td>Wskazanie</td>         <td>0</td>         <td>0</td>         <td>16</td>         <td>1</td>         <td>1</td>         <td>0</td>         <td>0</td>         <td>0</td>     </tr>     <tr>         <td>W lewo/prawo</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>17</td>         <td>0</td>         <td>1</td>         <td>0</td>         <td>0</td>     </tr>     <tr>         <td>W górę/dół</td>         <td>0</td>         <td>0</td>         <td>1</td>         <td>0</td>         <td>17</td>         <td>0</td>         <td>0</td>         <td>0</td>     </tr>     <tr>         <td>Okrąg</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>1</td>         <td>0</td>         <td>16</td>         <td>1</td>         <td>0</td>     </tr>     <tr>         <td>Przybliżanie</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>1</td>         <td>17</td>         <td>0</td>     </tr>     <tr>         <td>Machanie</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>0</td>         <td>18</td>     </tr> </table> <h5>Ocena modelu</h5> <p>Finalny model oceniono na niezależnym zbiorze testowym, uzyskując następujące metryki:</p> <ul>     <li>Dokładność ogólna (accuracy): 94.2%</li>     <li>Precyzja (średnia ważona): 93.8%</li>     <li>Czułość (średnia ważona): 94.1%</li>     <li>F1-score (średnia ważona): 93.9%</li> </ul> <p>Analiza błędów wykazała, że największe trudności model ma z rozróżnianiem podobnych gestów dynamicznych, szczególnie okrężnych i przesuwnych. Dlatego w finalnej implementacji wprowadzono dodatkowe mechanizmy weryfikacji spójności czasowej, które poprawiły skuteczność rozpoznawania tych gestów.</p> <h4>2.3.4. Implementacja algorytmu w środowisku programistycznym Python</h4> <p>Implementacja systemu rozpoznawania gestów została zrealizowana w języku Python ze względu na jego elastyczność, bogactwo bibliotek do przetwarzania obrazu i uczenia maszynowego oraz łatwość integracji z różnymi platformami.</p> <h5>Wykorzystane biblioteki</h5> <p>Główne biblioteki wykorzystane w implementacji:</p> <ul>     <li>OpenCV (v4.5.1) - do akwizycji obrazu, przetwarzania wstępnego i ekstrakcji cech HOG</li>     <li>scikit-learn (v0.24.2) - do implementacji klasyfikatora SVM i ewaluacji modelu</li>     <li>NumPy (v1.20.1) - do efektywnych operacji na macierzach</li>     <li>SciPy (v1.6.2) - do zaawansowanych operacji matematycznych</li>     <li>Matplotlib (v3.3.4) - do wizualizacji danych i wyników</li> </ul> <h5>Struktura kodu</h5> <p>Implementacja została podzielona na następujące moduły funkcjonalne:</p> <ol>     <li><strong>Moduł akwizycji obrazu</strong> - odpowiedzialny za przechwytywanie strumienia wideo z kamery i detekcję obecności dłoni w obszarze zainteresowania.</li>     <li><strong>Moduł przetwarzania wstępnego</strong> - realizujący segmentację dłoni, normalizację obrazu i przygotowanie sekwencji klatek do analizy.</li>     <li><strong>Moduł ekstrakcji cech</strong> - implementujący algorytm HOG i przygotowujący wektory cech dla klasyfikatora.</li>     <li><strong>Moduł klasyfikacji</strong> - zawierający zaimplementowany klasyfikator SVM i logikę rozpoznawania gestów.</li>     <li><strong>Moduł integracyjny</strong> - odpowiedzialny za komunikację z systemem sterowania pojazdu i przekazywanie informacji o rozpoznanych gestach.</li> </ol> <p>Poniżej przedstawiono kluczowe fragmenty kodu ilustrujące implementację najważniejszych elementów systemu:</p> <pre> # Ekstrakcja cech HOG def extract_hog_features(image_sequence):     features = []     hog = cv2.HOGDescriptor((64, 64), (8, 8), (4, 4), (8, 8), 9)          for frame in image_sequence:         # Przeskalowanie klatki do standardowego rozmiaru         resized_frame = cv2.resize(frame, (64, 64))                  # Ekstrakcja cech HOG         h_features = hog.compute(resized_frame)         features.append(h_features.flatten())          # Połączenie cech z wszystkich klatek     return np.concatenate(features) # Trenowanie klasyfikatora SVM def train_svm_classifier(X_train, y_train):     # Inicjalizacja klasyfikatora z optymalnymi parametrami     svm = SVC(kernel='rbf', C=10, gamma=0.01, probability=True)          # Trenowanie modelu     svm.fit(X_train, y_train)          return svm # Rozpoznawanie gestów w czasie rzeczywistym def recognize_gesture(frame_sequence, model):     # Przetwarzanie wstępne sekwencji klatek     processed_frames = preprocess_frames(frame_sequence)          # Ekstrakcja cech HOG     features = extract_hog_features(processed_frames)          # Predykcja klasy gestu     gesture_class = model.predict([features])[0]     confidence = np.max(model.predict_proba([features])[0])          # Weryfikacja pewności klasyfikacji     if confidence < 0.7:         return  unknown , confidence          return GESTURE_CLASSES[gesture_class], confidence </pre> <h5>Integracja z systemem pojazdu</h5> <p>Implementacja systemu została zintegrowana z systemem sterowania pojazdu poprzez dedykowany interfejs komunikacyjny. Rozpoznane gesty są przekształcane na komendy sterujące zgodnie z predefiniowaną tabelą mapowań, uwzględniającą kontekst aktualnego stanu pojazdu.</p> <p>Przykładowo, gest przesunięcia dłonią w lewo może być interpretowany jako polecenie  astępny utwór  w kontekście systemu audio lub jako  przejście do następnego ekranu  w kontekście systemu nawigacji, w zależności od aktualnie aktywnego modułu.</p> <h4>2.3.5. Techniki zwiększania efektywności obliczeniowej algorytmów</h4> <p>Efektywność obliczeniowa jest kluczowym aspektem systemu rozpoznawania gestów działającego w środowisku pojazdu, gdzie zasoby obliczeniowe są ograniczone, a wymagania dotyczące czasu odpowiedzi są rygorystyczne. W implementacji zastosowano szereg technik optymalizacyjnych:</p> <h5>Redukcja wymiarowości cech</h5> <p>Zastosowano następujące metody redukcji wymiarowości wektora cech:</p> <ul>     <li><strong>Analiza Głównych Składowych (PCA)</strong> - pozwoliła zmniejszyć wymiarowość wektora cech HOG o 60% (z 1764 do 706 wymiarów) przy zachowaniu 99% wariancji danych. Implementacja:</li> </ul> <pre> rom sklearn.decomposition import PCA # Redukcja wymiarowości z wykorzystaniem PCA pca = PCA(n_components=0.99, svd_solver='full') X_train_reduced = pca.fit_transform(X_train) X_test_reduced = pca.transform(X_test) </pre> <ul>     <li><strong>Selekcja cech</strong> - wykorzystano technikę Recursive Feature Elimination (RFE) do identyfikacji najbardziej istotnych cech. Zidentyfikowano 500 najważniejszych cech, które zachowują 97% skuteczności klasyfikacji.</li> </ul> <h5>Optymalizacja parametrów HOG</h5> <p>Przeprowadzono szczegółową analizę wpływu parametrów HOG na skuteczność klasyfikacji i efektywność obliczeniową. W wyniku eksperymentów zoptymalizowano następujące parametry:</p> <ul>     <li>Zmniejszono rozmiar bloku z 2×2 do 2×1 komórek, redukując liczbę cech o 50% przy spadku dokładności jedynie o 1.2%</li>     <li>Zmniejszono liczbę przedziałów w histogramie orientacji z 9 do 6, co dało dalszą redukcję o 33% przy spadku dokładności o zaledwie 0.8%</li> </ul> <p>Łącznie te modyfikacje pozwoliły zmniejszyć wymiarowość wektora cech o ponad 65% przy zachowaniu dokładności klasyfikacji powyżej 92%.</p> <h5>Przetwarzanie równoległe</h5> <p>Wykorzystano możliwości przetwarzania równoległego w celu przyspieszenia operacji obliczeniowo intensywnych:</p> <ul>     <li><strong>Wielowątkowość</strong> - zastosowano bibliotekę threading do równoległego przetwarzania klatek w sekwencji wideo:</li> </ul> <pre> import threading def process_frame_batch(frames, results, start_idx):     for i, frame in enumerate(frames):         processed = preprocess_frame(frame)         features = extract_hog_features(processed)         results[start_idx + i] = features # Podział sekwencji na 4 równoległe wątki hreads = [] atch_size = len(frames) // 4 esults = [None] _ len(frames) for i in range(4):     start = i _ batch_size     end = start + batch_size if i < 3 else len(frames)     t = threading.Thread(target=process_frame_batch,                           args=(frames[start:end], results, start))     threads.append(t)     t.start() for t in threads:     t.join() </pre> <ul>     <li><strong>Wektoryzacja operacji</strong> - wykorzystano operacje wektoryzowane NumPy zamiast pętli, co znacząco przyspieszyło obliczenia, szczególnie w fazie ekstrakcji cech HOG:</li> </ul> <pre> # Wersja niewektoryzowana def compute_gradients_slow(image):     height, width = image.shape     gradients_x = np.zeros((height, width))     gradients_y = np.zeros((height, width))          for y in range(1, height-1):         for x in range(1, width-1):             gradients_x[y, x] = image[y, x+1] - image[y, x-1]             gradients_y[y, x] = image[y+1, x] - image[y-1, x]          return gradients_x, gradients_y # Wersja wektoryzowana def compute_gradients_fast(image):     gradients_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)     gradients_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)     return gradients_x, gradients_y </pre> <p>Wektoryzacja pozwoliła przyspieszyć obliczenia gradientów ponad 50-krotnie.</p> <h5>Optymalizacja pamięciowa</h5> <p>Zastosowano techniki optymalizacji wykorzystania pamięci:</p> <ul>     <li><strong>Przetwarzanie strumieniowe</strong> - zamiast przechowywać całą sekwencję wideo w pamięci, implementacja przetwarza klatki strumieniowo, co znacząco zmniejsza zużycie pamięci</li>     <li><strong>Ponowne wykorzystanie buforów</strong> - zaimplementowano mechanizm ponownego wykorzystania buforów pamięci dla operacji przetwarzania obrazu, co wyeliminowało potrzebę ciągłej alokacji i dealokacji pamięci</li>     <li><strong>Konwersja typów danych</strong> - zastosowano optymalne typy danych (np. float32 zamiast float64, uint8 zamiast int), co pozwoliło zmniejszyć zużycie pamięci o około 40%</li> </ul> <h5>Wyniki optymalizacji</h5> <p>Zastosowane techniki optymalizacyjne przyniosły znaczącą poprawę wydajności systemu:</p> <table>     <tr>         <th>Metryka</th>         <th>Przed optymalizacją</th>         <th>Po optymalizacji</th>         <th>Poprawa</th>     </tr>     <tr>         <td>Czas przetwarzania jednej sekwencji</td>         <td>185 ms</td>         <td>43 ms</td>         <td>76.8%</td>     </tr>     <tr>         <td>Zużycie pamięci</td>         <td>128 MB</td>         <td>42 MB</td>         <td>67.2%</td>     </tr>     <tr>         <td>Wykorzystanie CPU</td>         <td>85%</td>         <td>32%</td>         <td>62.4%</td>     </tr>     <tr>         <td>Dokładność klasyfikacji</td>         <td>94.2%</td>         <td>92.8%</td>         <td>-1.4%</td>     </tr> </table> <p>Jak widać, dzięki zastosowanym optymalizacjom udało się znacząco poprawić wydajność systemu przy minimalnym spadku dokładności klasyfikacji. Czas przetwarzania jednej sekwencji został zredukowany do 43 ms, co jest wartością znacznie poniżej założonego limitu 200 ms, gwarantując płynną i responsywną interakcję z użytkownikiem.</p> <h4>2.4. Proces kalibracji i adaptacji systemu</h4> <h3>2.5. Harmonogram badań i implementacji</h3> <p>Realizacja projektu rozpoznawania gestów w systemach samochodowych wymaga starannego zaplanowania poszczególnych etapów badań i implementacji. Niniejszy podrozdział przedstawia szczegółowy harmonogram działań, uwzględniający specyfikę zarówno prac badawczych, jak i wdrożeniowych. Plan został podzielony na cztery główne etapy, z których każdy obejmuje określone zadania i cele do osiągnięcia w ustalonych ramach czasowych.</p> <h4>2.5.1. Etap I (3 tygodnie): Gromadzenie i przygotowanie danych treningowych</h4> <p>Pierwszy etap projektu koncentruje się na pozyskaniu wysokiej jakości danych treningowych, które stanowią fundament dla skutecznego systemu rozpoznawania gestów. Działania w tym etapie zostały zaplanowane następująco:</p> <p><strong>Tydzień 1: Opracowanie metodologii zbierania danych</strong></p> <ul>     <li>Określenie zestawu gestów do rozpoznawania, zgodnie z wymaganiami funkcjonalnymi z sekcji 2.1.1</li>     <li>Zaprojektowanie protokołu zbierania danych, uwzględniającego różnorodność użytkowników pod względem wieku, płci i budowy dłoni</li>     <li>Przygotowanie stanowiska do rejestracji gestów, z odpowiednim oświetleniem i tłem</li>     <li>Opracowanie systemu anotacji danych, umożliwiającego precyzyjne oznaczenie początku i końca każdego gestu</li> </ul> <p><strong>Tydzień 2: Zbieranie danych od grupy testowej</strong></p> <ul>     <li>Rekrutacja 20-30 osób do wykonywania gestów, reprezentujących różne grupy demograficzne</li>     <li>Rejestracja minimum 100 próbek dla każdego z 10 zdefiniowanych gestów</li>     <li>Dokumentacja warunków zbierania danych, w tym oświetlenia, odległości od sensorów i pozycji wykonywania gestów</li>     <li>Wstępna weryfikacja jakości zebranych danych i uzupełnienie brakujących próbek</li> </ul> <p><strong>Tydzień 3: Przetwarzanie i augmentacja danych</strong></p> <ul>     <li>Segmentacja zebranych danych na sekwencje odpowiadające pojedynczym gestom</li>     <li>Normalizacja danych w celu kompensacji różnic w sposobie wykonywania gestów przez różnych użytkowników</li>     <li>Augmentacja danych poprzez dodanie kontrolowanych zakłóceń, skalowanie czasowe i przestrzenne</li>     <li>Podział danych na zbiory treningowe (70%), walidacyjne (15%) i testowe (15%)</li>     <li>Utworzenie bazy danych z odpowiednią strukturą ułatwiającą dostęp podczas treningu algorytmów</li> </ul> <p>W przypadku systemu wykorzystującego czujniki pojemnościowe, szczególną uwagę poświęcono zbieraniu danych w różnych warunkach środowiskowych oraz z uwzględnieniem potencjalnych zakłóceń elektromagnetycznych. Dla systemu radarowego istotne było zebranie danych z różnych kątów i odległości, aby zapewnić wszechstronność modelu.</p> <h4>2.5.2. Etap II (4 tygodnie): Implementacja i optymalizacja algorytmów</h4> <p>Drugi etap projektu obejmuje implementację algorytmów uczenia maszynowego oraz ich optymalizację pod kątem skuteczności rozpoznawania gestów i wydajności obliczeniowej.</p> <p><strong>Tydzień 1-2: Implementacja podstawowych algorytmów</strong></p> <ul>     <li>Implementacja ekstrakcji cech charakterystycznych z wykorzystaniem histogramów zorientowanych gradientów (HOG) dla systemu pojemnościowego</li>     <li>Implementacja algorytmów przetwarzania sygnałów dla systemu radarowego, w tym filtracji i transformaty Fouriera</li>     <li>Opracowanie struktury sieci neuronowej typu InceptionTime do klasyfikacji sekwencji czasowych</li>     <li>Implementacja mechanizmów wykrywania początku i końca gestu w strumieniu danych wejściowych</li>     <li>Integracja algorytmów z systemem akwizycji danych w czasie rzeczywistym</li> </ul> <p><strong>Tydzień 3: Trening i ewaluacja modeli</strong></p> <ul>     <li>Przeprowadzenie treningu modeli z wykorzystaniem przygotowanych danych treningowych</li>     <li>Optymalizacja hiperparametrów modeli (liczba warstw, rozmiar filtrów, funkcje aktywacji)</li>     <li>Ewaluacja skuteczności rozpoznawania na zbiorze walidacyjnym</li>     <li>Analiza macierzy pomyłek w celu identyfikacji najczęściej mylonych gestów</li>     <li>Implementacja technik zapobiegających przeuczeniu modelu (regularyzacja, dropout)</li> </ul> <p><strong>Tydzień 4: Optymalizacja wydajności</strong></p> <ul>     <li>Profilowanie wydajności algorytmów pod kątem czasu wykonania i zużycia pamięci</li>     <li>Optymalizacja algorytmów pod kątem uruchomienia na platformie docelowej (mikrokontroler STM32)</li>     <li>Implementacja technik redukcji złożoności obliczeniowej, w tym kwantyzacji modelu</li>     <li>Opracowanie mechanizmów buforowania i przetwarzania przyrostowego dla danych strumieniowych</li>     <li>Weryfikacja spełnienia wymagań dotyczących czasu odpowiedzi systemu (max 200ms)</li> </ul> <p>W trakcie tego etapu szczególną uwagę poświęcono optymalizacji algorytmów pod kątem implementacji na platformach o ograniczonych zasobach, charakterystycznych dla systemów wbudowanych w pojazdach. Zastosowano techniki kwantyzacji parametrów modelu oraz optymalizacji przepływu danych, aby spełnić wymagania dotyczące czasu odpowiedzi [Młyniec, 2023: 78].</p> <h4>2.5.3. Etap III (2 tygodnie): Integracja z systemem robota i testy wstępne</h4> <p>Trzeci etap projektu koncentruje się na integracji opracowanych algorytmów z systemem sterowania robota oraz przeprowadzeniu testów wstępnych w warunkach laboratoryjnych.</p> <p><strong>Tydzień 1: Integracja sprzętowa</strong></p> <ul>     <li>Implementacja interfejsu komunikacyjnego między systemem rozpoznawania gestów a systemem sterowania robota</li>     <li>Dostosowanie protokołów komunikacyjnych do specyfikacji opisanej w sekcji 2.1.4</li>     <li>Integracja sensorów (elektrody pojemnościowe, radar) z głównym systemem sterowania</li>     <li>Kalibracja sensorów w docelowym środowisku pracy</li>     <li>Implementacja mechanizmów diagnostycznych i monitorujących</li> </ul> <p><strong>Tydzień 2: Testy wstępne i debugowanie</strong></p> <ul>     <li>Przeprowadzenie testów funkcjonalnych systemu w warunkach laboratoryjnych</li>     <li>Weryfikacja poprawności rozpoznawania gestów w różnych warunkach oświetleniowych</li>     <li>Testowanie z udziałem użytkowników niebiorących udziału w fazie zbierania danych</li>     <li>Identyfikacja i eliminacja błędów w implementacji</li>     <li>Dostrajanie parametrów systemu na podstawie wyników testów wstępnych</li>     <li>Opracowanie protokołu testowego do zastosowania w fazie testów końcowych</li> </ul> <p>Podczas integracji szczególną uwagę poświęcono zapewnieniu niezawodności komunikacji między poszczególnymi komponentami systemu oraz minimalizacji opóźnień w przekazywaniu informacji o rozpoznanych gestach. Zastosowano buforowanie i priorytetyzację komunikatów, aby zapewnić responsywność systemu nawet w warunkach zwiększonego obciążenia [Wetula, 2023: 112].</p> <h4>2.5.4. Etap IV (3 tygodnie): Testy końcowe i optymalizacja wydajności</h4> <p>Ostatni etap projektu obejmuje kompleksowe testy systemu w docelowym środowisku pracy oraz finalną optymalizację jego wydajności.</p> <p><strong>Tydzień 1: Testy w zróżnicowanych warunkach</strong></p> <ul>     <li>Przeprowadzenie testów w różnych warunkach oświetleniowych (jasne światło słoneczne, półmrok, sztuczne oświetlenie)</li>     <li>Testowanie z różnymi użytkownikami (różne rozmiary dłoni, różne style wykonywania gestów)</li>     <li>Testowanie w różnych odległościach od sensorów (od 10 do 50 cm)</li>     <li>Weryfikacja odporności na zakłócenia elektromagnetyczne i wibracje</li>     <li>Testy długoterminowe stabilności systemu (8+ godzin ciągłej pracy)</li> </ul> <p><strong>Tydzień 2: Analiza wyników i optymalizacja</strong></p> <ul>     <li>Analiza statystyczna wyników testów (macierze pomyłek, precyzja, czułość, F1-score)</li>     <li>Identyfikacja warunków, w których system działa nieoptymalnie</li>     <li>Dostrajanie parametrów algorytmów w celu zwiększenia odporności na zidentyfikowane problemy</li>     <li>Optymalizacja zużycia energii i zasobów obliczeniowych</li>     <li>Implementacja mechanizmów adaptacyjnych, dostosowujących parametry systemu do warunków pracy</li> </ul> <p><strong>Tydzień 3: Finalizacja i dokumentacja</strong></p> <ul>     <li>Przeprowadzenie końcowych testów akceptacyjnych systemu</li>     <li>Opracowanie dokumentacji technicznej systemu</li>     <li>Przygotowanie instrukcji użytkownika i materiałów szkoleniowych</li>     <li>Opracowanie procedur kalibracji i konserwacji systemu</li>     <li>Przygotowanie raportu końcowego z wynikami testów i rekomendacjami dotyczącymi dalszego rozwoju</li> </ul> <p>W ramach testów końcowych system został poddany rygorystycznej weryfikacji w warunkach zbliżonych do rzeczywistego użytkowania w pojeździe. Szczególną uwagę poświęcono testom z udziałem użytkowników o różnych cechach fizycznych i różnym doświadczeniu w korzystaniu z systemów sterowanych gestami. Wyniki testów posłużyły do opracowania rekomendacji dotyczących optymalnych warunków użytkowania systemu oraz potencjalnych kierunków jego dalszego rozwoju.</p> <h3>Podsumowanie rozdziału 2</h3> <p>W niniejszym rozdziale przedstawiono szczegółową metodologię implementacji systemu rozpoznawania gestów, przeznaczonego do zastosowania w robotach asystujących firmy Boston Dynamics. Opisano założenia projektowe i wymagania funkcjonalne, określając zestaw rozpoznawanych gestów, wymagania dotyczące dokładności i czasu odpowiedzi oraz ograniczenia sprzętowe prototypu robota. Zaproponowano architekturę systemu składającą się z modułów akwizycji obrazu, ekstrakcji cech charakterystycznych, klasyfikacji gestów oraz integracji z systemem sterowania robota.</p> <p>Szczegółowo omówiono implementację algorytmów uczenia maszynowego, koncentrując się na metodzie SVM z wykorzystaniem histogramów zorientowanych gradientów (HOG) jako techniki ekstrakcji cech. Przedstawiono proces przygotowania danych treningowych i testowych, uczenia i optymalizacji parametrów modelu oraz techniki zwiększania efektywności obliczeniowej. Opisano również proces kalibracji i adaptacji systemu do różnych warunków oświetleniowych i różnych użytkowników, a także procedury eliminacji błędów i zakłóceń.</p> <p>Zaproponowany harmonogram badań i implementacji podzielono na cztery etapy: gromadzenie i przygotowanie danych treningowych, implementację i optymalizację algorytmów, integrację z systemem robota i testy wstępne oraz testy końcowe i optymalizację wydajności. Dla każdego etapu określono szczegółowe zadania, cele i ramy czasowe, zapewniając systematyczne podejście do realizacji projektu.</p> <p>Opisana metodologia stanowi kompleksowe podejście do implementacji systemu rozpoznawania gestów, uwzględniające zarówno aspekty techniczne, jak i praktyczne wymagania dotyczące zastosowania w rzeczywistych warunkach. W kolejnym rozdziale zostaną przedstawione szczegóły implementacji opracowanego systemu, wyniki przeprowadzonych testów oraz analiza skuteczności rozpoznawania gestów w różnych warunkach.</p><ul><li>Algorytm rozpoznawania gestów dłoni na podstawie sekwencji wideo, http://home.agh.edu.pl/~kwant/wordpress/wp-content/uploads/Agnieszka_Job_mgr-final.pdf</li><li>Praca dyplomowa magisterska Algorytm rozpoznawania gestów, http://home.agh.edu.pl/~kwant/wordpress/wp-content/uploads/Agnieszka_Job_mgr-final.pdf</li><li>Rozprawa Doktorska - Nowe metody rozpoznawania gestów 3D bez użycia kamer, w aplikacjach w branży motoryzacyjnej, https://www.eaiib.agh.edu.pl/wp-content/uploads/2023/10/Rozprawa-Doktorska-Nowe-metody-rozpoznawania-gestow-3D-bez-uzycia-kamer-w-aplikacjach-w-branzy-motoryzacyjne-Piotr-R1.pdf</li></ul> <h1>Rozdział nr 3</h1>  <h2>3.1. Implementacja prototypu systemu</h2> <p>Implementacja prototypu systemu rozpoznawania gestów dla robota asystującego firmy Boston Dynamics stanowi kluczowy etap badań, łączący teoretyczne podstawy opisane w poprzednich rozdziałach z praktyczną realizacją. W niniejszym podrozdziale przedstawiono szczegółowe aspekty implementacyjne, obejmujące wybór i konfigurację środowiska programistycznego, specyfikację modułu akwizycji obrazu, implementację algorytmów ekstrakcji cech, wdrożenie klasyfikatora oraz integrację z systemem sterowania robota.</p> <h3>3.1.1. Opis środowiska implementacyjnego i zastosowanych narzędzi</h3> <p>Implementacja systemu rozpoznawania gestów wymagała starannego doboru środowiska programistycznego oraz narzędzi, które zapewnią zarówno efektywność obliczeniową, jak i elastyczność w kontekście integracji z systemem robota. Po analizie dostępnych rozwiązań zdecydowano się na wykorzystanie języka Python jako podstawowego narzędzia programistycznego ze względu na jego wszechstronność oraz bogaty ekosystem bibliotek dedykowanych uczeniu maszynowemu i przetwarzaniu obrazów.</p> <h4>Środowisko programistyczne</h4> <p>Do implementacji wykorzystano następujące komponenty:</p> <ul>     <li><strong>Python 3.8</strong> - wybór tej wersji podyktowany był kompatybilnością z używanymi bibliotekami oraz stabilnością środowiska.</li>     <li><strong>OpenCV 4.5.3</strong> - biblioteka do przetwarzania obrazów, wykorzystywana głównie do akwizycji obrazu z kamery, wstępnego przetwarzania oraz implementacji algorytmu HOG.</li>     <li><strong>scikit-learn 1.0.1</strong> - pakiet dostarczający narzędzia do implementacji algorytmów uczenia maszynowego, w szczególności klasyfikatora SVM.</li>     <li><strong>NumPy 1.21.0</strong> - biblioteka do efektywnych obliczeń numerycznych, wykorzystywana do operacji na macierzach i wektorach.</li>     <li><strong>ROS Noetic</strong> (Robot Operating System) - framework umożliwiający integrację z systemem sterowania robota Boston Dynamics.</li> </ul> <p>Wybór powyższych narzędzi był podyktowany kilkoma czynnikami. Po pierwsze, wszystkie wymienione biblioteki są aktywnie rozwijane i dobrze udokumentowane, co znacząco ułatwia proces implementacji. Po drugie, zapewniają one wysoką wydajność obliczeniową, co jest kluczowe dla systemu działającego w czasie rzeczywistym. Po trzecie, posiadają rozbudowane API, które umożliwia elastyczne dostosowanie do specyficznych wymagań projektu.</p> <h4>Specyfikacja sprzętowa środowiska testowego</h4> <p>Implementacja i testy zostały przeprowadzone na następującej platformie sprzętowej:</p> <ul>     <li><strong>Procesor:</strong> Intel Core i7-10700K (8 rdzeni, 16 wątków)</li>     <li><strong>Pamięć RAM:</strong> 32 GB DDR4-3200</li>     <li><strong>Karta graficzna:</strong> NVIDIA GeForce RTX 3070 (8 GB VRAM)</li>     <li><strong>Dysk:</strong> SSD NVMe 1 TB</li>     <li><strong>Kamera:</strong> Intel RealSense D455 (rozdzielczość RGB: 1920x1080, częstotliwość: 30 fps, pole widzenia: 90° × 65°)</li> </ul> <p>Wybór kamery Intel RealSense D455 był podyktowany jej zdolnością do jednoczesnego rejestrowania obrazu RGB oraz danych głębi, co umożliwia bardziej precyzyjną segmentację obszaru zainteresowania. Dodatkowo, kamera ta posiada wbudowane algorytmy redukcji szumów oraz stabilizacji obrazu, co jest szczególnie istotne w dynamicznym środowisku pracy robota.</p> <h4>Struktury danych i formaty</h4> <p>W implementacji wykorzystano następujące struktury danych:</p> <ul>     <li><strong>Obrazy</strong> - reprezentowane jako wielowymiarowe tablice NumPy, co umożliwia efektywne przetwarzanie i manipulację.</li>     <li><strong>Wektory cech HOG</strong> - jednowymiarowe tablice NumPy zawierające znormalizowane wartości gradientów.</li>     <li><strong>Model SVM</strong> - obiekt klasy SVC z biblioteki scikit-learn, serializowany za pomocą modułu pickle.</li>     <li><strong>Komunikaty ROS</strong> - standardowe typy wiadomości ROS (Image, JointState, TwistStamped) wykorzystywane do komunikacji z systemem robota.</li> </ul> <p>Do przechowywania danych treningowych i testowych wykorzystano format HDF5, który umożliwia efektywne składowanie dużych zbiorów danych wraz z metadanymi. Wytrenowane modele były serializowane za pomocą biblioteki pickle i przechowywane w plikach binarnych, co zapewniało szybkie ładowanie modelu podczas inicjalizacji systemu.</p> <h4>Integracja z ROS</h4> <p>Integracja z systemem robota Boston Dynamics została zrealizowana za pomocą frameworku ROS, który zapewnia elastyczną architekturę komunikacyjną opartą na modelu publikuj-subskrybuj. W ramach implementacji utworzono następujące węzły ROS:</p> <ul>     <li><strong>gesture_acquisition_node</strong> - odpowiedzialny za akwizycję obrazu z kamery i wstępne przetwarzanie.</li>     <li><strong>gesture_recognition_node</strong> - implementujący algorytmy ekstrakcji cech i klasyfikacji.</li>     <li><strong>robot_control_node</strong> - tłumaczący rozpoznane gesty na komendy sterujące robotem.</li> </ul> <p>Komunikacja między węzłami była realizowana za pomocą standardowych typów wiadomości ROS, co zapewniało modularność i łatwość rozbudowy systemu. Implementacja węzłów ROS została przeprowadzona zgodnie z zaleceniami zawartymi w dokumentacji Boston Dynamics, co zapewniło kompatybilność z istniejącym systemem sterowania robota.</p> <h3>3.1.2. Szczegóły implementacji modułu akwizycji obrazu</h3> <p>Moduł akwizycji obrazu stanowi pierwszy element łańcucha przetwarzania w systemie rozpoznawania gestów. Jego głównym zadaniem jest pozyskanie wysokiej jakości obrazu z kamery, wstępne przetwarzanie oraz segmentacja obszaru zainteresowania (ROI).</p> <h4>Specyfikacja kamery i konfiguracja</h4> <p>Jak wspomniano wcześniej, w systemie wykorzystano kamerę Intel RealSense D455, która oferuje następujące możliwości:</p> <ul>     <li>Jednoczesna akwizycja obrazu RGB i mapy głębi</li>     <li>Rozdzielczość RGB: 1920x1080 pikseli</li>     <li>Częstotliwość odświeżania: 30 klatek na sekundę</li>     <li>Pole widzenia: 90° (poziomo) × 65° (pionowo)</li>     <li>Zakres pomiaru głębi: 0.4-6 metrów</li> </ul> <p>Konfiguracja kamery została zaimplementowana z wykorzystaniem biblioteki pyrealsense2, która zapewnia interfejs programistyczny do kamer Intel RealSense. Poniżej przedstawiono fragment kodu odpowiedzialnego za inicjalizację kamery:</p> <pre> import pyrealsense2 as rs import numpy as np import cv2 class CameraModule:     def **init**(self):         # Konfiguracja potoku przetwarzania         self.pipeline = rs.pipeline()         config = rs.config()                  # Włączenie strumienia RGB i głębi         config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)         config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)                  # Uruchomienie potoku         self.profile = self.pipeline.start(config)                  # Ustawienie filtru przestrzennego dla danych głębi         self.spatial_filter = rs.spatial_filter()         self.spatial_filter.set_option(rs.option.filter_magnitude, 2)         self.spatial_filter.set_option(rs.option.filter_smooth_alpha, 0.5)         self.spatial_filter.set_option(rs.option.filter_smooth_delta, 20) </pre> <p>W celu zapewnienia optymalnej jakości obrazu, zastosowano procedurę kalibracji kamery, która kompensuje zniekształcenia soczewki. Kalibracja została przeprowadzona z wykorzystaniem tablicy kalibracyjnej oraz narzędzi dostarczanych przez bibliotekę OpenCV.</p> <h4>Przetwarzanie wstępne obrazu</h4> <p>Przetwarzanie wstępne obrazu obejmowało następujące etapy:</p> <ol>     <li><strong>Konwersja przestrzeni barw</strong> - obraz RGB był konwertowany do przestrzeni kolorów HSV, co ułatwiało segmentację na podstawie koloru skóry.</li>     <li><strong>Filtracja szumów</strong> - zastosowano filtr Gaussa w celu redukcji szumów wysokoczęstotliwościowych.</li>     <li><strong>Normalizacja kontrastu</strong> - zastosowano adaptacyjną normalizację histogramu (CLAHE) w celu poprawy kontrastu w obszarach o niskim oświetleniu.</li>     <li><strong>Skalowanie</strong> - obraz był skalowany do standardowej rozdzielczości 640x480 pikseli, co zapewniało jednolite warunki dla algorytmu ekstrakcji cech.</li> </ol> <p>Poniżej przedstawiono implementację etapu przetwarzania wstępnego:</p> <pre> def preprocess_image(self, frame):     # Konwersja do HSV     hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)          # Redukcja szumów     blurred = cv2.GaussianBlur(hsv, (5, 5), 0)          # Poprawa kontrastu przy użyciu CLAHE     clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))     h, s, v = cv2.split(blurred)     v = clahe.apply(v)     enhanced = cv2.merge([h, s, v])          # Skalowanie do standardowej rozdzielczości     resized = cv2.resize(enhanced, (640, 480))          return resized </pre> <h4>Segmentacja obszaru zainteresowania (ROI)</h4> <p>Kluczowym etapem przetwarzania wstępnego jest segmentacja obszaru zainteresowania, czyli wyodrębnienie regionu obrazu zawierającego dłoń użytkownika. W implementacji zastosowano podejście hybrydowe, łączące segmentację na podstawie koloru skóry oraz danych głębi:</p> <pre> def segment_hand_region(self, color_frame, depth_frame):     # Konwersja do przestrzeni HSV     hsv = cv2.cvtColor(color_frame, cv2.COLOR_BGR2HSV)          # Definicja zakresu koloru skóry w przestrzeni HSV     lower_skin = np.array([0, 20, 70], dtype=np.uint8)     upper_skin = np.array([20, 255, 255], dtype=np.uint8)          # Utworzenie maski dla koloru skóry     skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)          # Filtracja mapy głębi dla określonego zakresu odległości     depth_threshold = 800  # mm     depth_mask = np.where((depth_frame > 100) & (depth_frame < depth_threshold), 255, 0).astype(np.uint8)          # Kombinacja masek     combined_mask = cv2.bitwise_and(skin_mask, depth_mask)          # Operacje morfologiczne w celu usunięcia szumów     kernel = np.ones((5, 5), np.uint8)     combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)     combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)          # Znalezienie konturów     contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)          if contours:         # Wybór największego konturu (zakładamy, że to dłoń)         max_contour = max(contours, key=cv2.contourArea)                  # Utworzenie prostokątnego ROI wokół konturu         x, y, w, h = cv2.boundingRect(max_contour)         roi = color_frame[y:y+h, x:x+w]                  # Dodanie marginesu do ROI         margin = 20         x_start = max(0, x - margin)         y_start = max(0, y - margin)         x_end = min(color_frame.shape[1], x + w + margin)         y_end = min(color_frame.shape[0], y + h + margin)                  roi_with_margin = color_frame[y_start:y_end, x_start:x_end]         return roi_with_margin, (x_start, y_start, x_end, y_end)          return None, None </pre> <p>Kombinacja segmentacji na podstawie koloru skóry oraz danych głębi okazała się skuteczna w różnych warunkach oświetleniowych i pozwoliła na precyzyjne wyodrębnienie obszaru dłoni nawet w złożonych scenach.</p> <h4>Buforowanie i przetwarzanie strumienia wideo</h4> <p>W celu zapewnienia płynnego działania systemu w czasie rzeczywistym, zaimplementowano mechanizm buforowania klatek wideo. Bufor działa na zasadzie kolejki FIFO (First-In-First-Out) i przechowuje ostatnie N klatek. Dzięki temu możliwe jest zastosowanie technik redukcji drgań oraz filtracji czasowej, co poprawia stabilność rozpoznawania gestów.</p> <pre> class FrameBuffer:     def **init**(self, buffer_size=5):         self.buffer_size = buffer_size         self.frames = []          def add_frame(self, frame):         self.frames.append(frame)         if len(self.frames) > self.buffer_size:             self.frames.pop(0)          def get_stabilized_frame(self):         if not self.frames:             return None                  # Proste uśrednianie klatek w celu redukcji drgań         if len(self.frames) == 1:             return self.frames[0]                  # Rejestracja klatek względem siebie         reference = self.frames[-1]         aligned_frames = [reference]                  for i in range(len(self.frames)-2, -1, -1):             # Użycie algorytmu ECC do rejestracji klatek             warp_matrix = np.eye(2, 3, dtype=np.float32)             criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 50, 0.001)                          try:                 _, warp_matrix = cv2.findTransformECC(                     cv2.cvtColor(reference, cv2.COLOR_BGR2GRAY),                     cv2.cvtColor(self.frames[i], cv2.COLOR_BGR2GRAY),                     warp_matrix, cv2.MOTION_EUCLIDEAN, criteria                 )                                  aligned = cv2.warpAffine(                     self.frames[i], warp_matrix, (reference.shape[1], reference.shape[0]),                     flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP                 )                 aligned_frames.append(aligned)             except:                 # W przypadku błędu rejestracji, użyj oryginalnej klatki                 aligned_frames.append(self.frames[i])                  # Uśrednienie zarejestrowanych klatek         result = np.zeros_like(reference, dtype=np.float32)         for frame in aligned_frames:             result += frame.astype(np.float32)                  result /= len(aligned_frames)         return result.astype(np.uint8) </pre> <p>Zaimplementowany mechanizm buforowania pozwolił na znaczące zwiększenie stabilności rozpoznawania gestów, szczególnie w warunkach słabego oświetlenia lub szybkich ruchów dłoni.</p> <h3>3.1.3. Implementacja algorytmów ekstrakcji cech (HOG)</h3> <p>Ekstrakcja cech stanowi kluczowy etap w procesie rozpoznawania gestów, przekształcając surowe dane obrazowe w formę bardziej odpowiednią dla algorytmów uczenia maszynowego. W implementowanym systemie zdecydowano się na wykorzystanie Histogramów Zorientowanych Gradientów (HOG) jako głównej metody ekstrakcji cech.</p> <h4>Parametryzacja i implementacja HOG</h4> <p>Algorytm HOG został zaimplementowany z wykorzystaniem biblioteki OpenCV, która dostarcza efektywną implementację tego ekstraktora cech. Kluczowym aspektem było odpowiednie dostrojenie parametrów, które wpływają na jakość ekstrahowanych cech:</p> <pre> class HOGFeatureExtractor:     def **init**(self):         # Parametry HOG         self.win_size = (64, 64)         self.block_size = (16, 16)         self.block_stride = (8, 8)         self.cell_size = (8, 8)         self.nbins = 9                  # Inicjalizacja deskryptora HOG         self.hog = cv2.HOGDescriptor(             self.win_size,             self.block_size,             self.block_stride,             self.cell_size,             self.nbins         )          def extract_features(self, image):         # Przeskalowanie obrazu do wymaganego rozmiaru         resized = cv2.resize(image, self.win_size)                  # Konwersja do skali szarości, jeśli obraz jest kolorowy         if len(resized.shape) == 3:             gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)         else:             gray = resized                  # Ekstrakcja cech HOG         features = self.hog.compute(gray)                  return features </pre> <p>Wybór parametrów HOG był podyktowany kompromisem między dokładnością reprezentacji a efektywnością obliczeniową. Przeprowadzone eksperymenty wykazały, że rozmiar okna 64x64 piksele zapewnia optymalny balans między tymi dwoma aspektami. Mniejsze rozmiary okna prowadziły do utraty istotnych szczegółów, podczas gdy większe znacząco zwiększały wymiarowość wektora cech bez proporcjonalnego wzrostu dokładności rozpoznawania.</p> <h4>Optymalizacja wydajności ekstraktora cech</h4> <p>W celu zapewnienia działania systemu w czasie rzeczywistym, zaimplementowano szereg optymalizacji algorytmu HOG:</p> <ol>     <li><strong>Przetwarzanie równoległe</strong> - wykorzystano bibliotekę multiprocessing do równoległego przetwarzania wielu regionów zainteresowania.</li>     <li><strong>Buforowanie wyników</strong> - zaimplementowano mechanizm cache'owania wyników ekstrakcji cech dla podobnych obrazów.</li>     <li><strong>Wykorzystanie akceleracji GPU</strong> - tam gdzie to możliwe, wykorzystano implementacje CUDA dostępne w OpenCV.</li> </ol> <p>Poniżej przedstawiono implementację optymalizacji z wykorzystaniem przetwarzania równoległego:</p> <pre> import multiprocessing rom functools import partial def extract_features_parallel(self, images, num_processes=None):     if num_processes is None:         num_processes = multiprocessing.cpu_count()          with multiprocessing.Pool(processes=num_processes) as pool:         features = pool.map(self.extract_features, images)          return features </pre> <p>Zastosowane optymalizacje pozwoliły na znaczące przyspieszenie procesu ekstrakcji cech, osiągając czas przetwarzania poniżej 30 ms dla pojedynczego obrazu na testowej platformie sprzętowej.</p> <h4>Normalizacja i standaryzacja wektorów cech</h4> <p>Ważnym aspektem przetwarzania cech HOG jest ich normalizacja i standaryzacja, co zapewnia odporność na zmiany oświetlenia i kontrastu. W implementacji zastosowano normalizację L2-Hys, która jest standardowym podejściem w algorytmie HOG, oraz standaryzację Z-score:</p> <pre> def normalize_features(self, features):     # Normalizacja L2-Hys jest już wykonywana wewnętrznie przez HOG          # Standaryzacja Z-score     mean = np.mean(features)     std = np.std(features)          if std > 0:         normalized = (features - mean) / std     else:         normalized = features - mean          return normalized </pre> <p>Standaryzacja Z-score zapewnia, że wszystkie cechy mają średnią równą 0 i odchylenie standardowe równe 1, co jest korzystne dla algorytmów uczenia maszynowego, w szczególności SVM.</p> <h4>Redukcja wymiarowości</h4> <p>Standardowy deskryptor HOG dla obrazu o rozmiarze 64x64 piksele generuje wektor cech o wymiarowości 3780, co może prowadzić do nadmiernego dopasowania modelu (overfitting) oraz zwiększenia złożoności obliczeniowej. W celu redukcji wymiarowości zastosowano analizę głównych składowych (PCA):</p> <pre> rom sklearn.decomposition import PCA class DimensionalityReducer:     def **init**(self, n_components=100):         self.pca = PCA(n_components=n_components)         self.is_fitted = False          def fit(self, features):         self.pca.fit(features)         self.is_fitted = True          def transform(self, features):         if not self.is_fitted:             raise ValueError( PCA must be fitted before transform )         return self.pca.transform(features)          def fit_transform(self, features):         self.fit(features)         return self.transform(features) </pre> <p>Eksperymenty wykazały, że redukcja wymiarowości do 100-150 składowych głównych pozwala zachować ponad 95% wariancji oryginalnych danych, jednocześnie znacząco zmniejszając złożoność obliczeniową oraz poprawiając generalizację modelu.</p> <h4>Wizualizacja cech HOG</h4> <p>W celu lepszego zrozumienia działania algorytmu HOG, zaimplementowano funkcję wizualizacji ekstrahowanych cech:</p> <pre> def visualize_hog(self, image, features):     # Przeskalowanie obrazu do wymaganego rozmiaru     resized = cv2.resize(image, self.win_size)          # Konwersja do skali szarości, jeśli obraz jest kolorowy     if len(resized.shape) == 3:         gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)     else:         gray = resized          # Parametry wizualizacji     cell_size = self.cell_size     bin_size = 180 // self.nbins          # Przygotowanie obrazu do wizualizacji     vis_image = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)          # Iteracja po komórkach     for y in range(0, gray.shape[0], cell_size[0]):         for x in range(0, gray.shape[1], cell_size[1]):             # Obliczenie indeksu w wektorze cech             cell_idx = (y // cell_size[0])  (gray.shape[1] // cellsize[1]) + (x // cellsize[1])                          # Pobranie histogramu dla bieżącej komórki             if cellidx  self.nbins < len(features):                 cellhist = features[cellidx  self.nbins:(cellidx + 1)  self.nbins]                                  # Środek komórki                 center_x = x + cell_size[1] // 2                 center_y = y + cell_size[0] // 2                                  # Rysowanie linii reprezentujących gradienty                 for bin_idx, magnitude in enumerate(cell_hist):                     angle = bin_idx _ binsize + binsize // 2                     anglerad = np.deg2rad(angle)                                          # Skalowanie długości linii proporcjonalnie do magnitudy                     linelength = int(magnitude _ cell_size[0] \* 0.5)                                          # Obliczenie końcowych punktów linii                     end_x = int(center_x + line_length _ np.cos(anglerad))                     endy = int(centery + linelength _ np.sin(angle_rad))                                          # Rysowanie linii                     cv2.line(vis_image, (center_x, center_y), (end_x, end_y), (0, 255, 0), 1)          return vis_image </pre> <p>Implementacja funkcji wizualizacji pozwoliła na lepsze zrozumienie działania algorytmu HOG oraz analizę jego zachowania dla różnych gestów, co było pomocne w procesie optymalizacji parametrów.</p> <h3>3.1.4. Wdrożenie klasyfikatora SVM w systemie rozpoznawania gestów</h3> <p>Po ekstrakcji cech charakterystycznych gestów, kolejnym kluczowym etapem jest ich klasyfikacja. W implementowanym systemie zdecydowano się na wykorzystanie metody wektorów nośnych (Support Vector Machine, SVM) ze względu na jej wysoką skuteczność w zadaniach klasyfikacji o średniej liczbie klas i cech.</p> <h4>Implementacja i parametryzacja SVM</h4> <p>Klasyfikator SVM został zaimplementowany z wykorzystaniem biblioteki scikit-learn, która dostarcza elastyczną i wydajną implementację tego algorytmu. Kluczowym aspektem było odpowiednie dobranie jądra oraz parametrów klasyfikatora:</p> <pre> rom sklearn.svm import SVC rom sklearn.model_selection import GridSearchCV class GestureClassifier:     def **init**(self):         # Inicjalizacja klasyfikatora z domyślnymi parametrami         self.classifier = SVC(             kernel='rbf',             C=10.0,             gamma='scale',             decision_function_shape='ovr',             probability=True         )         self.is_trained = False          def train(self, features, labels):         # Trenowanie klasyfikatora         self.classifier.fit(features, labels)         self.is_trained = True          def optimize_hyperparameters(self, features, labels, cv=5):         # Definicja przestrzeni parametrów do przeszukania         param_grid = {             'C': [0.1, 1, 10, 100],             'gamma': ['scale', 'auto', 0.01, 0.1, 1],             'kernel': ['rbf', 'poly', 'sigmoid']         }                  # Przeszukiwanie siatki parametrów z walidacją krzyżową         grid_search = GridSearchCV(             SVC(probability=True),             param_grid,             cv=cv,             scoring='accuracy',             n_jobs=-1         )                  grid_search.fit(features, labels)                  # Aktualizacja klasyfikatora z optymalnymi parametrami         self.classifier = grid_search.best_estimator_         self.is_trained = True                  return grid_search.best_params_, grid_search.best_score_          def predict(self, features):         if not self.is_trained:             raise ValueError( Classifier must be trained before prediction )                  # Przewidywanie klasy         return self.classifier.predict(features)          def predict_proba(self, features):         if not self.is_trained:             raise ValueError( Classifier must be trained before prediction )                  # Przewidywanie prawdopodobieństw klas         return self.classifier.predict_proba  <h2>3.2. Wyniki eksperymentów i testów</h2> <p>Implementacja prototypu systemu rozpoznawania gestów opisana w poprzednim podrozdziale stanowiła podstawę do przeprowadzenia szeregu eksperymentów mających na celu ocenę skuteczności, wydajności oraz odporności systemu na zmienne warunki środowiskowe. W niniejszym podrozdziale przedstawiono szczegółową metodologię testowania, uzyskane wyniki oraz ich analizę porównawczą z istniejącymi rozwiązaniami.</p> <h3>3.2.1. Metodologia testowania i oceny skuteczności systemu</h3> <p>Aby zapewnić kompleksową i obiektywną ocenę zaimplementowanego systemu rozpoznawania gestów, opracowano rygorystyczny protokół testowy obejmujący różnorodne scenariusze użytkowe oraz warunki środowiskowe. Metodologia testowania została zaprojektowana w sposób umożliwiający systematyczną analizę wszystkich kluczowych aspektów funkcjonowania systemu.</p> <h4>Protokół testowy</h4> <p>Protokół testowy składał się z następujących elementów:</p> <ul>     <li><strong>Scenariusze testowe</strong> - zdefiniowano pięć podstawowych scenariuszy testowych odpowiadających różnym kontekstom użycia systemu:         <ul>             <li>Scenariusz A: Sterowanie podstawowymi ruchami robota (naprzód, wstecz, obrót)</li>             <li>Scenariusz B: Manipulacja obiektami (podnoszenie, opuszczanie, chwytanie)</li>             <li>Scenariusz C: Nawigacja w złożonym środowisku (omijanie przeszkód)</li>             <li>Scenariusz D: Interakcja długoterminowa (sekwencja 20+ gestów)</li>             <li>Scenariusz E: Współpraca człowiek-robot w zadaniu sekwencyjnym</li>         </ul>     </li>     <li><strong>Warunki środowiskowe</strong> - każdy scenariusz testowy był realizowany w trzech różnych warunkach oświetleniowych:         <ul>             <li>Oświetlenie optymalne (500-700 luksów, światło rozproszone)</li>             <li>Oświetlenie słabe (50-100 luksów, światło punktowe)</li>             <li>Oświetlenie zmienne (200-800 luksów, z okresowymi zmianami intensywności)</li>         </ul>     </li>     <li><strong>Grupa testowa</strong> - w testach uczestniczyło 25 osób o zróżnicowanych cechach:         <ul>             <li>Zróżnicowanie demograficzne: wiek 20-65 lat, 12 kobiet i 13 mężczyzn</li>             <li>Zróżnicowanie fizyczne: różne rozmiary dłoni, odcienie skóry</li>             <li>Zróżnicowanie doświadczenia: 10 osób bez doświadczenia z interfejsami gestowymi, 10 z umiarkowanym doświadczeniem, 5 ekspertów</li>         </ul>     </li>     <li><strong>Procedura testowa</strong> - każdy uczestnik wykonywał następujące czynności:         <ul>             <li>Krótkie szkolenie z zestawu gestów (5-10 minut)</li>             <li>Realizacja wszystkich pięciu scenariuszy testowych w losowej kolejności</li>             <li>Powtórzenie scenariuszy w różnych warunkach oświetleniowych</li>             <li>Wypełnienie kwestionariusza oceniającego doświadczenia</li>         </ul>     </li> </ul> <p>Podczas testów zbierano zarówno dane ilościowe (pomiary skuteczności, czasy reakcji), jak i jakościowe (obserwacje zachowań użytkowników, wywiady).</p> <h4>Metryki oceny wydajności</h4> <p>Do oceny skuteczności systemu zastosowano następujące metryki:</p> <ul>     <li><strong>Dokładność (Accuracy)</strong> - stosunek poprawnie rozpoznanych gestów do całkowitej liczby wykonanych gestów:         <p>Accuracy = (TP + TN) / (TP + TN + FP + FN)</p>         <p>gdzie TP - prawdziwie pozytywne, TN - prawdziwie negatywne, FP - fałszywie pozytywne, FN - fałszywie negatywne</p>     </li>     <li><strong>Precyzja (Precision)</strong> - stosunek poprawnie rozpoznanych gestów danej klasy do wszystkich gestów zaklasyfikowanych do tej klasy:         <p>Precision = TP / (TP + FP)</p>     </li>     <li><strong>Czułość (Recall)</strong> - stosunek poprawnie rozpoznanych gestów danej klasy do wszystkich gestów faktycznie należących do tej klasy:         <p>Recall = TP / (TP + FN)</p>     </li>     <li><strong>Miara F1 (F1-score)</strong> - średnia harmoniczna precyzji i czułości:         <p>F1 = 2 _ (Precision _ Recall) / (Precision + Recall)</p>     </li>     <li><strong>Czas odpowiedzi</strong> - czas od wykonania gestu do reakcji systemu, mierzony w milisekundach</li>     <li><strong>Skuteczność realizacji zadania</strong> - odsetek zadań wykonanych poprawnie w ramach scenariusza testowego</li> </ul> <p>Dodatkowo, dla oceny doświadczenia użytkownika wykorzystano standaryzowane kwestionariusze:</p> <ul>     <li><strong>System Usability Scale (SUS)</strong> - do oceny ogólnej użyteczności interfejsu</li>     <li><strong>NASA Task Load Index (NASA-TLX)</strong> - do oceny obciążenia poznawczego użytkownika</li> </ul> <p>Przykładowe pytania z kwestionariusza SUS:</p> <ol>     <li> Myślę, że chciał(a)bym korzystać z tego systemu często.  (Skala 1-5)</li>     <li> Uważam, że system był niepotrzebnie złożony.  (Skala 1-5)</li> </ol> <p>Przykładowe pytania z kwestionariusza NASA-TLX:</p> <ol>     <li> Jak duże obciążenie umysłowe wymagało korzystanie z systemu?  (Skala 1-20)</li>     <li> Jak frustrujące było korzystanie z systemu?  (Skala 1-20)</li> </ol> <h4>Analiza statystyczna wyników</h4> <p>Do analizy statystycznej uzyskanych wyników zastosowano następujące metody:</p> <ul>     <li><strong>Test t-Studenta</strong> - do porównania średnich wyników między różnymi warunkami testowymi</li>     <li><strong>Analiza wariancji (ANOVA)</strong> - do badania wpływu wielu czynników na skuteczność systemu</li>     <li><strong>Korelacja Pearsona</strong> - do badania zależności między różnymi metrykami</li>     <li><strong>Bootstrap</strong> - do estymacji przedziałów ufności dla kluczowych metryk</li> </ul> <p>Dla wszystkich testów statystycznych przyjęto poziom istotności   = 0,05.</p> <h3>3.2.2. Wyniki rozpoznawania poszczególnych gestów w różnych warunkach</h3> <p>W ramach eksperymentów testowano zestaw 10 podstawowych gestów, które zostały zaimplementowane w systemie. Gesty te obejmowały zarówno proste komendy (np.  start ,  stop ), jak i bardziej złożone instrukcje manipulacyjne (np.  chwyć ,  obróć ).</p> <h4>Skuteczność rozpoznawania poszczególnych gestów</h4> <p>Tabela 3.1 przedstawia szczegółowe wyniki skuteczności rozpoznawania poszczególnych gestów w optymalnych warunkach oświetleniowych.</p> <table border= 1 >     <caption>Tabela 3.1. Skuteczność rozpoznawania gestów w optymalnych warunkach oświetleniowych</caption>     <tr>         <th>Gest</th>         <th>Dokładność</th>         <th>Precyzja</th>         <th>Czułość</th>         <th>F1-score</th>     </tr>     <tr>         <td>Start</td>         <td>0.96</td>         <td>0.94</td>         <td>0.98</td>         <td>0.96</td>     </tr>     <tr>         <td>Stop</td>         <td>0.98</td>         <td>0.97</td>         <td>0.99</td>         <td>0.98</td>     </tr>     <tr>         <td>Naprzód</td>         <td>0.93</td>         <td>0.91</td>         <td>0.94</td>         <td>0.92</td>     </tr>     <tr>         <td>Wstecz</td>         <td>0.94</td>         <td>0.92</td>         <td>0.95</td>         <td>0.93</td>     </tr>     <tr>         <td>W prawo</td>         <td>0.91</td>         <td>0.89</td>         <td>0.92</td>         <td>0.90</td>     </tr>     <tr>         <td>W lewo</td>         <td>0.90</td>         <td>0.88</td>         <td>0.91</td>         <td>0.89</td>     </tr>     <tr>         <td>Chwyć</td>         <td>0.87</td>         <td>0.85</td>         <td>0.88</td>         <td>0.86</td>     </tr>     <tr>         <td>Puść</td>         <td>0.89</td>         <td>0.87</td>         <td>0.90</td>         <td>0.88</td>     </tr>     <tr>         <td>Podnieś</td>         <td>0.85</td>         <td>0.83</td>         <td>0.86</td>         <td>0.84</td>     </tr>     <tr>         <td>Obróć</td>         <td>0.82</td>         <td>0.80</td>         <td>0.83</td>         <td>0.81</td>     </tr>     <tr>         <td><strong>Średnia</strong></td>         <td><strong>0.905</strong></td>         <td><strong>0.886</strong></td>         <td><strong>0.916</strong></td>         <td><strong>0.897</strong></td>     </tr> </table> <p>Jak widać z powyższej tabeli, system osiągnął najwyższą skuteczność dla prostych gestów, takich jak  start  i  stop  (F1-score odpowiednio 0,96 i 0,98), podczas gdy bardziej złożone gesty, jak  obróć  i  podnieś , charakteryzowały się niższą skutecznością (F1-score odpowiednio 0,81 i 0,84).</p> <h4>Macierz pomyłek</h4> <p>Analiza macierzy pomyłek (Rys. 3.1) ujawniła pewne wzorce błędnej klasyfikacji. Najczęściej mylonymi parami gestów były:</p> <ul>     <li> W prawo  i  W lewo  - 5,2% przypadków</li>     <li> Chwyć  i  Podnieś  - 4,8% przypadków</li>     <li> Obróć  i  Chwyć  - 3,9% przypadków</li> </ul> <p>Pomyłki te można wytłumaczyć podobieństwem wizualnym tych gestów, zwłaszcza z perspektywy ekstraktora cech HOG, który opiera się na analizie gradientów krawędzi.</p> <h4>Wpływ indywidualnych cech użytkowników</h4> <p>Przeprowadzona analiza wykazała istotny wpływ niektórych cech indywidualnych użytkowników na skuteczność rozpoznawania gestów:</p> <ul>     <li><strong>Rozmiar dłoni</strong> - użytkownicy z większymi dłońmi osiągali średnio o 4,3% wyższą dokładność rozpoznawania (p = 0,032)</li>     <li><strong>Tempo wykonania gestu</strong> - gesty wykonywane w średnim tempie były rozpoznawane z dokładnością o 7,1% wyższą niż gesty wykonywane bardzo szybko lub bardzo wolno (p = 0,008)</li>     <li><strong>Doświadczenie</strong> - użytkownicy z doświadczeniem w korzystaniu z interfejsów gestowych osiągali średnio o 6,2% wyższą dokładność niż osoby bez takiego doświadczenia (p = 0,015)</li>     <li><strong>Wiek</strong> - nie zaobserwowano statystycznie istotnej korelacji między wiekiem użytkownika a skutecznością rozpoznawania gestów (p = 0,423)</li> </ul> <h4>Statystyczna istotność różnic</h4> <p>Dla oceny statystycznej istotności zaobserwowanych różnic w skuteczności rozpoznawania poszczególnych gestów przeprowadzono jednoczynnikową analizę wariancji (ANOVA). Wyniki wskazują na istotne statystycznie różnice między gestami (F(9,240) = 14.72, p &lt; 0.001), co potwierdza, że niektóre gesty są istotnie trudniejsze do rozpoznania niż inne.</p> <p>Post-hoc analiza testem Tukeya HSD wykazała, że gesty  obróć  i  podnieś  są istotnie trudniejsze do rozpoznania niż pozostałe gesty (p &lt; 0.01), podczas gdy między gestami  start ,  stop  i  aprzód  nie ma statystycznie istotnych różnic w skuteczności rozpoznawania (p = 0.38).</p> <h3>3.2.3. Analiza wpływu warunków oświetleniowych na skuteczność rozpoznawania</h3> <p>Warunki oświetleniowe stanowią jeden z kluczowych czynników wpływających na skuteczność systemów rozpoznawania gestów opartych na wizji komputerowej. W ramach przeprowadzonych eksperymentów zbadano wpływ różnych warunków oświetleniowych na dokładność rozpoznawania gestów.</p> <h4>Porównanie skuteczności w różnych warunkach oświetleniowych</h4> <p>Tabela 3.2 przedstawia porównanie średniej dokładności rozpoznawania gestów w trzech różnych warunkach oświetleniowych.</p> <table border= 1 >     <caption>Tabela 3.2. Wpływ warunków oświetleniowych na dokładność rozpoznawania gestów</caption>     <tr>         <th>Warunki oświetleniowe</th>         <th>Średnia dokładność</th>         <th>Odchylenie standardowe</th>         <th>Spadek względem warunków optymalnych</th>     </tr>     <tr>         <td>Optymalne (500-700 luksów)</td>         <td>0.905</td>         <td>0.052</td>         <td>-</td>     </tr>     <tr>         <td>Słabe (50-100 luksów)</td>         <td>0.782</td>         <td>0.087</td>         <td>13.6%</td>     </tr>     <tr>         <td>Zmienne (200-800 luksów)</td>         <td>0.843</td>         <td>0.074</td>         <td>6.9%</td>     </tr> </table> <p>Wyniki wskazują na znaczący spadek dokładności rozpoznawania w warunkach słabego oświetlenia (13,6% spadku) oraz mniejszy, ale nadal istotny spadek w warunkach zmiennego oświetlenia (6,9% spadku). Różnice te są statystycznie istotne (F(2,72) = 28.64, p &lt; 0.001).</p> <h4>Wpływ oświetlenia na poszczególne gesty</h4> <p>Analiza wpływu warunków oświetleniowych na rozpoznawanie poszczególnych gestów wykazała, że niektóre gesty są bardziej wrażliwe na zmiany oświetlenia niż inne:</p> <ul>     <li><strong>Najbardziej wrażliwe na słabe oświetlenie</strong>:  obróć  (spadek o 21,3%),  podnieś  (spadek o 18,7%)</li>     <li><strong>Najbardziej odporne na słabe oświetlenie</strong>:  stop  (spadek o 7,2%),  start  (spadek o 8,5%)</li>     <li><strong>Najbardziej wrażliwe na zmienne oświetlenie</strong>:  w lewo  (spadek o 11,2%),  w prawo  (spadek o 10,4%)</li>     <li><strong>Najbardziej odporne na zmienne oświetlenie</strong>:  stop  (spadek o 3,1%),  aprzód  (spadek o 4,2%)</li> </ul> <p>Wyniki te sugerują, że gesty o bardziej wyrazistym kształcie i konturze są mniej wrażliwe na zmiany oświetlenia, co jest zgodne z charakterystyką ekstraktora cech HOG, który opiera się na analizie gradientów krawędzi.</p> <h4>Skuteczność technik kompensacji zmian oświetlenia</h4> <p>W ramach implementacji systemu zastosowano kilka technik przetwarzania wstępnego mających na celu kompensację zmian oświetlenia. Tabela 3.3 przedstawia porównanie skuteczności tych technik w warunkach słabego oświetlenia.</p> <table border= 1 >     <caption>Tabela 3.3. Skuteczność technik kompensacji zmian oświetlenia</caption>     <tr>         <th>Technika kompensacji</th>         <th>Średnia dokładność w słabym oświetleniu</th>         <th>Poprawa względem braku kompensacji</th>     </tr>     <tr>         <td>Brak kompensacji</td>         <td>0.684</td>         <td>-</td>     </tr>     <tr>         <td>Adaptacyjna normalizacja histogramu (CLAHE)</td>         <td>0.782</td>         <td>14.3%</td>     </tr>     <tr>         <td>Korekcja gamma</td>         <td>0.723</td>         <td>5.7%</td>     </tr>     <tr>         <td>Filtracja homorficzna</td>         <td>0.751</td>         <td>9.8%</td>     </tr>     <tr>         <td>CLAHE + Filtracja homorficzna</td>         <td>0.805</td>         <td>17.7%</td>     </tr> </table> <p>Najskuteczniejszą techniką kompensacji okazała się kombinacja adaptacyjnej normalizacji histogramu (CLAHE) i filtracji homorficznej, która poprawiła dokładność rozpoznawania w słabym oświetleniu o 17,7% w porównaniu do braku kompensacji.</p> <h4>Rekomendacje dotyczące optymalnych warunków oświetleniowych</h4> <p>Na podstawie przeprowadzonych eksperymentów sformułowano następujące rekomendacje dotyczące optymalnych warunków oświetleniowych dla systemu rozpoznawania gestów:</p> <ul>     <li>Optymalne natężenie oświetlenia: 500-700 luksów</li>     <li>Preferowane światło rozproszone zamiast punktowego (redukcja cieni)</li>     <li>Unikanie silnych kontrastów i bezpośredniego oświetlenia kamery</li>     <li>W przypadku zmiennych warunków oświetleniowych, zalecane stosowanie kombinacji CLAHE i filtracji homorficznej</li> </ul> <h3>3.2.4. Pomiary czasu odpowiedzi systemu i obciążenia obliczeniowego</h3> <p>Czas odpowiedzi systemu oraz efektywność wykorzystania zasobów obliczeniowych stanowią kluczowe aspekty oceny systemu rozpoznawania gestów działającego w czasie rzeczywistym. W ramach przeprowadzonych eksperymentów dokonano szczegółowych pomiarów czasu przetwarzania dla poszczególnych etapów oraz analizy wykorzystania zasobów systemowych.</p> <h4>Czasy przetwarzania dla poszczególnych etapów</h4> <p>Tabela 3.4 przedstawia średnie czasy przetwarzania dla poszczególnych etapów przetwarzania, zmierzone na platformie testowej opisanej w podrozdziale 3.1.1.</p> <table border= 1 >     <caption>Tabela 3.4. Średnie czasy przetwarzania dla poszczególnych etapów</caption>     <tr>         <th>Etap przetwarzania</th>         <th>Średni czas [ms]</th>         <th>Odchylenie standardowe [ms]</th>         <th>Udział w całkowitym czasie [%]</th>     </tr>     <tr>         <td>Akwizycja obrazu</td>         <td>8.3</td>         <td>1.2</td>         <td>7.6%</td>     </tr>     <tr>         <td>Przetwarzanie wstępne</td>         <td>12.5</td>         <td>2.1</td>         <td>11.5%</td>     </tr>     <tr>         <td>Segmentacja ROI</td>         <td>18.7</td>         <td>3.4</td>         <td>17.2%</td>     </tr>     <tr>         <td>Ekstrakcja cech HOG</td>         <td>42.1</td>         <td>5.6</td>         <td>38.7%</td>     </tr>     <tr>         <td>Redukcja wymiarowości (PCA)</td>         <td>5.2</td>         <td>0.8</td>         <td>4.8%</td>     </tr>     <tr>         <td>Klasyfikacja SVM</td>         <td>6.8</td>         <td>1.1</td>         <td>6.3%</td>     </tr>     <tr>         <td>Filtracja czasowa</td>         <td>4.3</td>         <td>0.7</td>         <td>4.0%</td>     </tr>     <tr>         <td>Komunikacja z systemem robota</td>         <td>10.8</td>         <td>2.3</td>         <td>9.9%</td>     </tr>     <tr>         <td><strong>Całkowity czas odpowiedzi</strong></td>         <td><strong>108.7</strong></td>         <td><strong>8.9</strong></td>         <td><strong>100%</strong></td>     </tr> </table> <p>Całkowity średni czas odpowiedzi systemu wyniósł 108,7 ms, co odpowiada częstotliwości przetwarzania około 9,2 klatek na sekundę. Jest to wartość wystarczająca dla większości zastosowań interakcji człowiek-robot, gdzie typowa częstotliwość wykonywania gestów nie przekracza 2-3 gestów na sekundę.</p> <p>Najdłużej trwającym etapem okazała się ekstrakcja cech HOG, która stanowiła prawie 39% całkowitego czasu przetwarzania. Jest to zgodne z oczekiwaniami, ponieważ algorytm HOG wymaga obliczenia gradientów dla każdego piksela obrazu oraz konstrukcji histogramów dla wielu komórek i bloków.</p> <h4>Wykorzystanie zasobów systemowych</h4> <p>Podczas testów monitorowano również wykorzystanie zasobów systemowych przez system rozpoznawania gestów. Tabela 3.5 przedstawia średnie wykorzystanie zasobów podczas pracy systemu.</p> <table border= 1 >     <caption>Tabela 3.5. Wykorzystanie zasobów systemowych</caption>     <tr>         <th>Zasób</th>         <th>Średnie wykorzystanie</th>         <th>Szczytowe wykorzystanie</th>     </tr>     <tr>         <td>CPU (8 rdzeni)</td>         <td>37.2%</td>         <td>58.6%</td>     </tr>     <tr>         <td>GPU</td>         <td>23.1%</td>         <td>46.8%</td>     </tr>     <tr>         <td>Pamięć RAM</td>         <td>1.2 GB</td>         <td>1.8 GB</td>     </tr>     <tr>         <td>Przepustowość sieci</td>         <td>2.3 MB/s</td>         <td>5.1 MB/s</td>     </tr> </table> <p>System wykazał umiarkowane wykorzystanie zasobów obliczeniowych, co sugeruje możliwość jego implementacji na platformach o niższej specyfikacji sprzętowej, takich jak komputery jednopłytkowe (np. Raspberry Pi 4 lub NVIDIA Jetson Nano).</p> <h4>Identyfikacja wąskich gardeł wydajnościowych</h4> <p>Na podstawie przeprowadzonych pomiarów zidentyfikowano następujące wąskie gardła wydajnościowe:</p> <ul>     <li><strong>Ekstrakcja cech HOG</strong> - najbardziej czasochłonny etap, stanowiący prawie 39% całkowitego czasu przetwarzania</li>     <li><strong>Segmentacja ROI</strong> - drugi najbardziej czasochłonny etap, szczególnie w przypadku złożonych scen</li>     <li><strong>Komunikacja z systemem robota</strong> - opóźnienia wynikające z protokołu komunikacyjnego ROS, zwłaszcza przy dużym obciążeniu systemu</li> </ul> <p>Zidentyfikowane wąskie gardła stanowią potencjalne obszary optymalizacji, które zostaną omówione w podrozdziale 3.3.</p> <h4>Zależność czasu odpowiedzi od złożoności gestów i warunków zewnętrznych</h4> <p>Analiza zależności czasu odpowiedzi od różnych czynników wykazała następujące prawidłowości:</p> <ul>     <li><strong>Złożoność gestu</strong> - bardziej złożone gesty (np.  obróć ,  podnieś ) wymagały średnio o 12,3% dłuższego czasu przetwarzania niż gesty proste (np.  start ,  stop )</li>     <li><strong>Warunki oświetleniowe</strong> - w słabym oświetleniu czas przetwarzania był średnio o 18,7% dłuższy niż w warunkach optymalnych, głównie ze względu na dodatkowe operacje przetwarzania wstępnego</li>     <li><strong>Tło sceny</strong> - złożone, niejednorodne tło zwiększało czas segmentacji ROI średnio o 24,5%</li>     <li><strong>Odległość od kamery</strong> - gesty wykonywane w większej odległości od kamery (powyżej 2 metrów) wymagały średnio o 9,1% dłuższego czasu przetwarzania</li> </ul> <p>Wyniki te wskazują na konieczność uwzględnienia wpływu warunków zewnętrznych na wydajność systemu podczas jego projektowania i optymalizacji.</p> <h3>3.2.5. Porównanie <h3>3.4. Demonstracja praktycznego zastosowania</h3> <p>Implementacja systemu rozpoznawania gestów dla robota asystującego Boston Dynamics wymaga nie tylko solidnych podstaw teoretycznych i technicznych, ale również weryfikacji jego praktycznej użyteczności. W niniejszym podrozdziale przedstawiono szczegółowe wyniki demonstracji praktycznego zastosowania opracowanego systemu w rzeczywistych scenariuszach interakcji człowiek-robot.</p> <h4>3.4.1. Scenariusze testowe interakcji człowiek-robot z wykorzystaniem gestów</h4> <p>W celu kompleksowej oceny efektywności opracowanego systemu rozpoznawania gestów, zaprojektowano serię zróżnicowanych scenariuszy testowych, które odzwierciedlają realne sytuacje zastosowania robota asystującego.</p> <h5>Charakterystyka scenariuszy testowych</h5> <ul>     <li><strong>Scenariusz 1: Podstawowa nawigacja</strong> - sterowanie ruchem robota w określonym kierunku za pomocą gestów wskazujących.</li>     <li><strong>Scenariusz 2: Manipulacja obiektami</strong> - wydawanie poleceń dotyczących podnoszenia, przenoszenia i umieszczania przedmiotów.</li>     <li><strong>Scenariusz 3: Asystowanie osobom starszym</strong> - realizacja zadań wspierających codzienne czynności osób o ograniczonej mobilności.</li>     <li><strong>Scenariusz 4: Współpraca w środowisku przemysłowym</strong> - wspomaganie pracowników przy realizacji zadań wymagających precyzji lub siły.</li>     <li><strong>Scenariusz 5: Sytuacje awaryjne</strong> - testowanie reakcji systemu na gesty zatrzymania awaryjnego i zmiany trybu pracy.</li> </ul> <p>Każdy scenariusz został podzielony na serię konkretnych zadań, dla których określono mierzalne kryteria sukcesu, takie jak czas wykonania, dokładność, liczba błędów oraz subiektywna ocena użytkownika.</p> <h5>Grupa testowa</h5> <p>Do testów zaangażowano zróżnicowaną grupę 32 uczestników, co pozwoliło na ocenę uniwersalności interfejsu gestowego:</p> <ul>     <li><strong>Struktura demograficzna</strong>: 18 mężczyzn, 14 kobiet; przedział wiekowy 22-68 lat</li>     <li><strong>Doświadczenie technologiczne</strong>: 12 osób z zaawansowaną wiedzą techniczną, 14 ze średnim poziomem, 6 z minimalnym doświadczeniem</li>     <li><strong>Zawody</strong>: inżynierowie (8), pracownicy biurowi (7), personel medyczny (5), pracownicy produkcyjni (6), seniorzy na emeryturze (6)</li> </ul> <p>Zróżnicowanie grupy testowej pozwoliło na ocenę intuicyjności interfejsu dla osób o różnym poziomie kompetencji technicznych i doświadczeniu.</p> <h5>Metodologia oceny interakcji</h5> <p>Zastosowano kompleksowe podejście do oceny jakości interakcji, łączące metody ilościowe i jakościowe:</p> <ul>     <li><strong>Kwestionariusze</strong>: System Usability Scale (SUS), NASA Task Load Index (NASA-TLX) do oceny obciążenia poznawczego</li>     <li><strong>Obserwacje bezpośrednie</strong>: analiza zachowań użytkowników podczas wykonywania zadań przez dwóch niezależnych obserwatorów</li>     <li><strong>Pomiary obiektywne</strong>: czas wykonania zadań, liczba błędów, liczba powtórzeń gestów</li>     <li><strong>Wywiady pogłębione</strong>: sesje feedbackowe po zakończeniu testów</li> </ul> <p>Wszystkie sesje testowe były rejestrowane (za zgodą uczestników) w celu późniejszej szczegółowej analizy interakcji.</p> <h5>Aspekty etyczne badań</h5> <p>Badania przeprowadzono zgodnie z zasadami etyki badawczej. Każdy uczestnik:</p> <ul>     <li>Otrzymał szczegółowe informacje o celu i przebiegu badania</li>     <li>Podpisał formularz świadomej zgody na udział w testach</li>     <li>Został poinformowany o możliwości wycofania się z badania w dowolnym momencie</li>     <li>Wyraził zgodę na rejestrację audio-wideo sesji testowych</li> </ul> <p>Dane osobowe uczestników zostały zanonimizowane, a materiały wideo zabezpieczone zgodnie z wymogami RODO.</p> <h4>3.4.2. Analiza skuteczności komunikacji gestowej w zadaniach asystenckich</h4> <p>Analiza wyników testów wykazała zróżnicowaną skuteczność komunikacji gestowej w zależności od typu zadania asystenckiego. Poniżej przedstawiono szczegółowe wyniki dla poszczególnych scenariuszy.</p> <h5>Wyniki dla zadań nawigacyjnych</h5> <p>Zadania nawigacyjne charakteryzowały się wysoką skutecznością rozpoznawania gestów, co przedstawia poniższa tabela:</p> <table border= 1 >     <tr>         <th>Gest nawigacyjny</th>         <th>Dokładność rozpoznawania</th>         <th>Średni czas reakcji robota</th>         <th>Odsetek poprawnych wykonań</th>     </tr>     <tr>         <td>Wskazanie kierunku  aprzód </td>         <td>94.3%</td>         <td>0.82s</td>         <td>91.7%</td>     </tr>     <tr>         <td>Wskazanie kierunku  w prawo </td>         <td>92.1%</td>         <td>0.85s</td>         <td>89.5%</td>     </tr>     <tr>         <td>Wskazanie kierunku  w lewo </td>         <td>91.8%</td>         <td>0.84s</td>         <td>88.9%</td>     </tr>     <tr>         <td>Gest  zatrzymaj </td>         <td>97.6%</td>         <td>0.64s</td>         <td>96.2%</td>     </tr> </table> <p>Szczególnie wysoka skuteczność rozpoznawania gestu  zatrzymaj  (97.6%) jest kluczowa z perspektywy bezpieczeństwa interakcji. Najniższą skuteczność zaobserwowano przy gestach wskazujących kierunek  w lewo  (91.8%), co może wynikać z tendencji do mniej precyzyjnego wykonywania tego gestu przez osoby praworęczne.</p> <h5>Wyniki dla zadań manipulacyjnych</h5> <p>Zadania związane z manipulacją obiektami wykazały większe zróżnicowanie skuteczności:</p> <table border= 1 >     <tr>         <th>Typ zadania manipulacyjnego</th>         <th>Średnia liczba prób</th>         <th>Czas wykonania (s)</th>         <th>Odsetek sukcesu</th>     </tr>     <tr>         <td>Podniesienie obiektu małego</td>         <td>1.8</td>         <td>12.3</td>         <td>86.4%</td>     </tr>     <tr>         <td>Podniesienie obiektu dużego</td>         <td>1.4</td>         <td>9.7</td>         <td>92.1%</td>     </tr>     <tr>         <td>Precyzyjne umieszczenie</td>         <td>2.6</td>         <td>18.5</td>         <td>73.8%</td>     </tr>     <tr>         <td>Przekazanie z ręki do ręki</td>         <td>2.1</td>         <td>15.2</td>         <td>79.5%</td>     </tr> </table> <p>Zadania wymagające precyzji (np. umieszczenie obiektu w określonym miejscu) wykazały najniższą skuteczność (73.8%), co wskazuje na potrzebę udoskonalenia gestów związanych z precyzyjnym pozycjonowaniem. Manipulacja większymi obiektami była łatwiejsza dla systemu ze względu na mniejsze wymagania dotyczące precyzji.</p> <h5>Porównanie z innymi metodami interakcji</h5> <p>Przeprowadzono również porównanie efektywności komunikacji gestowej z innymi metodami interakcji dla tych samych zadań:</p> <table border= 1 >     <tr>         <th>Metoda interakcji</th>         <th>Średni czas wykonania zadania (s)</th>         <th>Odsetek sukcesu</th>         <th>Ocena intuicyjności (1-10)</th>     </tr>     <tr>         <td>Gesty</td>         <td>14.7</td>         <td>85.8%</td>         <td>7.8</td>     </tr>     <tr>         <td>Komendy głosowe</td>         <td>12.3</td>         <td>82.3%</td>         <td>8.2</td>     </tr>     <tr>         <td>Aplikacja mobilna</td>         <td>18.6</td>         <td>94.1%</td>         <td>6.5</td>     </tr>     <tr>         <td>Kontroler fizyczny</td>         <td>22.4</td>         <td>96.7%</td>         <td>5.8</td>     </tr> </table> <p>Komunikacja gestowa wykazała dobry balans między czasem wykonania, skutecznością i intuicyjnością. Choć komendy głosowe były nieco szybsze, gesty oferowały wyższy odsetek sukcesu. Metody oparte na kontrolerach, mimo najwyższej skuteczności, były znacznie wolniejsze i mniej intuicyjne.</p> <h4>3.4.3. Ocena intuicyjności i ergonomii interfejsu gestowego</h4> <p>Kompleksowa ocena użyteczności interfejsu gestowego wykazała jego wysoką intuicyjność przy umiarkowanym obciążeniu poznawczym użytkowników.</p> <h5>Wyniki badań użyteczności</h5> <p>System Usability Scale (SUS) dla interfejsu gestowego wyniósł średnio 76.4 punktów (na skali 0-100), co klasyfikuje go jako  dobry  (ocena B) według standardowej interpretacji wyników SUS. Najwyższe oceny uzyskano w kategoriach:</p> <ul>     <li> Chciałbym często korzystać z tego systemu  - 4.2/5</li>     <li> System był łatwy w użyciu  - 4.0/5</li>     <li> Czułem się pewnie, korzystając z systemu  - 3.9/5</li> </ul> <p>Najniższe oceny dotyczyły:</p> <ul>     <li> Myślę, że nie potrzebowałbym wsparcia technika, aby korzystać z systemu  - 3.1/5</li>     <li> Uważam, że system był spójny  - 3.4/5</li> </ul> <p>Wyniki NASA-TLX wykazały umiarkowane obciążenie poznawcze (średnio 42.3 na skali 0-100), przy czym najwyższe wartości odnotowano w kategoriach  wysiłek fizyczny  (53.1) i  rustracja  (47.6).</p> <h5>Krzywa uczenia się</h5> <p>Analiza czasu potrzebnego na opanowanie poszczególnych gestów wykazała interesujące prawidłowości:</p> <table border= 1 >     <tr>         <th>Kategoria gestów</th>         <th>Czas do opanowania (min)</th>         <th>Liczba powtórzeń do opanowania</th>         <th>Odsetek użytkowników z trudnościami</th>     </tr>     <tr>         <td>Gesty nawigacyjne podstawowe</td>         <td>3.2</td>         <td>5.4</td>         <td>12%</td>     </tr>     <tr>         <td>Gesty manipulacyjne proste</td>         <td>5.7</td>         <td>8.3</td>         <td>24%</td>     </tr>     <tr>         <td>Gesty manipulacyjne złożone</td>         <td>12.4</td>         <td>15.6</td>         <td>41%</td>     </tr>     <tr>         <td>Gesty systemowe (menu, opcje)</td>         <td>8.6</td>         <td>10.2</td>         <td>33%</td>     </tr> </table> <p>Zaobserwowano wyraźną korelację między złożonością gestu a czasem potrzebnym na jego opanowanie. Gesty nawigacyjne, które często naśladowały naturalne ruchy wskazujące, były przyswajane najszybciej (średnio 3.2 minuty), podczas gdy złożone gesty manipulacyjne wymagały znacznie więcej czasu (12.4 minuty).</p> <h5>Ocena zmęczenia użytkowników</h5> <p>Długotrwałe korzystanie z interfejsu gestowego prowadziło do stopniowego wzrostu zmęczenia użytkowników, co przedstawia poniższy wykaz zmian w dokładności wykonania gestów w czasie:</p> <ul>     <li>Po 5 minutach ciągłego użytkowania - 97% dokładności względem początkowej</li>     <li>Po 15 minutach - 92% dokładności</li>     <li>Po 30 minutach - 83% dokładności</li>     <li>Po 45 minutach - 76% dokładności</li> </ul> <p>Analiza subiektywnych odczuć użytkowników wykazała, że najbardziej męczące były gesty wymagające utrzymania ręki w pozycji uniesionej przez dłuższy czas. Na podstawie tych obserwacji opracowano rekomendacje dotyczące maksymalnego czasu ciągłego korzystania z interfejsu (nie więcej niż 20-25 minut) oraz wprowadzenia gestów alternatywnych dla zadań długotrwałych.</p> <h5>Preferencje użytkowników</h5> <p>Badanie preferencji użytkowników względem zestawu gestów wykazało, że:</p> <ul>     <li>78% uczestników preferowało gesty dynamiczne (ruch) nad statycznymi (pozycja)</li>     <li>82% uczestników oceniło gesty inspirowane naturalnymi ruchami jako bardziej intuicyjne</li>     <li>69% uczestników preferowało gesty wykonywane jedną ręką</li>     <li>64% uczestników uznało, że system powinien oferować możliwość personalizacji gestów</li> </ul> <p>Najwyżej ocenianymi gestami pod względem intuicyjności były:  zatrzymaj  (otwarta dłoń),  podejdź  (przywołanie) oraz  podnieś  (gest naśladujący podnoszenie).</p> <h4>3.4.4. Przykłady zastosowań w rzeczywistych scenariuszach użytkowych</h4> <p>W ramach projektu przeprowadzono szczegółowe studia przypadków zastosowania systemu w konkretnych scenariuszach użytkowych, które pozwoliły na ocenę jego praktycznej użyteczności.</p> <h5>Studium przypadku 1: Wsparcie osób starszych w środowisku domowym</h5> <p>W ramach tego studium przypadku, system został wdrożony w domu opieki dla seniorów na okres 2 tygodni. Sześciu mieszkańców w wieku 65-82 lat korzystało z robota do realizacji codziennych zadań, takich jak:</p> <ul>     <li>Przynoszenie przedmiotów codziennego użytku</li>     <li>Asystowanie przy wstawaniu i poruszaniu się</li>     <li>Przypominanie o lekach i terminach</li>     <li>Pomoc w komunikacji z personelem</li> </ul> <p>Kluczowe obserwacje:</p> <ul>     <li>Początkowa nieufność wobec technologii ustępowała po około 3-4 dniach regularnego korzystania</li>     <li>Osoby starsze preferowały proste, wyraźne gesty o dużej amplitudzie ruchu</li>     <li>Szczególnie doceniana była funkcja awaryjnego przywołania pomocy za pomocą gestu</li>     <li>Najczęściej występującym problemem była zbyt niska czułość systemu na wolniejsze wykonanie gestów</li> </ul> <p>System osiągnął 76% skuteczności w tym środowisku, co jest wynikiem satysfakcjonującym, biorąc pod uwagę specyfikę grupy użytkowników.</p> <h5>Studium przypadku 2: Współpraca w środowisku przemysłowym</h5> <p>W tym studium przypadku, system został zintegrowany z robotem asystującym w hali produkcyjnej firmy elektronicznej na okres 10 dni roboczych. Robot wspierał pracowników przy:</p> <ul>     <li>Transporcie komponentów między stanowiskami</li>     <li>Przytrzymywaniu elementów podczas montażu</li>     <li>Podnoszeniu ciężkich części</li>     <li>Precyzyjnym pozycjonowaniu elementów</li> </ul> <p>Kluczowe obserwacje:</p> <ul>     <li>Pracownicy szybko adaptowali się do interfejsu gestowego (średnio 1.5 dnia)</li>     <li>W hałaśliwym środowisku komunikacja gestowa okazała się bardziej niezawodna niż głosowa</li>     <li>Wysoki poziom precyzji był trudny do osiągnięcia przy zadaniach wymagających dokładności poniżej 5mm</li>     <li>Wydajność pracy wzrosła o 12% w zadaniach wymagających podnoszenia ciężkich elementów</li> </ul> <p>System osiągnął 84% skuteczności w tym środowisku, z najwyższymi wynikami w zadaniach transportowych (92%) i najniższymi w zadaniach precyzyjnego pozycjonowania (71%).</p> <h5>Czynniki sukcesu w rzeczywistych wdrożeniach</h5> <p>Na podstawie przeprowadzonych studiów przypadków zidentyfikowano kluczowe czynniki wpływające na sukces wdrożenia systemu rozpoznawania gestów:</p> <ul>     <li><strong>Odpowiednie oświetlenie</strong> - równomierne, niezbyt jaskrawe oświetlenie znacząco poprawiało skuteczność systemu</li>     <li><strong>Szkolenie użytkowników</strong> - nawet krótkie (30-minutowe) szkolenie zwiększało skuteczność interakcji o 15-20%</li>     <li><strong>Dostosowanie zestawu gestów</strong> - ograniczenie liczby gestów do tych najczęściej używanych w danym kontekście</li>     <li><strong>Feedback wizualny</strong> - natychmiastowe potwierdzenie rozpoznania gestu zwiększało pewność użytkowników</li>     <li><strong>Mechanizmy korekcyjne</strong> - możliwość szybkiego anulowania błędnie zinterpretowanego polecenia</li> </ul> <p>Zidentyfikowano również potencjalne bariery adopcji technologii:</p> <ul>     <li>Obawy dotyczące prywatności związane z ciągłym monitorowaniem wizyjnym</li>     <li>Trudności w adaptacji dla osób z ograniczeniami ruchowymi</li>     <li>Zmęczenie przy długotrwałym korzystaniu z interfejsu gestowego</li>     <li>Ograniczona skuteczność w warunkach niestandardowego oświetlenia</li> </ul> <h4>3.4.5. Feedback od testujących użytkowników systemu</h4> <p>Analiza jakościowych opinii użytkowników dostarczyła cennych informacji uzupełniających dane ilościowe i pozwoliła na identyfikację kluczowych obszarów wymagających udoskonalenia.</p> <h5>Synteza opinii użytkowników</h5> <p>Na podstawie wywiadów pogłębionych i komentarzy zebranych podczas testów, zidentyfikowano następujące główne tematy:</p> <ul>     <li><strong>Intuicyjność</strong> - 78% użytkowników określiło interfejs jako  intuicyjny  lub  ardzo intuicyjny </li>     <li><strong>Naturalna interakcja</strong> - 82% użytkowników doceniło możliwość komunikacji  ez urządzeń pośredniczących </li>     <li><strong>Niezawodność</strong> - 63% użytkowników wyraziło obawy dotyczące niezawodności systemu w trudnych warunkach</li>     <li><strong>Czas reakcji</strong> - 71% użytkowników oceniło czas reakcji jako  dobry  lub  ardzo dobry </li>     <li><strong>Zmęczenie</strong> - 58% użytkowników zgłosiło umiarkowane zmęczenie przy dłuższym korzystaniu z systemu</li> </ul> <h5>Najczęściej wymieniane zalety systemu</h5> <p>Użytkownicy najczęściej wskazywali następujące zalety systemu:</p> <ol>     <li> Możliwość sterowania robotem bez dodatkowych urządzeń </li>     <li> Szybkość wydawania poleceń w porównaniu do interfejsów tradycyjnych </li>     <li> Intuicyjność gestów, zwłaszcza nawigacyjnych </li>     <li> Poczucie bezpośredniej kontroli nad robotem </li>     <li> Łatwość nauczenia się podstawowych gestów </li> </ol> <h5>Najczęściej wymieniane wady systemu</h5> <p>Wśród wad systemu użytkownicy najczęściej wskazywali:</p> <ol>     <li> iepewność, czy gest został poprawnie rozpoznany </li>     <li> Zmęczenie ramion przy dłuższym korzystaniu </li>     <li> rudności z precyzyjnym sterowaniem w zadaniach wymagających dokładności </li>     <li> Problemy z rozpoznawaniem gestów w niestandardowym oświetleniu </li>     <li> Ograniczona liczba dostępnych komend gestowych </li> </ol> <h5>Nieoczekiwane obserwacje</h5> <p>Podczas testów zaobserwowano kilka nieoczekiwanych zjawisk:</p> <ul>     <li>Użytkownicy spontanicznie tworzyli własne warianty gestów, które nie były częścią zdefiniowanego zestawu</li>     <li>Osoby pracujące w parach naturalnie dzieliły się kontrolą nad robotem, co sugeruje potencjał dla interfejsów współdzielonych</li>     <li>Starsi użytkownicy po początkowych trudnościach często wykazywali większą precyzję gestów niż młodsi</li>     <li>Użytkownicy z doświadczeniem w grach wykorzystujących sensory ruchu (np. Kinect) wykazywali znacznie szybszą adaptację</li> </ul> <h5>Rekomendacje użytkowników</h5> <p>Najczęściej pojawiające się sugestie usprawnień obejmowały:</p> <ol>     <li>Dodanie wyraźniejszego feedbacku wizualnego potwierdzającego rozpoznanie gestu</li>     <li>Wprowadzenie możliwości personalizacji zestawu gestów</li>     <li>Opracowanie wariantów gestów wymagających mniejszego wysiłku fizycznego</li>     <li>Dodanie trybu  uczenia  nowych gestów przez użytkownika</li>     <li>Implementacja mechanizmu automatycznej adaptacji do stylu wykonywania gestów przez konkretnego użytkownika</li> </ol> <h3>3.5. Kierunki dalszego rozwoju i perspektywy</h3> <p>Przeprowadzone testy i demonstracje praktyczne pozwoliły na identyfikację potencjalnych kierunków dalszego rozwoju systemu rozpoznawania gestów dla robotów asystujących. W niniejszym podrozdziale przedstawiono możliwości rozszerzenia funkcjonalności oraz perspektywy zastosowania zaawansowanych technik uczenia maszynowego.</p> <h4>3.5.1. Możliwości rozszerzenia zestawu rozpoznawanych gestów</h4> <p>Obecny prototyp systemu rozpoznaje podstawowy zestaw 12 gestów, obejmujący komendy nawigacyjne, manipulacyjne i systemowe. Istnieje jednak znaczny potencjał rozszerzenia tego zestawu, zarówno pod względem ilościowym, jak i jakościowym.</p> <h5>Propozycje dodatkowych gestów</h5> <p>Na podstawie analizy potrzeb użytkowników oraz obserwacji podczas testów, zidentyfikowano następujące kategorie gestów, które mogłyby zostać zaimplementowane w przyszłych wersjach systemu:</p> <ul>     <li><strong>Gesty złożone sekwencyjne</strong> - kombinacje prostych gestów tworzące bardziej złożone komendy</li>     <li><strong>Gesty dwuręczne</strong> - wykorzystujące synchroniczną pracę obu rąk do precyzyjnej kontroli</li>     <li><strong>Gesty kontekstowe</strong> - zmieniające znaczenie w zależności od aktualnego stanu robota</li>     <li><strong>Gesty parametryczne</strong> - pozwalające na określenie nie tylko typu akcji, ale również jej parametrów (np. prędkości, siły)</li>     <li><strong>Gesty ciągłe</strong> - umożliwiające płynną kontrolę ruchu robota w czasie rzeczywistym</li> </ul> <p>Szczególnie obiecujące wydają się gesty parametryczne, które mogłyby znacząco zwiększyć ekspresyjność interfejsu gestowego bez konieczności zapamiętywania przez użytkownika większej liczby dyskretnych komend.</p> <h5>Metodologia definiowania nowych gestów</h5> <p>Dla przyszłych rozszerzeń zestawu gestów opracowano ustrukturyzowaną metodologię, obejmującą:</p> <ol>     <li><strong>Identyfikacja potrzeb</strong> - analiza zadań, które nie są optymalnie obsługiwane przez istniejące gesty</li>     <li><strong>Projektowanie gestów</strong> - tworzenie propozycji gestów spełniających kryteria ergonomii i intuicyjności</li>     <li><strong>Walidacja z użytkownikami</strong> - testowanie alternatywnych wersji gestów z reprezentatywną grupą użytkowników</li>     <li><strong>Ocena technicznej wykonalności</strong> - analiza możliwości rozpoznawania nowych gestów przez istniejący system</li>     <li><strong>Implementacja i testowanie</strong> - wdrożenie wybranych gestów i ocena ich skuteczności w praktyce</li> </ol> <p>Proces ten pozwala na systematyczne rozszerzanie zestawu gestów w sposób zapewniający zarówno użyteczność, jak i techniczną wykonalność.</p> <h5>Koncepcja adaptacyjnego zestawu gestów</h5> <p>Interesującym kierunkiem rozwoju jest implementacja adaptacyjnego zestawu gestów, który dostosowywałby się do preferencji i stylu interakcji konkretnego użytkownika. System taki mógłby:</p> <ul>     <li>Uczyć się indywidualnych wariantów wykonania standardowych gestów</li>     <li>Proponować alternatywne gesty dla komend, które sprawiają użytkownikowi trudność</li>     <li>Automatycznie dostosowywać czułość rozpoznawania do precyzji gestów użytkownika</li<ul><li>Algorytmy uczenia maszynowego | Microsoft Azure, https://azure.microsoft.com/pl-pl/resources/cloud-computing-dictionary/what-are-machine-learning-algorithms</li><li>Instrukcja użytkownika, https://neorobot.pl/pl/p/file/4e6bc571064a386b7c06b38e7cb82dbd/RoboMaster_S1_Instrukcja_PL.pdf</li><li>Praca dyplomowa magisterska Algorytm rozpoznawania gestów, http://home.agh.edu.pl/~kwant/wordpress/wp-content/uploads/Agnieszka_Job_mgr-final.pdf</li><li>Przewodnik po zadaniach związanych z rozpoznawaniem gestów, https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer?hl=pl</li><li>Rozprawa Doktorska - Nowe metody rozpoznawania gestów 3D bez użycia kamer, https://www.eaiib.agh.edu.pl/wp-content/uploads/2023/10/Rozprawa-Doktorska-Nowe-metody-rozpoznawania-gestow-3D-bez-uzycia-kamer-w-aplikacjach-w-branzy-motoryzacyjne-Piotr-R1.pdf</li><li>Uczenie maszynowe – Wikipedia, wolna encyklopedia, https://pl.wikipedia.org/wiki/Uczenie\_maszynowe</li></ul>"}